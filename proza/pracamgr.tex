\documentclass[a4paper,11pt,twoside]{book}
\usepackage[left=2.5cm,right=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
%\usepackage[polish]{babel}
\usepackage{polski}
\usepackage{amsmath, amsthm, amssymb} %Rozne matematyczne symbole
\usepackage{graphicx} %Zalaczanie obrazkow
\usepackage{titlesec} 
\usepackage{color}
\usepackage{array}
\usepackage{wrapfig} % Opływające obrazki
\usepackage[chapter]{algorithm} % allows to keep algorithms as floats
\usepackage{algpseudocode} % allows writing pseudocodes
\usepackage{textcomp} % symbol 1/2
\usepackage{bbm} %jedineczka
\usepackage[section]{placeins} % keeps floats in their places
\usepackage{fancyhdr}


%\addto\captionsenglish{
%  \renewcommand\chaptername{}}
\renewcommand\chaptername{Część}
\titleformat{\chapter}[display]
  {\normalfont\Large\filcenter\sffamily}
  {\titlerule[1pt]%
   \vspace{1pt}%
   \titlerule
   \vspace{1pc}%
   \LARGE\MakeUppercase{\chaptertitlename} \Roman{chapter}
  }
  {1pc}
  {\titlerule
  \vspace{1pc}%
  \Huge}
%\renewcommand\thechapter{\Roman{chapter}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Ustawienia środowika algorytmów %%%%%%%%%%%%%%%
\definecolor{comment}{RGB}{96,96,192}
\definecolor{colorForKeyWord}{RGB}{165,42,42}
\makeatletter
 \renewcommand{\ALG@name}{Algorytm} 
\makeatother  
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndFor}
\algtext*{EndFunction}
\renewcommand{\algorithmicif}{\textbf{jeżeli}}
\renewcommand{\algorithmicthen}{\textbf{to}}
\renewcommand{\algorithmicfor}{\textbf{dla}}
\renewcommand{\algorithmicwhile}{\textbf{dopóki}}
\renewcommand{\algorithmicdo}{\textbf{wykonuj}}
\renewcommand{\algorithmicreturn}{\textbf{zwróć}}
\renewcommand{\algorithmicfunction}{\textbf{procedura}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{N}{@{}m{0pt}@{}}

\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Pro}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\conv}{\rightarrow}
\newcommand{\Conv}{\longrightarrow}
\newcommand{\ceil}[1]{\lceil#1\rceil}
\newcommand{\Sg}[1]{S^*_#1}
\newcommand{\Slil}[1]{S^{lil}_#1}
\newcommand{\Sasin}[1]{S^{asin}_#1}
\newcommand{\norm}[2]{\mathcal{N}\left(#1, #2\right)}
  
\newtheorem{twier}{Twierdzenie}[chapter]
\newtheorem{lemat}[twier]{Lemat}
\newtheorem{fakt}[twier]{Fakt}
\theoremstyle{definition}
\newtheorem{mydef}{Definicja}[chapter]


\newenvironment{nscenter}
 {\parskip=0pt\par\nopagebreak\centering}
 {\par\noindent\ignorespacesafterend}

 
 
\title{Testowanie generatorów liczb pseudolosowych}
\author{Grzegorz Łoś}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO]{\small\bfseries\thepage}
\fancyhead[LE,RO]{\small\bfseries\thepage} %do odkomentowania w wersji dwustronnej
\fancyhead[LO]{\small\bfseries\nouppercase\rightmark}
\fancyhead[RE]{\small\bfseries\nouppercase\leftmark} %do odkomentowania w wersji dwustronnej

\begin{document}

\thispagestyle{empty}
\begin{center}
\textbf{\large Uniwersytet Wrocławski\\
Wydział Matematyki i Informatyki\\
Instytut Matematyczny}\\
\vspace{4cm}
\textbf{\textit{\large Grzegorz Łoś}\\
\vspace{0.5cm}
{\Large Zastosowanie błądzenia przypadkowego do testowania generatorów liczb pseudolosowych}}\\
\end{center}
\vspace{3cm}
{\large \hspace*{6.5cm}Praca magisterska\\
\hspace*{6.5cm}napisana pod kierunkiem\\
\hspace*{6.5cm}dr. Pawła Lorka }\\
\vfill
\begin{center}
{\large Wrocław 2015}\\
\end{center}

\newpage
\thispagestyle{empty}
\begin{minipage}{0.5\linewidth}
\end{minipage}

\newpage

% \begin{minipage}{0.8\linewidth}
% \maketitle
% \end{minipage}

\begin{minipage}{0.8\linewidth}
\tableofcontents
\end{minipage}

\chapter*{Wprowadzenie}
\addcontentsline{toc}{chapter}{\bfseries Wprowadzenie}
Wiele współczesnych technologii opiera się na randomizacji. W informatyce losowość pojawia się na każdym kroku i często nie zdajemy sprawy jak bardzo jesteśmy od niej uzależnieni. Programiści korzystają z niej na co dzień, często zupełnie nieświadomie, na przykład używając bibliotecznych implementacji algorytmu quicksort lub tablic haszujących. Randomizacja jest niezbędnym elementem w wielu innych specjalistycznych dziedzinach. Przykładowo w finansach ważną rolę odgrywają metody Monte Carlo polegające na wielokrotnej symulacji rozwoju rynku. Metody optymalizacji oparte o metaheurystyki lub algorytmy ewolucyjne nie miałyby bez losowości racji bytu.

Podane wyżej przykłady mają pewną wspólną cechę: drobne wady generatora liczb pseudolosowych (GLP), na których oparte są wspomniane metody, mogą obniżyć efektywność działania lub dokładność wyników, ale nie rujnują algorytmów całkowicie. Są jednak dziedziny, w których jakość GLP ma zasadnicze znaczenie. Dobrym przykładem jest kryptografia. Zauważalne odstępstwa od losowości mogą istotnie zwiększyć szanse złamania protokołu kryptograficznego, odkrycia klucza prywatnego, itp. Wynika stąd potrzeba zidentyfikowania tych GLP, na których można polegać.

Wyjście GLP jest po prostu ciągiem binarnych danych. Rozstrzygnięcie czy dany GLP jest wystarczająco solidny sprowadza się do odpowiedzi na pytanie czy jego wyjście jest nieodróżnialne od ciągu prawdziwie losowego. Interpretując wygenerowane bity jako +1 oraz -1, możemy łatwo zobaczyć, że wyjście GLP odpowiada realizacji błądzenia przypadkowego. Ten proces stochastyczny jest dobrze zbadany i opisany w literaturze (np. w \cite{feller}). Znamy wiele jego własności. Testowanie GLP polega na sprawdzeniu czy jego wyjście również je posiada.

W części \ref{czesc:bladzenie} opisujemy te własności błądzenia przypadkowego, które przydadzą się w dalszej części pracy. Autorzy \cite{wang-nic} zauważyli użyteczność prawa iterowanego logarytmu do testowania generatorów. W niniejszej pracy proponujemy metodę testowania generatorów opartą o prawo arcusa sinusa.

Część \ref{czesc:metoda} opisujemy dokładniej jak wykorzystać przytoczone prawa do testowania GLP. Postępujemy nieco inaczej niż w statystyce matematycznej, choć idea jest podobna. Uruchamiamy GLP $m$ razy (każdorazowo z innym ziarnem!). Na podstawie każdego ciągu zerojedynkowego otrzymanego z GLP (lub patrząc inaczej: na podstawie każdej realizacji błądzenia przypadkowego) obliczamy wartość pewnej funkcji. Znamy teoretyczny rozkład tej wartości i możemy obliczyć jego odległość od rozkładu otrzymanego empirycznie. Jeśli ta odległość jest duża, to możemy powiedzieć, że GLP jest niskiej jakości, w przeciwnym razie nie ma podstaw by go zdyskredytować.

Wyniki testów kilku znanych GLP przedstawione są w części \ref{czesc:testy}

{\bigskip \color{red} \LARGE{TODO!} Rozdmuchać wstęp}


\chapter{Błądzenie przypadkowe}
\label{czesc:bladzenie}

Jak napisaliśmy we wprowadzeniu, własności błądzenia przypadkowego (inaczej: losowego) będą kluczowe dla testowania GLP. Zebrane w tej części wiadomości opracowane są na podstawie \cite{feller}. Większość oznaczeń jest również wzorowane na tej książce.

Wyjście generowane przez GLP zawsze można traktować jako ciąg zerojedynkowy. Dlatego ważnym pojęciem będzie dla nas ciąg niezależnych prób Bernoulliego (inaczej: proces Bernoulliego) $(B_i)_{i \in \mathbb{N}}$. Dla ustalonego $p \in [0,1]$ oznaczamy w ten sposób ciąg niezależnych zmiennych losowych o jednakowym rozkładzie, taki że
\[ \Prob(B_1 = 1) = p = 1 - \Prob(B_1 = 0). \]
Możemy postrzegać $i$-ty bit wygenerowany przez GLP jako wyniki $i$-tej próby Bernoulliego. Dobry generator powinien z takim samym prawdopodobieństwem losować 0 oraz 1, dlatego ograniczymy się do przypadku $p = \frac{1}{2}$.

Często będzie nam wygodniej posługiwać się ciągiem prób $(X_i)_{i \in \mathbb{N}}$, który przyjmuje wartości -1 zamiast 0, czyli
\[ X_i \stackrel{D}{=} 2 B_i -1. \]
Ponadto oznaczmy
\[ S_n = \sum_{i=1}^{n} X_i. \]
Tak zdefiniowany proces $(S_i)_{i \in \mathbb{N}}$ jest nazywany błądzeniem przypadkowym. Ciąg ten w każdym kolejnym kroku zmienia swoją wartość o 1 lub -1. Czasem wygodnie jest go postrzegać jako wynik następującej gry. Dwóch graczy rzuca idealną monetą. Jeśli wypada orzeł, to pierwszy gracz otrzymuję złotówkę od drugiego, w przeciwnym przypadku pierwszy płaci złotówkę drugiemu. Proces $S$ przedstawia zysk ustalonego gracza.

\begin{figure}[ht]
 \centering
 \includegraphics[scale=0.4]{obrazki/rw.pdf}
 \caption{Przykładowe trajektorie procesu $S$.}
 \label{fig:bladzenie}
\end{figure}

Sporo miejsca w rachunku prawdopodobieństwa poświęcono badaniu własności błądzenia przypadkowego, z których dwie omawiamy poniżej. Ideą testów, które przedstawiamy w części~\ref{czesc:metoda} jest sprawdzanie czy wyjście GLP zachowuje się tak jak to wynika z praw rachunku prawdopodobieństwa.


\section{Prawo iterowanego logarytmu}
Jest jasne, że $|S_n| \leq n$. Można się jednak domyślać, choćby na podstawie rysunku \ref{fig:bladzenie}, że duże wartości $|S_n|$ są jednak bardzo mało prawdopodobne i w praktyce z dużym prawdopodobieństwem wartości $S_n$ znajdą się w znacznie węższym przedziale niż $[-n, n]$. Słabe i mocne prawo wielkich liczb mówią nam, że
\[\frac{S_n}{n} \stackrel{\Prob}{\rightarrow} 0 \hbox{, a nawet } \frac{S_n}{n} \stackrel{p.n.}{\rightarrow} 0.\] Jest więc jasne, że odchylenia procesu $S$ od zera rosną znacznie wolniej niż liniowo. Z drugiej strony centralne twierdzenie graniczne (CTG) mówi nam, że $\frac{S_n}{\sqrt{n}} \stackrel{D}{\rightarrow} \norm{0}{1}$
co jest w pewnym sensie oszacowaniem fluktuacji $S_n$ od dołu -- będą one wychodzić poza przedział $[-\sqrt{n}, \sqrt{n}]$, mamy bowiem
\begin{fakt}
\label{fakt:bladzenie_clt}
 Błądzenie przypadkowe $S_n$  z prawdopodobieństwem 1 spełnia \[ \limsup_{n \conv \infty} \frac{S_n}{\sqrt{n}} = \infty. \]
\end{fakt}
\begin{proof}
 Z prawa 0-1 Kołmogorowa wynika, że dla dowolnego ciągu zmiennych losowych $(X_i)$ i.i.d., zdarzenia typu $\left\{ \limsup\limits_{n \conv \infty} X_n > M \right\}$ mają prawdopodobieństwo równe 0 lub 1 (patrz \cite{jak-szt}, \S7.2, zadanie 1). Weźmy dowolnie duże $M$. Mamy
 \begin{align*}
  \Prob\left(\limsup_{n \conv \infty} \frac{S_n}{\sqrt{n}} > M \right)
  &= \Prob\left(\bigcap_{n=1}^\infty \bigcup_{k \geq n} \left\{ \frac{S_k}{\sqrt{k}} > M \right\} \right)\\
  &= \lim_{n \conv \infty} \Prob\left( \bigcup_{k \geq n} \left\{ \frac{S_k}{\sqrt{k}} > M \right\} \right) \\
  &\geq \lim_{n \conv \infty} \Prob\left(\frac{S_n}{\sqrt{n}} > M \right) \\
  &= 1 - \Phi(M) > 0.
 \end{align*}
 Czyli $\Prob\left(\limsup\limits_{n \conv \infty} \frac{S_n}{\sqrt{n}} > M \right) = 1$, co wobec dowolności $M$ oznacza, że \[ \Prob\left(\limsup_{n \conv \infty} \frac{S_n}{\sqrt{n}} = \infty \right) = 1. \]
\end{proof}

Okazuje się, że fluktuacje $S$ można oszacować precyzyjniej, mówi o tym
\begin{twier}[\textbf{Prawo iterowanego logarytmu}]
\label{tw:pil}
 Błądzenie przypadkowe $S_n$  z prawdopodobieństwem 1 spełnia \[ \limsup_{n \conv \infty} \frac{S_n}{ \sqrt{2 n \log \log n} } = 1. \]
\end{twier}
\noindent Dowód można znaleźć w \cite{feller}, rozdział VIII, \S5. Oczywiście ze względu na symetrię mamy analogiczne własności do Faktu \ref{fakt:bladzenie_clt} i Twierdzenia \ref{tw:pil} dla $\liminf$.

Jak widać $n$ było zbyt dużym dzielnikiem, a $\sqrt{n}$ zbyt małym -- odchylenia $S_n$ od zera rosną proporcjonalnie do $\sqrt{n \log \log n}$. Można zatem powiedzieć, że prawo iterowanego logarytmu~(PIL) ``działa pomiędzy'' prawem wielkich liczb i centralnym twierdzeniem granicznym. Te trzy twierdzenia dają nam własności błądzenia przypadkowego, które zebrano w Tabeli \ref{tab:wlasnosci_bladzenia}.

\begin{table}[ht]
\centering
 \caption{Wnioski dotyczące błądzenia przypadkowego wynikające ze znanych twierdzeń.}
 \label{tab:wlasnosci_bladzenia}
\begin{tabular} {||c | M{2.8cm} | M{2.8cm} | M{4cm} | M{4cm} || N}  
 \hline 
   & Zbieżność według prawdop. & Zbieżność prawie na pewno & Wartość limes superior prawie na pewno & Wartość limes inferior prawie na pewno  \\ \hline 
   PWL & $ \frac{S_n}{n} \stackrel{\Prob}{\Conv} 0 $ & $ \frac{S_n}{n} \stackrel{p.n.}{\Conv} 0 $ & $\limsup\limits_{n \conv \infty} \frac{S_n}{n} = 0 $ &  $\liminf\limits_{n \conv \infty} \frac{S_n}{n} = 0 $ &\\[1cm] \hline
   PIL & $ \frac{S_n}{\sqrt{2 n \log \log n}} \stackrel{\Prob}{\Conv} 0 $ & $ \frac{S_n}{\sqrt{2 n \log \log n}} \stackrel{p.n.}{\nrightarrow} 0 $ & $\limsup\limits_{n \conv \infty} \frac{S_n}{\sqrt{2n \log \log n}} = 1 $ &  $\liminf\limits_{n \conv \infty} \frac{S_n}{\sqrt{2n \log \log n}} = -1 $ &\\[1cm] \hline
   CTG & $ \forall x\ \frac{S_n}{\sqrt{n}} \stackrel{\Prob}{\nrightarrow} x $ &  $ \forall x\ \frac{S_n}{\sqrt{n}} \stackrel{p.n.}{\nrightarrow} x $ & $\limsup\limits_{n \conv \infty} \frac{S_n}{\sqrt{n}} = \infty $ &  $\liminf\limits_{n \conv \infty} \frac{S_n}{\sqrt{n}} = -\infty $ &\\[1cm] \hline
\end{tabular}  
\end{table}

Przyjrzyjmy się Rysunkowi \ref{fig:itlog}. Widać, że funkcja $\sqrt{2 n \log \log n}$ z grubsza odpowiada fluktuacjom procesu $S_n$. Można jednak zauważyć, że kilka trajektorii po około miliardzie kroków ciągle nie mieści się w przedziale $[-\sqrt{2 n \log \log n}, \sqrt{2 n \log \log n}]$. Prawo iterowanego logarytmu mówimy nam, że dla odpowiednio dużych $n$ trajektorie nie będą wykraczać poza ten zakres z prawdopodobieństwem 1. Wniosek jaki możemy wyciągnąć z tego obrazka jest taki, że mowa tu o naprawdę olbrzymich wartościach $n$.

\begin{figure}[!ht]
 \centering
 \includegraphics[scale=0.4]{obrazki/itlog.pdf}
 \caption{Ilustracja prawa iterowanego logarytmu. Przedstawia ona 500 trajektorii błądzenia losowego, długości $2^{30}$. Im ciemniejszy jest obszar wykresu, tym większe jest w nim zagęszczenie trajektorii. Niebieska krzywa to wykresy funkcji $\sqrt{x}$ oraz $-\sqrt{x}$, zaś czerwona funkcji $\sqrt{2 x \log \log x}$ oraz $-\sqrt{2 x \log \log x}$.}
 \label{fig:itlog}
\end{figure}

Choć nie będzie przydatna w dalszej części pracy, jeszcze jedna ciekawa własność narzuca się by o niej wspomnieć. Niech $\Slil{n} = \frac{S_n}{\sqrt{2n \log \log n}}$. Z PIL wynika, że wielkość $\Slil{n}$ nie zbiega punktowo do żadnej stałej. Zachodzi natomiast zbieżność według prawdopodobieństwa. Ustalmy więc dowolnie małe $\varepsilon > 0$ i zastanówmy się jak często $\Slil{n}$ opuści epsilonowy pasek wokół zera. Możemy przyjąć $p < 1$ dowolnie bliskie jedności, a mimo to dla prawie wszystkich $n$ możemy powiedzieć, że z prawdopodobieństwem $p$ wielkość $\Slil{n}$ nie wyjdzie poza przedział $(-\varepsilon, \varepsilon)$. Tymczasem PIL równocześnie mówi nam, że ten epsilonowy pasek opuścimy nieskończenie wiele razy. Ta niesamowita, pozorna sprzeczność pokazuje jak bardzo nasza intuicja zawodzi, gdy myślimy o zjawiskach zachodzących w nieskończoności.

\section{Prawo arcusa sinusa}
\label{par:asin}

Kolejna własność błądzenia przypadkowego, którą postaramy się wykorzystać do testowania GLP jest znana jako prawo arcusa sinusa. Odpowiada ono na pytanie przez jaką frakcję czasu ustalony gracz będzie na prowadzeniu. Spodziewalibyśmy się, że w przypadku bardzo długiej gry, obaj gracze będą na prowadzeniu przez mniej więcej tyle samo czasu. Jednak pokażemy, że również w tym przypadku nasza intuicja płata nam figla.

Powiemy, że bilans gry w $k$-tym kroku ($k \geq 1$) był dodatni, jeżeli $S_k > 0$ lub $S_{k-1}~>~0$. Pomijamy tu remisy przyjmując, że w przypadku wystąpienia równej liczby reszek i orłów przewagę ma ten, kto miał ją w poprzedniej chwili. Geometrycznie oznacza to, że odcinek wykresu błądzenia losowego przebiegający pomiędzy odciętymi $k-1$ oraz $k$, musi znajdować się nad osią x-ów.

Wprowadźmy następujące oznaczenia:
\begin{itemize}
  \setlength\itemsep{1pt}
 \item $U_n$ -- zdarzenie, że w $n$-tym kroku nastąpił powrót do zera,
 \item $F_n$ -- zdarzenie, ze w $n$-tym kroku nastąpił \emph{pierwszy} powrót do zera,
 \item $u_n = \Prob(U_n)$, $f_n = \Prob(F_n)$.
 \item $p_{k,n}$ -- prawdopodobieństwo, że przez $k$ spośród pierwszych $n$ kroków gry, bilans był dodatni.
\end{itemize}
Łatwo zauważyć, że powrót do zera może nastąpić tylko w parzystym kroku, zatem
\[ \forall n \in \mathbb{N}\ \ u_{2n-1} = f_{2n-1} = 0, \]
\[ \forall k,n \in \mathbb{N}\ \ p_{2k-1, 2n} = 0, \]
Ponadto przyjmujemy, że $p_{0,0} = u_0 = 1$. Zachodzi również

\begin{lemat}
 \label{lem:uf_val}
 Dla każdego $n \in \mathbb{N}$ spełnione są poniższe tożsamości:
 \begin{align}
  u_{2n} &= \binom{2n}{n}2^{-2n}   \label{eq:u_val}\\
  u_{2n} &= \sum_{r=1}^n f_{2r} u_{2n-2r}   \label{eq:u_val_cond}\\
  f_{2n} &= \frac{1}{2n} u_{2n-2} \label{eq:f_val}\\
  f_{2n} &= u_{2n-2} - u_{2n} \label{eq:f_val2}
 \end{align}
\end{lemat}
\begin{proof}
 Wzór (\ref{eq:u_val}) wynika stąd, że wszystkich dróg długości $2n$ jest $2^{2n}$, a drogi wracające na końcu do zera odpowiadają ustawieniu $n$ orłów i $n$ reszek na $2n$ miejscach -- co robimy na $\binom{2n}{n}$ sposobów.
 
 Tożsamość (\ref{eq:u_val_cond}) wynika wprost ze wzoru na prawdopodobieństwo całkowite:
 \[ u_{2n} = \Prob(U_{2n}) = \sum_{r=1}^n \Prob(U_{2n}|F_{2r}) \Prob(F_{2r}) = \sum_{r=1}^n \Prob(U_{2n-2r}) \Prob(F_{2r}) = \sum_{r=1}^n u_{2n-2r}f_{2r}  \]
 
 Dla dowodu (\ref{eq:f_val}) wprowadźmy dodatkowe oznaczenia:
 
\begin{itemize}
  \setlength\itemsep{1pt}
  \item $N_n(a,b)$ -- liczba ścieżek od stanu $a$ do stanu $b$ w $n$ krokach,
  \item $N_n^{\neq 0}(a,b)$ -- jak $N_n(a,b)$, ale ścieżki nie mogą dotykać 0 (za wyjątkiem co najwyżej końców),
  \item $N_n^{=0}(a,b)$ -- jak $N_n(a,b)$, ale ścieżki muszą dotknąć lub przeciąć 0.
\end{itemize}
Łatwo zauważyć, że $N_n(a,b) = \binom{n}{(n+b-a)/2}$ oraz $N_n(a,b) = N_n^{\neq 0}(a,b) + N_n^{= 0}(a,b)$. Wartość $f_{2n}$ to oczywiście stosunek $N_{2n}^{\neq 0}(0,0)$ do liczby wszystkich ścieżek od stanu 0 do stanu 0 w $2n$ krokach. Dlatego liczymy
\[ N_{2n}^{\neq 0}(0,0) = N_{2n-1}^{\neq 0}(1,0) + N_{2n-1}^{\neq 0}(-1,0) = 2N_{2n-1}^{\neq 0}(1,0)= 2N_{2n-2}^{\neq 0}(1,1) \]
Patrząc na Rysunek \ref{fig:forlemma} łatwo zauważyć, że $N_{2n-2}^{=0}(1,1) = N_{2n-2}(-1,1)$. 
\begin{figure}[h]
 \centering
 \includegraphics[scale=0.5]{obrazki/forlemma.pdf}
 \caption{Ilustracja faktu $N_n^{=0}(1,1) = N_n(-1,1)$. Łatwo zobaczyć jednoznaczną odpowiedniość między oboma rodzajami ścieżek. Aż do momentu pierwszego powrotu do zera ścieżka jednego rodzaju jest odbiciem symetrycznym względem osi odciętych ścieżki drugiego rodzaju, zaś dalej ścieżki się pokrywają.}
 \label{fig:forlemma}
\end{figure}
Zatem
\begin{equation*}
 \begin{split}
   N_{2n-2}^{\neq 0}(1,1)
   &= N_{2n-2}(1,1) - N_{2n-2}^{=0}(1,1) =  N_{2n-2}(1,1) - N_{2n-2}(-1,1) \\
   &= \binom{2n-2}{n-1} - \binom{2n-2}{n} = \binom{2n-2}{n-1} - \frac{n-1}{n}\binom{2n-2}{n-1} \\
   &= \frac{1}{n} \binom{2n-2}{n-1} = \frac{2^{2n-2}}{n} u_{2n-2}
 \end{split}
\end{equation*}
Ostatecznie

\begin{equation*}
 \begin{split}
   f_{2n} &= \frac{N_{2n}^{\neq 0}(0,0)}{2^{2n}} = \frac{2N_{2n-2}^{\neq 0}(1,1)}{2^{2n}} = \frac{\frac{2^{2n-1}}{n} u_{2n-2}}{2^{2n}} = \frac{u_{2n-2}}{2n}
 \end{split}
\end{equation*}

 Formuła (\ref{eq:f_val2}) to prosta konsekwencja (\ref{eq:u_val}) i (\ref{eq:f_val}), bo
 \begin{equation*}
 \begin{split}
    u_{2n-2} - u_{2n} &= u_{2n-2} - \binom{2n}{n}2^{-2n} =  u_{2n-2} - \binom{2n-2}{n-1} \frac{(2n-1)2n}{4n^2}2^{-(2n-2)} = \\
    &= u_{2n-2}\left(1 - \frac{(2n-1)}{2n} \right) = \frac{1}{2n} u_{2n-2} =  f_{2n}.
 \end{split}
 \end{equation*}
\end{proof}

Tożsamości z Lematu \ref{lem:uf_val} intensywnie wykorzystujemy w dowodzie następującego, kluczowego faktu.
\begin{twier}
 \label{twier:disc_asine_law}
 Dla wszystkich $k, n \in \mathbb{N}$
 \begin{equation}
  p_{2k,2n} = u_{2k} u_{2n-2k} = \binom{2k}{k}\binom{2n-2k}{n-k}2^{-2n} \label{eq:disc_asine_law}
 \end{equation}
\end{twier}
\begin{proof}
Niech $q_{2n}$ oznacza prawdopodobieństwo, że w pierwszych $2n$ krokach gry ani razu nie doszło do remisu. Wzór (\ref{eq:f_val2}) daje nam
\[ q_{2n} = 1 - f_2 - f_4 - \cdots - f_{2n} = 1 - (1- u_2) - (u_2 - u_4) - \cdots - (u_{2n-2} - u_{2n}) = u_{2n}. \]
Udowodnimy teraz indukcyjnie, że
\begin{equation}
 p_{0,2n} = u_{2n}. \label{eq:disc_asine_law_k0}
\end{equation}
Łatwo sprawdzić, że $p_{0,2} = \frac{1}{2} = u_2$. Załóżmy, że $p_{0,2\tilde{n}} = u_{2\tilde{n}}$ dla $\tilde{n} < n$.  Zauważmy, że aby spędzić całą grę na minusie, musieliśmy w pierwszym kroku pójść w dół, co dzieje się z prawdopodobieństwem $\frac{1}{2}$. Dalej musiała zajść jedna z dwóch możliwości. Z prawdopodobieństwem $q_{2n}$ mogliśmy ani razu nie wrócić do zera. Mogło się też zdarzyć, że dla pewnego $r$ wróciliśmy do zera po raz pierwszy w kroku $2r$ (z prawdopodobieństwem $f_{2r}$), ale resztę czasu mimo tego spędziliśmy ``pod kreską'' (z prawdopodobieństwem $p_{0,2n-2r}$). Te rozważania, założenie indukcyjne oraz wzór (\ref{eq:u_val_cond}) dają
\begin{equation*}
 \begin{split}
  p_{0,2n} &= \frac{1}{2} \left( q_{2n} + \sum_{r=1}^n f_{2r} p_{0,2n-2r} \right) = \frac{1}{2} \left( u_{2n} + \sum_{r=1}^n f_{2r} u_{2n-2r}  \right) \\
  &= \frac{1}{2} \left( u_{2n} + u_{2n}  \right) = u_{2n},
 \end{split}
\end{equation*}
co chcieliśmy pokazać.

Teraz uogólniamy ten wynik postępując również indukcyjnie. Twierdzenie \ref{twier:disc_asine_law} jest w oczywisty sposób prawdziwe dla $n=0$. Załóżmy teraz, że dla wszystkich $\tilde{n} < n$ zachodzi $\forall 0 \leq k \leq \tilde{n}\ \ p_{2k,2\tilde{n}} = u_{2k} u_{2\tilde{n}-2k}$ i pokażemy, że $\forall 0 \leq k \leq n\ \ p_{2k,2n} = u_{2k} u_{2n-2k}$. 
Wiemy już, że teza jest prawdziwa dla $k = 0$ oraz $k = n$, gdyż
\[  p_{2n,2n} = p_{0,2n} = u_{2n} = u_{2n}u_0. \]
Dlatego weźmy dowolne $k$, takie że $0 < k < n$. Aby zaszło rozważane zdarzenie, błądzenie musi przechodzić przez 0. Załóżmy, że pierwszy raz dzieje się to w pewnym punkcie $2r$. Jeżeli w pierwszym kroku poszliśmy w górę (co dzieje się z prawdopodobieństwem $\frac{1}{2}$), to po powrocie musimy spędzić ``nad kreską'' jeszcze $2k-2r$ kroków, a szanse tego zdarzenia wynoszą $p_{2k-2r, 2n-2r}$. W przeciwnym razie po powrocie ciągle musimy być na plusie przez $2k$ kroków, co zdarzy się z prawdopodobieństwem $p_{2k,2n-2r}$. Stąd
\begin{equation*}
 \begin{split}
  p_{2k,2n} &= \frac{1}{2} \left( \sum_{r=1}^k f_{2r} p_{2k-2r,2n-2r} + \sum_{r=1}^{n-k} f_{2r} p_{2k, 2n-2r} \right) = (\bigstar)
 \end{split}
\end{equation*}
\noindent Z założenia indukcyjnego
\[ p_{2k-2r,2n-2r} = u_{2k-2r}u_{2n-2r - (2k-2r)} = u_{2k-2r}u_{2n-2k} \]
oraz
\[ p_{2k,2n-2r} = u_{2k}u_{2n-2r-2k}, \]
zatem
\begin{equation*}
 \begin{split}
  (\bigstar) &= \frac{1}{2} \left( \sum_{r=1}^k f_{2r} u_{2k-2r}u_{2n-2k} + \sum_{r=1}^{n-k} f_{2r} u_{2k}u_{2n-2r-2k} \right) \\
             &= \frac{1}{2} \left( u_{2n-2k}\sum_{r=1}^k f_{2r} u_{2k-2r} + u_{2k}\sum_{r=1}^{n-k} f_{2r} u_{2n-2r-2k} \right) \\
             &= \frac{1}{2} \left( u_{2n-2k}u_{2k} + u_{2k} u_{2n-2k} \right) = u_{2k} u_{2n-2k},
 \end{split}
\end{equation*}
co było do okazania. Korzystając z (\ref{eq:u_val}) otrzymujemy tezę.
\end{proof}

Dzięki Twierdzeniu \ref{twier:disc_asine_law} możemy obliczać dokładne prawdopodobieństwa frakcji przewagi. Na Rysunku \ref{fig:disc_asine} przedstawiony jest ich rozkład dla $n=20$. Widać wyraźnie, że równomierny podział czasu na przewagę jednego i drugiego gracza jest najmniej prawdopodobny. Najbardziej prawdopodobna jest dominacja jednego z graczy przez większość czasu. Przykładowo prawdopodobieństwo, że po 100 rzutach
\begin{itemize}
 \item jeden z graczy wygrywa przez 90-100\% czasu, wynosi 44\%.
 \item jeden z graczy ani razu nie wyjdzie na prowadzenie, wynosi 16\%.
 \item ustalony gracz będzie prowadził przez 40-60\% czasu, wynosi 14\%.
\end{itemize}

\begin{figure}[ht]
 \centering
 \includegraphics[scale=0.4]{obrazki/discasine.pdf}
 \caption{Rozkład czasu prowadzenia ustalonego gracza przy 40 rzutach monetą.}
 \label{fig:disc_asine}
\end{figure}

Wzór (\ref{eq:disc_asine_law}) jest dokładny, ale często nieporęczny. Spróbujmy znaleźć rozsądne przybliżenie. Zakładając $k \conv \infty,\ n-k \conv \infty$ i korzystając ze wzoru Stirlinga ($n! \approx \sqrt{2\pi n} \left( \frac{n}{e} \right)^n$), dostajemy
\[ \binom{2k}{k} = \frac{(2k)!}{k! k!} \approx \frac{\sqrt{4\pi k} \left( \frac{2k}{e} \right)^{2k}}{2\pi k \left( \frac{k}{e} \right)^{2k}} = \frac{2^{2k}}{\sqrt{\pi k}}, \]
i podobnie
\[ \binom{2n-2k}{n-k} \approx \frac{2^{2n-2k}}{\sqrt{\pi (n-k)}}. \]
Podstawiając to do wzoru (\ref{eq:disc_asine_law}) otrzymujemy
\begin{equation}
  \label{eq:disc_asine_approx}
   p_{2k,2n} \approx \frac{1}{\pi \sqrt{k(n-k)}}
\end{equation}

% \begin{equation*}
%  \begin{split}
%    p_{2k,2n}
%    &= \binom{2k}{k}\binom{2n-2k}{n-k} 2^{-2n} = \frac{(2k)!}{k! k!} \frac{(2n - 2k)!}{(n-k)! (n-k)!} 2^{-2n} \\
%    &\approx \frac{\sqrt{4\pi k} \left( \frac{2k}{e} \right)^{2k}}{2\pi k \left( \frac{k}{e} \right)^{2k}}
%             \frac{\sqrt{4\pi (n-k)} \left( \frac{2n-2k}{e} \right)^{2n-2k}}{2\pi (n-k) \left( \frac{n-k}{e} \right)^{2n-2k}} 2^{-2n} \\
%    &=       \frac{2^{2k}}{ \sqrt{\pi k} } \frac{ 2^{2n-2k}}{ \sqrt{\pi (n-k)} } 2^{-2n} = \frac{1}{\pi \sqrt{k(n-k)}}
%  \end{split}
% \end{equation*}

Odpowiemy teraz na następujące pytanie: \textbf{jaka jest szansa, że w bardzo długiej grze byliśmy na prowadzeniu przez co najwyżej frakcję $x$ czasu?} ($0 < x < 1$)

Niech $P_{2n}(x)$ oznacza szukane prawdopodobieństwo przy $2n$ rzutach monetą. Załóżmy na początek, że $x > \frac{1}{2}$. Wtedy
\[ P_{2n}(x) = \sum_{k:\ \frac{k}{n} < x} p_{2k,2n} = \underbrace{\sum_{k:\ \frac{k}{n} \leq \frac{1}{2}} p_{2k,2n}}_{(\spadesuit)} + \underbrace{\sum_{k:\ \frac{1}{2} < \frac{k}{n} < x} p_{2k,2n}}_{(\clubsuit)}  \]
Pamiętając o symetryczności rozkładu można zauważyć, że $(\spadesuit) \Conv \frac{1}{2}$ (można to też uzasadnić inaczej -- jeden z graczy musi być na prowadzeniu przez co najwyżej połowę czasu). Przy $n \conv \infty$ i $\frac{1}{2} < \frac{k}{n} < x < 1$ zachodzi również $k \Conv \infty$ oraz $\ n-k \Conv \infty$. Dlatego drugą sumę możemy estymować korzystając z (\ref{eq:disc_asine_approx}) oraz definicji całki Riemanna
\begin{equation*}
 \begin{split}
 (\clubsuit) &\approx \sum_{k:\ \frac{1}{2} < \frac{k}{n} < x} \frac{1}{\pi \sqrt{k(n-k)}} =  \sum_{k:\ \frac{k}{n} < x} \frac{1}{\pi n} \frac{1}{ \sqrt{\frac{k}{n} (1 - \frac{k}{n})} } \\
 &\xrightarrow[n \conv \infty]{} \frac{1}{\pi} \int_{1/2}^x \frac{dt}{\sqrt{t(1-t)}} = \frac{2}{\pi}\arcsin(\sqrt{x}) - \frac{1}{2},
 \end{split}
\end{equation*}
czyli
\[  P_{2n}(x) \xrightarrow[n \conv \infty]{} \frac{2}{\pi}\arcsin(\sqrt{x}). \]
W celu znalezienia $P_{2n}(x)$ dla $0 < x < \frac{1}{2}$ skorzystamy ze znanych własności funkcji cyklometrycznych: $\arcsin x + \arccos x = \frac{\pi}{2}$ oraz $\arccos x = \arcsin(\sqrt{1-x^2})$.
\begin{equation*}
 \begin{split}
  P_{2n}(x) &= 1 - P_{2n}(1-x) \xrightarrow[n \conv \infty]{} 1 - \frac{2}{\pi} \arcsin(\sqrt{1-x}) = 1 - \frac{2}{\pi} \arccos(\sqrt{x}) \\
  &= 1 - \frac{2}{\pi} \left( \frac{\pi}{2} - \arcsin(\sqrt{x}) \right) = \frac{2}{\pi}\arcsin(\sqrt{x}).
 \end{split}
\end{equation*}
W ten sposób udowodniliśmy
\begin{twier}[\textbf{Prawo arcusa sinusa}]
 Prawdopodobieństwo, że w $n$ krokach frakcja czasu $x$ ($0 \leq x \leq 1$), w której ustalony gracz ma przewagę (stan błądzenia przypadkowego jest dodatni), dąży przy $n \Conv \infty$ do
 \[  \int_0^x \frac{dt}{\sqrt{t(1-t)}} = \frac{2}{\pi}\arcsin(\sqrt{x}) \]
\end{twier}

Innymi słowy w bardzo długiej grze frakcja czasu $x$ spędzona ``na plusie'' ma rozkład arcusa sinusa. Oto jego podstawowe własności:
\begin{itemize}
 \item gęstość $f(t) =  \frac{1}{\sqrt{t(1-t)}}$,
 \item dystrybuanta $F(t) =  \frac{2}{\pi}\arcsin(\sqrt{t})$,
 \item wartość oczekiwana: $\frac{1}{2}$,
 \item wariancja: $\frac{1}{8}$.
\end{itemize}
Wykres gęstości i dystrybuanty przedstawia Rysunek \ref{fig:asine_dist}. Funkcja gęstości w kształcie litery U pokazuje, że nierówny podział czasu przewagi jest zdecydowanie bardziej prawdopodobny niż względnie równomierny.
\begin{figure}[ht]
 \centering
 \includegraphics[scale=0.3]{obrazki/asinedist.pdf}
 \caption{Rozkład arcusa sinusa.}
 \label{fig:asine_dist}
\end{figure}

Ludzka intuicja silnie podpowiada, że w grze z symetryczną monetą, każdy z graczy powinien być na plusie przez około połowę czasu. Wydaje się to logiczne -- wiadomo, że liczba powrotów błądzenia przypadkowego do zera jest nieskończona w nieskończenie długiej grze. Zatem obaj gracze mają mniej więcej tyle samo fal kiedy są na plusie. Ponadto średnia długość dodatniej fali powinna być dla obu graczy zbliżona. Co z kolei prowadzi do wniosku, że obaj powinni być na prowadzeniu przez podobną frakcję czasu. Gdzie tkwi błąd w tym rozumowaniu? Otóż MPWL dotyczy zmiennych o skończonej wartości oczekiwanej. Tymczasem oczekiwany czas powrotu do zera w błądzeniu przypadkowym okazuje się być nieskończony, co kompletnie zmyla nasze intuicje.

\chapter{Generatory liczb pseudolosowych}
\label{czesc:generatory}

Każdy intuicyjnie rozumie czym jest generator liczb pseudolosowych. Jednak dla pełności matematycznego opisu zaczniemy od przedstawienia jego ścisłej definicji. Przedstawiona tu teoria opiera się na książce \cite{asmussen}. Na jej podstawie opisujemy również kilka rodzajów generatorów liczb pseudolosowych. Sposób otrzymania z GLP ciągu bitów o dobrych własnościach również wymaga pewnego komentarza, o czym piszemy dalej. Na końcu tej części krótko omówimy znane metody testowania generatorów liczb pseudolosowych.

\section{Definicja GLP}
Istnieją metody otrzymania liczb ,,prawdziwie losowych''. Najprostszym sposobem jest wielokrotne rzucenie kostką do gry lub monetą i stablicowanie otrzymanych wyników. Lepszym sposobem jest obserwowanie emitowanych cząsteczek przez próbkę radioaktywnego pierwiastka -- uważa się, że rozkład radioaktywny jest dobrze modelowany przez proces Poissona. Kolejnym pomysłem jest wykorzystanie szumu atmosferycznego, z tej metody korzysta strona \cite{rorg}. Źródła takiej losowości są jednak zazwyczaj zbyt wolne, trudno dostępne lub mają pewne inherentne wady (moneta może być niesymetryczna, detektor cząstek nie rejestruje zgłoszeń o zbyt krótkim odstępie, itp.). Wynika stąd potrzeba utworzenia deterministycznych algorytmów które imitowałyby losowość. Takie algorytmy nazywamy generatorami liczb pseudolosowych. Aby były praktyczne, muszą być szybkie i obliczalne na zwykłych komputerach. Liczby przez nie generowane jedynie ,,udają'' losowe, dlatego nazywamy je pseudolosowymi.
\begin{mydef}
\label{def:glp}
 Generator liczb pseudolosowych jest to piątka\footnote{Jest to nieco inna definicja niż w \cite{asmussen}, dostosowana do naszych potrzeb. W książce Asmussena przyjmuje się $V = [0,1]$, zaś zamiast $s_0$ w definicji znajduje się $\mu$ -- rozkład prawdopodobieństwa początkowego stanu.} $\langle E, V, s_0, f, g \rangle$, gdzie $E$ jest skończoną przestrzenią stanów, $V$ jest zbiorem wartości zwracanych przez generator, $s_0$ jest to tzw. ziarno, czyli początkowy stan w ciągu stanów $(s_i)_{i=0}^\infty$, funkcja $f:E \rightarrow E$ opisuje przejścia między kolejnymi stanami: $s_n = f(s_{n-1})$, zaś $g:E\rightarrow V$, odwzorowuje stan generatora w wartość przez niego zwracaną.
\end{mydef}
Najczęściej przyjmuje się $V = (0,1)$ lub $V = \bar{M}$ dla pewnego $M$ (dla $n \in \mathbb{N}$ symbol $\bar{n}$ oznacza zbiór $\{0,1,\ldots,n-1\}$). U nas będzie zachodzić właśnie ta druga możliwość.

Zauważmy, że każdy GLP prędzej lub później ,,zapętla się'', tzn. musi istnieć takie $d$, że dla pewnego $l$ zachodzi $s_{l+d} = s_l$ (wynika to ze skończoności przestrzeni stanów). Minimalne $d$ o tej własności nazywane jest \textit{okresem} generatora. Dobre generatory powinny mieć jak najdłuższe okresy, optymalnie równe $|E|$.

Poniżej przedstawiamy popularne rodzaje GLP.

\subsection*{LCG}
Generatory LCG (z ang. \textit{linear congruential generator}) zmieniają swój stan zgodnie z rekurencją
\begin{equation}
 \label{eq:lcg}
 s_n = (a s_{n-1} + c) \mod M.
\end{equation}
Generator tej klasy jest określony przez moduł $M$, mnożnik $a$ oraz przyrost $c$, co oznaczamy $LCG(M,a,c)$. Zauważmy, że $LCG(M,a,c,b)$ spełnia definicję GLP z $E = \bar{M}$, $V = \bar{M}$, $f(x) = (a x + c) \mod M$ oraz $g(x) = x$.

Dobranie wartości $M, a, c$ tak by generator miał długi okres nie jest łatwym zadaniem. Z pomocą przychodzi
\begin{twier}
  Przy poniższych warunkach $LCG(M,a,c)$ ma okres równy $M$:
  \begin{itemize}
   \item $c$ oraz $M$ są względnie pierwsze,
   \item jeśli $p$ jest liczbą pierwszą i $p | M$, to $p |(a - 1)$,
   \item jeśli $4 | M$, to $4 |(a-1)$.
  \end{itemize}
\end{twier}
\noindent Powyższe twierdzenie pochodzi z pracy \cite{hull}.

Zauważmy jeszcze, że znalezienie LCG o pełnym okresie nie gwarantuje, że generator będzie dobrej jakości. Łatwo zauważyć następujący
\begin{fakt}
 Niech $M = 2^k$. Wówczas $d$ najmniej istotnych bitów $LCG(M,a,c)$ ma okres równy co najwyżej $2^d$.
\end{fakt}
\noindent W tym przypadku nie ma więc mowy o niezależności liczb generowanych przez LCG. Niektóre pakiety korzystające z LCG obchodzą ten problem zwracając tylko najbardziej znaczące bity wygenerowanych liczb.

\subsection*{MCG}
MCG (z ang. \textit{multiplicative congruential generator}) znany jest też jako GLP Lehmera lub GLP Parka-Millera. 
Jest to szczególny przypadek LCG, w którym $c = 0$, czyli kolejne stany opisuje rekurencja
\begin{equation}
 \label{eq:mcg}
 s_n = a s_{n-1} \mod M.
\end{equation}
MCG o parametrach $M$ oraz $a$ oznaczamy $MCG(M, a)$. Aby $MCG(M,a)$ mogło mieć dobre własności $M$ powinno być liczbą pierwszą lub jej potęgą, $a$ powinno być generatorem grupy $\mathbb{Z}_M^*$, a ziarno $s_0$ powinno być względnie pierwsze z $M$. 

\subsection*{GLCG}
Wyżej opisane GLP dzielą pewną wadę -- mają stosunkowo krótkie okresy. W przypadku gdy potrzebujemy dłuższych okresów przydatne mogą być uogólnione LCG (z ang. \textit{generalized linear congruential generator}). Postępują one zgodnie z rekurencją
\begin{equation}
 \label{eq:glcg}
 x_n = (a_1 x_{n-1} + a_2 x_{n-2} + \ldots + a_k x_{n-k} + c) \mod M.
\end{equation}
GLCG zmieniający stany w ten sposób oznaczamy $GLCG(M, (a_i)_{i=1}^k, c)$. Jest to ciągle GLP w myśl definicji \ref{def:glp}, gdzie $E = \bar{M}^k$, $s_n = \langle x_n, x_{n-1}, \ldots  x_{n-k+1} \rangle$, $g(s_n) = x_n$. Dobry dobór parametrów może dać okres równy $M^k-1$.

\subsection*{Generatory mieszane}
Dobrym pomysłem na ulepszenie GLP jest połączenie kilku generatorów w jeden. Załóżmy, że mamy dane $k$ GLP $\langle E_j, V_j, s_{j,0}, f_j, g_j \rangle$, $1 \leq j \leq k$, gdzie $j$-ty generator zmienia stan według zależności 
\[ s_{j,n} = f_j(s_{j,n-1}). \]
Możemy teraz zdefiniować mieszany generator w taki sposób, aby ciąg jego stanów spełniał
\[ s_n = \langle s_{1,n}, s_{1,n}, \ldots s_{k,n} \rangle\]
Niech ponadto $d_j$ oznacza okres $j$-tego ,,składowego'' generatora. Jak pokazał L'Ecuyer (\cite{lecuyer}, Lemma 2) mieszany generator ma okres $d = NWW(d_1, d_2,\ldots,d_k)$.

Szczególnym przypadkiem generatorów mieszanych są \textbf{CMCG} (z ang. \textit{combined multiplicative congruential generator}). Składa się on $k$ generatorów $MCG(M_j, a_j)$, czyli funkcją przejścia jest
\[ s_{j,n} = a_j s_{j,n-1} \mod M_j. \]
Wyjście generatora mieszanego jest
\[ g(s_n) = \left( \sum_{j=1}^{k} (-1)^{j-1} s_{j,n} \right) \mod M_1 - 1\]
Ponadto jeśli liczby $\frac{M_j-1}{2}$ są względnie pierwsze, to CMCG ma optymalny okres wynoszący $\frac{1}{2^k}(M_1-1)\cdot \ldots \cdot (M_k-1)$.

\subsection*{Mersenne Twister}
{\bigskip \color{red} \LARGE{TODO!}}


\section{Generowanie ciągów bitów}
Komputery operują tylko i wyłącznie na ciągach bitów, dlatego wyjście każdego programu, w szczególności GLP, może być traktowane jako ciąg zerojedynkowy. My potrzebujemy jednak ciągów specyficznych: każdy bit musi być generowany niezależnie i z jednakowym prawdopodobieństwem przyjmować wartości zero i jeden. Dlatego dla wygody języka wprowadźmy poniższy termin.
\begin{mydef}
 \textit{Idealnym ciągiem losowych bitów} nazywamy proces Bernoulliego z prawdopodobieństwem sukcesu $p = \frac{1}{2}$
\end{mydef}
\noindent Otrzymanie takiego ciągu z GLP nie jest tak trywialne, jak to się może w pierwszej chwili wydawać.

\begin{algorithm}
 \begin{algorithmic}[1]
  \Function{GenerujCiągBitów}{\texttt{glp}}
    \State $s \gets \epsilon$
    \While {$s$ nie jest wystarczająco długi}
      \Comment{{\color{comment} $\epsilon$ to słowo puste}}
      \State $a \gets$ następna liczba z \texttt{glp}
      \State $b \gets$ binarny zapis $a$ na $\ceil{\log_2 M}$ bitach
      \State $s \gets s \odot b$
      \Comment{{\color{comment} $\odot$ to operator konkatenacji}}
     \EndWhile
    \State \Return $s$.
  \EndFunction
 \end{algorithmic}
 \caption{Prosta metoda generowania bitów z GLP.}
 \label{alg:GenerateBitSequence}
\end{algorithm}

% GLP możemy wykorzystać do wygenerowania idealnego ciągu losowych bitów w następujący sposób:
% \begin{enumerate}
%  \item Otrzymujemy z generatora liczbę $a$.
%  \item Zapisujemy $a$ binarnie na $\ceil{\log_2 M}$ bitach.
%  \item Binarny zapis $a$ doklejamy do wyjściowego ciągu.
%  \item Powtarzamy aż do otrzymynia pożądanie długiego ciągu bitów.
% \end{enumerate}

GLP zazwyczaj wykonują obliczenia modulo $M$, gdzie $M$ jest pewną stałą ustaloną dla danego generatora. Przy pojedynczym wywołaniu generatora otrzymujemy liczbę całkowitą ze zbioru $\bar{M} = \{0, 1, \ldots, M-1 \}$. Kolejne wywołania powinny dawać niezależne wyniki. Aby wykorzystać GLP do wygenerowania idealnego ciągu losowych bitów możemy użyć Algorytmu \ref{alg:GenerateBitSequence}. Jeśli $M$ jest potęgą dwójki, to zadziała on dobrze, mamy bowiem
\begin{lemat}
 Niech $M = 2^k$. Jeżeli GLP w każdym kroku generuje liczby niezależnie i jednostajnie w zbiorze $\bar{M}$, to Algorytm \ref{alg:GenerateBitSequence} generuje idealny ciąg losowych bitów.
\end{lemat}
\begin{proof}
 W przypadku $M = 2^k$ mamy wzajemnie jednoznaczną odpowiedniość pomiędzy zbiorem $\bar{M}$ oraz układami $k$ bitów. Oznacza to, że w jednym kroku otrzymujemy z GLP każdy możliwy układ $k$ bitów z jednakowym prawdopodobieństwem $\frac{1}{2^k}$. Łatwo zauważyć, że wtedy każdy generowany bit ma równe szanse bycia jedynką i zerem, oraz jest niezależny od pozostałych.
\end{proof}

Jednak jeśli $M$ nie jest potęgą dwójki, to procedura nie działa -- przykładowo dla $M=5$ w każdym kroku doklejamy jedną z sekwencji $\{000, 001, 010, 011, 100\}$. Wówczas w wygenerowanym ciągu spotkanie jedynki jest mniej prawdopodobne niż zera -- jedynki stanowią tylko około $\frac{1}{3}$ wszystkich wygenerowanych bitów. Ponadto nie ma niezależności -- wystąpienie jedynki na bicie o indeksie podzielnym przez 3 oznacza, że kolejne dwa bity będą zerami.

Jak widać, gdy $M$ nie jest postaci $2^k$ nie możemy w wyjściowym ciągu tak po prostu umieścić binarnego zapisu wygenerowanej liczby. Moglibyśmy przykładowo w generowanym ciągu umieszczać najmniej istotny bit liczby otrzymanej z GLP. Jeśli przy każdym wywołaniu GLP generuje niezależne liczby, to uzyskane bity również będą niezależne. Ponadto prawdopodobieństwo otrzymania jedynki wyniesie w przybliżeniu \textonehalf (dla $M$ parzystego jest to dokładnie \textonehalf, dla $M$ nieparzystego $\frac{M-1}{2M}$, co jest pomijalnym odstępstwem od \textonehalf). Zatem znalezienie nieprawidłowości w tak wygenerowanym ciągu oznaczałoby, że wykorzystany GLP jest wadliwy. Istnieje jednak możliwość, że wada generatora ujawnia się na innych bitach. W takiej sytuacji tego nie wykryjemy. Mamy tu więc do czynienia z dwoma problemami. Z jednej strony chcielibyśmy w testach uwzględnić jak najwięcej cech liczb otrzymanych z GLP. Z drugiej strony wiemy, że na bardziej znaczących bitach częściej pojawiają się zera, co mogłoby prowadzić do zaburzenia wyników testów. Dlatego zazwyczaj do wyjściowego ciągu bitów bierzemy jeden bajt wygenerowanej liczby. Wydaje się to być rozsądnym kompromisem.

{\bigskip \color{red} \LARGE{TODO!} Pewnie wypada napisać tu trochę więcej}

\section{Popularne metody testowania GLP}
Poniżej przedstawiamy kilka wybranych metod testowania GLP. Lista z pewnością jest daleka od kompletności, zwłaszcza, że zagadnienie testowania GLP cieszy się dużą popularnością.

Do testów używany jest ciąg liczb wygenerowanych przez GLP. Poniżej czasem będzie nam wygodnie przyjmować, że jest to ciąg liczb całkowitych nieujemnych
\[ X_1, X_2, X_3, \ldots \]
pretendujący do miana ciągu niezależnie i równomiernie rozłożonego w zbiorze $\bar{M}$, a czasem, że jest to ciąg
\[ U_1, U_2, U_3, \ldots \]
pretendujący do miana ciągu niezależnie i równomiernie rozłożonego na odcinku $(0,1)$.

\subsection*{Zgodność z rozkładem jednostajnym}
Pierwszym nasuwającym się sposobem sprawdzenia jakości liczb generowanych przez GLP jest zastosowanie znanego aparatu statystycznego. Możemy użyć testów zgodności z rozkładem jednostajnym, np. \textbf{testu Kołmogorowa-Smirnowa}. Niech $n$ będzie długością ciągu $(U_i)$. Dystrybuanta empiryczna jest zdefiniowana jako
\[ F_n(t) = \frac{1}{n} \sum_{i=1}^n \mathbbm{1}(U_i < t). \]
Niech
\[ D_n = \sup_{0 \leq t \leq 1} |F_n(t) - F(t)|. \]
Zauważmy, że obliczenie $D_n$ nie stanowi problemu, gdyż $F_n$ jest funkcją schodkową zmieniającą wartość w punktach $U_1, U_2,\ldots$. Twierdzenie Kołmogorowa mówi, że $\sqrt{n} D_n \stackrel{D}{\Conv} K$, gdzie $K$ jest zmienną losową o rozkładzie Kołmogorowa. Korzystając z tablic lub pakietów statystycznych możemy znaleźć $p$-wartość tego testu.

Można też użyć \textbf{testu $\chi^2$ Pearsona}. Polega on na podzieleniu odcinka na $r$ części. Niech $E_i$ będzie oczekiwaną liczbą zmiennych $U_i$, których wartość wpada to $i$-tego odcinka, zaś $O_i$ obserwowaną liczbą.

Wówczas statystyka
\begin{equation}
 \label{eq:chisq}
 T = \sum_{i=1}^r \frac{(O_i - E_i)^2}{E_i} 
\end{equation}
dąży do rozkład $\chi^2$ z $(r-1)$ stopniami swobody, oznaczany $\chi^2(r-1)$. Tak jak w przypadku testu Kołmogorowa-Smirnowa możemy poznać $p$-wartość testu korzystając z odpowiednich narzędzi.

Wiele testów zgodności opracowano dla rozkładu normalnego, do najbardziej znanych należą \textbf{test Shapiro-Wilka}, \textbf{test Jarque-Bera}, \textbf{test Andersona-Darlinga}. Aby móc z nich skorzystać, wystarczy zmapować ciąg $(U_i)$ na ciąg $(N_i)$ zmiennych losowych o rozkładzie normalnym, np. przy użyciu transformacji Boxa-Mullera.

Powyższe testy sprawdzają jedynie zgodność z rozkładem jednostajnym. Teoretycznie można ponownie wykorzystać test $\chi^2$ do sprawdzenia niezależności. Dla ustalonego $t$ dzielimy ciąg $(U_i)$ na bloki 
\begin{equation}
  \label{eq:u_blok}
 (U_1,\ldots,U_t), (U_{t+1},\ldots,U_{2t}), \ldots
\end{equation}

Otrzymujemy w ten sposób obserwacje, które powinny być jednostajnie rozłożone w hiperkostce $(0,1)^t$. Możemy ją podzielić na mniejsze kostki i skorzystać ze statystyki (\ref{eq:chisq}), aby stwierdzić czy wpada do nich odpowiednio wiele obserwacji. W praktyce jednak oczekiwana liczba obserwacji w pojedynczej kostce spada tak szybko do zera, że $T$ nie jest dobrze przybliżane przez rozkład~$\chi^2$.

\subsection*{Zgodność z twierdzeniami rachunku prawdopodobieństwa}
Wiele faktów w rachunku prawdopodobieństwa opiera się na ciągach niezależnych zmiennych losowych o jednakowym rozkładzie. Dzięki temu na podstawie ciągów $(X_i)$ lub $(U_i)$ jesteśmy w stanie otrzymać kolejne zmienne losowe, których teoretyczne rozkłady są znane. W \cite{knuth} zaproponowanych jest kilka praw, które można wykorzystać w ten sposób. Oto niektóre z nich:
\begin{itemize}
 \item \textbf{Test odstępów.} Dla ustalonego przedziału $(\alpha, \beta)$ mierzymy czasy oczekiwania na kolejne $U_i$ wpadające do tego przedziału. Otrzymane wartości powinny mieć rozkład geometryczny, co sprawdzamy testem $\chi^2$.
 \item \textbf{Test permutacyjny.} Podzielmy ciąg $(U_i)$ na bloki jak w (\ref{eq:u_blok}),  przy niezbyt dużym $t$. W każdym bloku zachodzi jedno z $t!$ możliwych uporządkowań. Rozkład na uporządkowaniach powinien być jednostajny, co ponownie weryfikujemy testem $\chi^2$.
 \item \textbf{Test kolizji.} Stanowi rozwiązanie, gdy mamy $n$ obserwacji wpadających $m$ ,,pudełek'', przy czym $n < m$. Jak powiedzieliśmy wcześniej, w takiej sytuacji nie możemy zastosować testu $\chi^2$. Jednak da się wyliczyć teoretyczne prawdopodobieństwo otrzymania $k$ kolizji (kolizją jest trafienie obserwacji do pudełka, w którym jest już inna obserwacja). Jeśli zaobserwowana liczba kolizji nie mieści się w pewnych ramach, to możemy stwierdzić, że ciąg nie jest losowy.
\end{itemize}
Metody opisane tutaj mają pewną zaletę w stosunku do metod z poprzedniego paragrafu -- niejawnie testują również niezależność.

\subsection*{Test spektralny}
Testowi spektralnemu Donald Knuth poświęcił kilkanaście stron w swoim dziele \cite{knuth}, czego nie sposób tutaj streścić. Idea polega na spostrzeżeniu, że punkty w $t$-wymiarowej przestrzeni, utworzone z kolejnych wyrazów ciągu $(U_i)$ wygenerowanego przez LCG, leżą na stosunkowo niewielkiej liczbie $(t-1)$-wymiarowych hiperpłaszczyzn. Zagłębiając się w ten temat można dojść do dość skomplikowanej metody testowania LCG. Jednak w niektórych przypadkach widać gołym okiem, że generator jest zły. Takim przykładem jest niechlubny RANDU (jest to $MCG(2^{31}, 2^{16}+3)$). Na Rysunku \ref{fig:spectral} przedstawiono punkty w przestrzeni otrzymane z tego generatora. Widać wyraźnie, że układają się one na 15 płaszczyznach.
\begin{figure}[!ht]
 \centering
 \includegraphics[scale=0.6]{obrazki/spectraltest.pdf}
 \caption{Ilustracja rozmieszczenia w przestrzeni punktów generowanych przez RANDU. Każdy punkt został utworzony z trzech kolejnych liczb otrzymanych z generatora i przeskalowanych na odcinek $(0,1)$}
 \label{fig:spectral}
\end{figure}

\subsection*{Złożoność Kołmogorowa}
Zacznijmy od przykładu. Rozważmy ciągi binarne długości 32
\begin{nscenter}
\texttt{01010101010101010101010101010101} 
\end{nscenter}
oraz
\begin{nscenter}
\texttt{00110111001101001101101000110110}. 
\end{nscenter}
Choć wylosowanie obydwu z nich jest równie prawdopodobne, ten drugi uznajemy za bardziej ,,losowy''. Dzieje się tak dlatego, że pierwszy ciąg można jednoznacznie opisać w znacznie mniejszej liczbie znaków:
\begin{nscenter}
\texttt{16$\times$01},
\end{nscenter}
zaś najkrótszym opisem drugiego ciągu jest prawdopodobnie przepisanie go całego.

Tego typu intuicje próbujemy wyjaśniać za pomocą, tzw. złożoności Kołmogorowa. Służy ona do mierzenia stopnia skomplikowania ciągów znaków. Ustalmy dowolny język programowania $L$. Złożonością Kołmogorowa łańcucha znaków $s$ jest długość najkrótszego programu $P$ w języku $L$, takiego że $P$ wypisuje $s$.

Rzadko kiedy jesteśmy w stanie znaleźć dokładną wartość złożoności łańcucha $s$. Dlatego może się wydawać, że powyższe podejście jest czysto teoretyczne. Okazuje się jednak, że można złożoność $s$ oszacować. Niech $P$ będzie programem implementującym ustalony algorytm kompresji, a $\tilde{P}$ odpowiadającym mu programem dekompresji. Ponadto niech $\tilde{s}$ będzie skompresowanym łańcuchem $s$. Wówczas złożoność łańcucha $s$ jest oszacowana z góry przez sumę długości $\tilde{P}$ oraz $\tilde{s}$. Odrzucamy hipotezę o losowości $s$, gdy otrzymana wartość jest znacznie mniejsza od długości~$s$.

\subsection*{Zagadnienie ruiny gracza}
Bardzo interesująca metoda testowania GLP została przedstawiona w pracy \cite{kim-choe}. Rozważane jest błądzenie po grupie $\mathbb{Z}_n$. Autorzy badają czas dojścia do stanu $0$, co odpowiada zbiciu majątku wysokości $n$ lub bankructwu gracza ze znanego zagadnienia. W pracy opisane są trzy warianty zastosowanej metody, tutaj przedstawiamy podstawową z nich.

Ustalmy $p \in (0,1)$. Ciąg $(U_i)$ otrzymany z generatora wykorzystywany jest do poruszania się po grupie $\mathbb{Z}_n$. Jeśli w kroku $i$-tym byliśmy w stanie $s$, to przechodzimy do stanu $s+1$, gdy $U_{i+1} < p$, a do $s-1$ w przeciwnym przypadku. Dla każdego $x \in \mathbb{Z}_n$, $x \neq 0$ niech $T_x$ oznacza czas dojścia do $0$ ze stanu $x$. $N$ krotnie rozpoczynamy błądzenie po grupie z punktu $x$. W ten sposób otrzymujemy $N$ replikacji zmiennej $T_x$. Oznaczmy ich średnią przez $\overline{T}_x$. Niech $\mu$ oraz $\sigma^2$ oznaczają wartość oczekiwaną oraz wariancję $T_x$, gdy prawdopodobieństwa przejść do stanów $s+1$ oraz $s-1$ wynoszą odpowiednio $p$ oraz $1-p$. Niech
\[ Z_x = \frac{\overline{T}_x - \mu}{\sigma \sqrt{N}}. \]
Przy założeniu hipotezy, o niezależności i rozkładzie jednostajnym zmiennych $(U_i)$, statystyka $Z_x$ ma w przybliżeniu rozkład normalny $\mathcal{N}(0,1)$. W związku z tym przy dużych wartościach $|Z_x|$ należy stwierdzić, że generator nie przeszedł testu w punkcie $x$. 


\subsection*{NIST}


\subsection*{Błądzenie przypadkowe}
Testom opartym na własnościach błądzenia przypadkowego poświęcone są kolejne dwie części pracy.

\chapter{Metoda testowania}
\label{czesc:metoda}
W części \ref{czesc:bladzenie} przedstawiliśmy teorię przydatną do testowania GLP. W tej części pokazujemy jak zastosować ją w praktyce.
Na początek omawiamy jak wykorzystać GLP do generowania sekwencji bitów. Następnie opisujemy w jaki sposób sprawdzić czy wygenerowane ciągi odpowiadają prawdziwie losowym realizacjom błądzenia przypadkowego. Przedstawiona tu metoda oparta o prawo iterowanego logarytmu pochodzi z pracy \cite{wang-nic}. Dodatkowo proponujemy nową metodę opartą o prawo arcusa sinusa. W wykonywanych obliczeniach często polegamy na aproksymacjach, dlatego w ostatnim paragrafie tej części analizujemy wielkość popełnianego błędu.



\section{Zastosowanie błądzenia przypadkowego do testowania GLP}

Ogólna idea testów GLP, którą stosujemy w tej pracy nie jest skomplikowana. Metoda polega na obliczeniu pewnych charakterystyk ciągów wygenerowanych przez GLP i porównaniu ich empirycznych rozkładów z rozkładami, które są znane dla idealnego ciągu losowych bitów.

Przykładowo, w części poświęconej prawu arcusa sinusa uzasadniliśmy, że bardziej prawdopodobna jest długa przewaga liczby jedynek nad liczbą zer niż równomierny rozkład prowadzenia. Jeśli generator sztucznie wyrównuje liczby zer i jedynek, to zauważymy odstępstwa od tej reguły. Zgodność z prawem arucsa sinusa sprawdzają testy oparte o zdefiniowaną poniżej charakterystykę $\Sasin{n}$.

Podobnie ktoś mógłby pomyśleć, że czymś pozytywnym byłyby niewielkie różnice między liczbą jedynek i zer w ciągu bitów otrzymanym z GLP. Moglibyśmy zdecydować się na jakąś ,,rozsądną'' stałą, powiedzmy 100, i uznać, że generator jest dobry jeśli różnica liczby zer i jedynek w ciągu nie przekroczy 100. Wszak duże różnice mogłyby sugerować, że mamy różne prawdopodobieństwa wystąpienia zer i jedynek. Jednak prawo iterowanego logarytmu pokazuje, że to rozumowanie jest błędne. \emph{Należy} spodziewać się fluktuacji i odstępstw od zera, a ich brak oznacza, że GLP nie generuje idealnego ciągu losowych bitów. Tę obserwację wykorzystują testy opartę o charakterystykę $\Slil{n}$.


\subsection{Testy oparte o prawo arcusa sinusa}
Niech
\begin{equation}
D_k = \mathbbm{1}\left(S_k > 0 \vee S_{k-1}>0 \right), k=1,2,\ldots,n.
\end{equation}
Zmienna $D_k$ przyjmuje wartość 1, gdy w $k$-tym kroku błądzenia przypadkowego zachodzi przewaga liczby jedynek nad zerami (traktując remisy tak jak w paragrafie \ref{par:asin}), zaś 0 w przeciwnym przypadku. Zdefiniujmy charakterystykę $\Sasin{n}$ wzorem
\begin{equation}
 \Sasin{n} = \frac{1}{n} \sum_{k=1}^n D_k.
\end{equation}
Zatem $\Sasin{n}$ jest to frakcja czasu podczas której jedynki dominowały nad zerami. Wiemy z paragrafu \ref{par:asin}, że
\[ \Pro{\Sasin{n} = \frac{k}{n}} = p_{k,n}, \]
zaś korzystając z prawa arcusa sinusa
\begin{equation}
\begin{split}
 \label{eq:prob_sasin}
  \Pro{\Sasin{n} \in (a,b)} &\xrightarrow[n \conv \infty]{} \int_a^b \frac{dt}{\sqrt{t(1-t)}}\\
  &= \frac{2}{\pi}\arcsin(\sqrt{b}) - \frac{2}{\pi}\arcsin(\sqrt{a}).
\end{split}
\end{equation}

W celu przetestowania GLP generujemy przy jego użyciu $m$ ciągów zerojedynkowych długości~$n$. Otrzymujemy w ten sposób $m$ realizacji zmiennej $\Sasin{n}$, $j$-tą replikację oznaczamy $\Sasin{{n,j}}$. Ustalamy partycję prostej rzeczywistej i dla każdego odcinka w tej partycji zliczamy ile realizacji $\Sasin{n}$ do niego wpadło.

W testach opartych o wielkość $\Sasin{n}$ korzystamy z $(s+2)$-elementowej partycji postaci $\mathcal{P}^{asin}_s = \{ P^{asin}_0, P^{asin}_1, \ldots, P^{asin}_{s+1}\}$, gdzie
\begin{equation*}
\begin{split}
  P^{asin}_0 &= \left(-\infty, -\frac{1}{2s}\right),\\
  P^{asin}_i &= \left[\frac{2i-3}{2s}, \frac{2i-1}{2s} \right),\ \ \ 1 \leq i \leq s,\\
  P^{asin}_{s+1} &= \left[1- \frac{1}{2s}, \infty\right).
\end{split}
\end{equation*}

Możemy teraz zdefiniować dwie miary określone na partycji  $\mathcal{P}^{asin}_s$. Pierwsza z nich, $ \mu^{asin}_n$, reprezentuje teoretyczny rozkład rozważanej charakterystyki:
\begin{equation}
 \mu^{asin}_n \left( P^{asin}_i \right) = \Pro{\Sasin{n} \in P^{asin}_i},\ \ \ 0 \leq i \leq s+1.
\end{equation}
Powyższą wartość możemy wyliczyć ze wzoru (\ref{eq:prob_sasin}).

Druga miara, $\nu^{asin}_n$,  reprezentuje rozkład empiryczny, wyznaczony w testach. Określamy
\begin{equation}
 \nu^{asin}_n \left( P^{asin}_i \right) = \frac{|\{ j:\ \Sasin{{n,j}} \in  P^{asin}_i, 1 \leq j \leq m\}|}{m},\ \ \ 0 \leq i \leq s+1.
\end{equation}


Jeżeli testowany GLP jest dobry, to obie miary powinny być ,,mniej więcej takie same''. Ściślej, odległość między otrzymanymi miarami powinna być mała. Korzystamy ze znanych funkcji odległości \textit{total variation distance} oraz \textit{separation distance}. Są one zdefiniowane następująco dla dowolnych miar $\mu$ oraz $\nu$:
\begin{align}
 d^{tv}(\mu, \nu) &= \sup_{A \subseteq \mathbb{R}} |\mu(A) - \nu(A)|,\\
 d^{sep}(\mu, \nu) &= \sup_{A \subseteq \mathbb{R}} \left(1 - \frac{\mu(A)}{\nu(A)}\right)
\end{align}
Ponieważ nie jesteśmy w stanie wyznaczyć supremum na całej prostej, skorzystamy z nieco uproszczonych definicji. Dla partycji $\mathcal{P}$ prostej rzeczywistej, definiujemy:
\begin{align}
 d^{tv}_\mathcal{P}(\mu, \nu) &= \frac{1}{2} \sum_{A \in \mathcal{P}} |\mu(A) - \nu(A)|
    = \sum_{\substack{A \in \mathcal{P},\\ \mu(A) >\nu(A)}} \Big(\mu(A) - \nu(A)\Big)\\
 d^{sep}_\mathcal{P}(\mu, \nu) &= \sup_{A \in \mathcal{P}}\left(1 - \frac{\mu(A)}{\nu(A)}\right)
\end{align}

Inne podejście polega na testowaniu hipotezy o zgodności rozkładów. Korzystając z terminologii statystycznej wielkości $\Sasin{{n,j}}$ będziemy nazywać obserwacjami. Hipotezą zerową jest stwierdzenie, że obserwacje mają rozkład $\mu^{asin}_n$, czyli de facto, że GLP generuje idealny ciąg losowych bitów. Niech $O_i$ oznacza liczbę obserwacji wpadających do przedziału $P^{asin}_i$, tzn.
\begin{equation}
 O_i = |\{ j:\ \Slil{{n,j}} \in  P^{asin}_i, 1 \leq j \leq m\}|,\ \ \ 0 \leq i \leq s+1,
\end{equation}
oraz $E_i$ oznacza \textit{oczekiwaną} liczbę obserwacji wpadających do tego przedziału, czyli
\begin{equation}
 E_i = m \cdot \Pro{\Sasin{n} \in P^{asin}_i},\ \ \ 0 \leq i \leq s+1.
\end{equation}
Wartość $E_i$ znamy dokładnie ze wzoru (\ref{eq:prob_sasin}). Przy hipotezie zerowej statystyka
\begin{equation}
 T = \sum_{i=0}^{s+1} \frac{(O_i - E_i)^2}{E_i}
\end{equation}
ma w przybliżeniu rozkład chi-kwadrat z $s+1$ stopniami swobody, oznaczany $\chi^2(s+1)$. Duże wartości tej statystyki są dowodem wadliwości GLP.


\subsection{Testy oparte o prawo iterowanego logarytmu}
Przyjrzyjmy się teraz metodzie zastosowanej w \cite{wang-nic}. Jest ona podobna do metody opisanej w poprzednim paragrafie. Liczymy jedynie inną charakterystykę ciągów i dostosowujemy partycję prostej. Przypomnijmy oznaczenie
\begin{equation}
\Slil{n} = \frac{S_n}{\sqrt{2n \log \log n}}.
\end{equation}
Można łatwo znaleźć teoretyczny rozkład tej charakterystyki (tzn. rozkład dla idealnego ciągu losowych bitów). Istotnie, korzystając z centralnego twierdzenia granicznego dostajemy
\begin{equation}
\begin{split}
  \label{eq:prob_slil}
 \Pro{\Slil{n} \in (a,b)} &= \Pro{\frac{S_n}{\sqrt{n}} \in \left(a\sqrt{2 \log \log n},  b\sqrt{2 \log \log n}\right)}\\
 &\approx \Phi(b\sqrt{2 \log \log n}) - \Phi(a\sqrt{2 \log \log n})
\end{split}
\end{equation}
Jako, że $\Slil{n}$ przyjmuje swoje wartości w szerszym przedziale niż $\Sasin{n}$, dlatego korzystamy z innej partycji prostej, mianowicie $\mathcal{P}^{lil}_s = \{ P^{lil}_0, P^{lil}_1, \ldots, P^{lil}_{s+1}\}$, gdzie
\begin{equation*}
\begin{split}
  P^{lil}_0 &= (-\infty, -1),\\
  P^{lil}_i &= \left[-1 + \frac{2(i-1)}{s}, -1 + \frac{2i}{s} \right),\ \ \ 1 \leq i \leq s,\\
  P^{lil}_{s+1} &= [1, \infty).
\end{split}
\end{equation*}
Teoretyczny i empiryczny rozkład określamy w tym przypadku następująco:
\begin{equation}
 \label{eq:mu_lil}
 \mu^{lil}_n \left( P^{lil}_i \right) = \Pro{\Slil{n} \in P^{lil}_i},\ \ \ 0 \leq i \leq s+1,
\end{equation}
\begin{equation}
 \nu^{lil}_n \left( P^{lil}_i \right) = \frac{|\{ j:\ \Slil{{n,j}} \in  P^{lil}_i, 1 \leq j \leq m\}|}{m},\ \ \ 0 \leq i \leq s+1,
\end{equation}
przy czym wartość (\ref{eq:mu_lil}) znamy dzięki (\ref{eq:prob_slil}), zaś $\Slil{{n,j}}$ oznacza oczywiście replikacje $\Slil{n}$, obliczone dla kolejnych ciągów.

\section{Analiza błędów}


\chapter{Testy}
\label{czesc:testy}

\section{Uwagi praktyczne}

Każdy GLP testowaliśmy wykorzystując go do wygenerowania $m = 10000$ ciągów długości $n = 2^{34}$ bitów. Używaliśmy partycji $\mathcal{P}^{lil}_{40}$ dla testów korzystających z charakterystyki $\Slil{n}$ i partycji $\mathcal{P}^{asin}_{40}$ dla testów korzystających z charakterystyki $\Sasin{n}$.

\section{Wyniki}
% \begin{table}[ht!]
% \centering
%  \caption{Wyniki dla $\Sasin{n}$.}
%  \label{tab:xxx_asin}
% \begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
%  \hline
%      n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
% 
%  
% \end{tabular}  
% \end{table}
% \begin{table}[ht!]
% \centering
%  \caption{Wyniki dla $\Slil{n}$.}
%  \label{tab:xxx_lil}
% \begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
%  \hline 
%      n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
%  
% \end{tabular}  
% \end{table}

\subsection{Bilioteczny rand w Borland C}

\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Sasin{n}$.}
 \label{tab:borland_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &  0.038 &   0.05 &   0.07 &  0.086 &  0.139 &  0.156 &  0.123 &  0.147 &  0.215\\ \hline
  sep1 &  0.129 &  0.184 &  0.251 &  0.373 &  0.549 &  0.515 &   0.57 &  0.687 &  0.822\\ \hline
  sep2 &  0.202 &  0.212 &   0.21 &  0.314 &   0.37 &  0.373 &   0.38 &  0.517 &  0.401\\ \hline
 p-val &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0\\ \hline


 
\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Slil{n}$.}
 \label{tab:borland_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &  0.049 &  0.084 &  0.124 &  0.182 &   0.31 &  0.407 &  0.465 &   0.59 &  0.941\\ \hline
  sep1 &  0.599 &  0.708 &    0.9 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0\\ \hline
  sep2 &  0.179 &  0.233 &  0.276 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0\\ \hline
 p-val &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0\\ \hline

 
\end{tabular}  
\end{table}

\FloatBarrier
\subsection{Biblioteczny rand w moim kompilatorze}

\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Sasin{n}$.}
 \label{tab:mojkomp_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &  0.029 &  0.022 &  0.026 &  0.023 &   0.02 &  0.029 &  0.024 &  0.025 &  0.023\\ \hline
  sep1 &  0.116 &  0.125 &  0.163 &  0.119 &  0.166 &  0.172 &  0.131 &  0.132 &   0.14\\ \hline
  sep2 &  0.175 &  0.191 &  0.144 &  0.187 &  0.111 &  0.134 &  0.179 &  0.126 &  0.188\\ \hline
 p-val &  0.065 &  0.551 &  0.257 &  0.489 &  0.797 &  0.111 &  0.588 &   0.26 &  0.393\\ \hline


 
\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Slil{n}$.}
 \label{tab:mojkomp_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &  0.021 &  0.029 &  0.029 &   0.03 &  0.023 &  0.023 &  0.034 &  0.024 &  0.022\\ \hline
  sep1 &  0.109 &   0.11 &  0.103 &  0.158 &  0.152 &  0.145 &  0.258 &  0.399 &  0.256\\ \hline
  sep2 &  0.322 &  0.343 &  0.326 &  0.322 &  0.252 &  0.284 &  0.376 &   0.22 &   0.14\\ \hline
 p-val &  0.576 &  0.012 &  0.005 &  0.003 &  0.274 &  0.237 &    0.0 &  0.469 &   0.79\\ \hline

 
\end{tabular}  
\end{table}

\FloatBarrier
\subsection{BSD rand}

\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Sasin{n}$.}
 \label{tab:bsd_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &  0.213 &  0.213 &  0.213 &  0.213 &  0.213 &  0.213 &  0.213 &  0.213 &  0.213\\ \hline
  sep1 &  0.826 &  0.826 &  0.826 &  0.826 &  0.826 &  0.826 &  0.826 &  0.826 &  0.826\\ \hline
  sep2 &  0.436 &  0.436 &  0.436 &  0.436 &  0.436 &  0.436 &  0.436 &  0.436 &  0.436\\ \hline
 p-val &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0\\ \hline


 
\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Slil{n}$.}
 \label{tab:bsd_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &   0.95 &   0.95 &   0.95 &   0.95 &   0.95 &   0.95 &   0.95 &   0.95 &   0.95\\ \hline
  sep1 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0\\ \hline
  sep2 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0\\ \hline
 p-val &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0\\ \hline

 
\end{tabular}  
\end{table}
\FloatBarrier
\subsection{Minstd}

\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Sasin{n}$.}
 \label{tab:minstd_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &  0.033 &  0.033 &  0.053 &  0.072 &  0.086 &  0.104 &   0.16 &  0.202 &  0.208\\ \hline
  sep1 &  0.151 &  0.134 &  0.184 &  0.241 &  0.223 &  0.667 &  0.494 &   0.71 &  0.828\\ \hline
  sep2 &  0.175 &  0.149 &  0.265 &  0.373 &  0.398 &  0.379 &  0.369 &  0.505 &  0.422\\ \hline
 p-val &  0.005 &  0.017 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0\\ \hline


 
\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Slil{n}$.}
 \label{tab:minstd_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &  0.029 &  0.051 &  0.076 &  0.095 &  0.181 &  0.205 &   0.35 &  0.316 &    0.9\\ \hline
  sep1 &  0.556 &  0.453 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0\\ \hline
  sep2 &  0.108 &  0.307 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0 &    1.0\\ \hline
 p-val &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0 &    0.0\\ \hline

 
\end{tabular}  
\end{table}
\FloatBarrier
\subsection{CMRG}

\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Sasin{n}$.}
 \label{tab:cmrg_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &  0.019 &  0.023 &  0.024 &  0.024 &  0.024 &  0.023 &  0.021 &  0.027 &  0.027\\ \hline
  sep1 &  0.113 &  0.209 &  0.119 &  0.145 &   0.15 &  0.119 &  0.123 &  0.159 &  0.208\\ \hline
  sep2 &   0.14 &  0.094 &  0.116 &  0.134 &  0.193 &  0.167 &  0.093 &  0.135 &  0.116\\ \hline
 p-val &  0.976 &  0.466 &  0.764 &  0.799 &  0.383 &  0.682 &  0.985 &  0.204 &  0.161\\ \hline


 
\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Slil{n}$.}
 \label{tab:cmrg_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
    tv &  0.031 &  0.026 &   0.03 &  0.022 &  0.026 &  0.026 &  0.033 &  0.023 &  0.023\\ \hline
  sep1 &  0.108 &  0.256 &  0.173 &  0.169 &   0.27 &  0.139 &  0.296 &  0.317 &  0.283\\ \hline
  sep2 &  0.469 &  0.326 &  0.263 &  0.225 &  0.266 &  0.199 &  0.266 &  0.266 &  0.176\\ \hline
 p-val &    0.0 &  0.001 &  0.007 &  0.386 &  0.207 &  0.296 &  0.007 &  0.387 &  0.689\\ \hline

 
\end{tabular}  
\end{table}
\FloatBarrier
\section{Podsumowanie}


\begin{thebibliography}{99}
 \bibitem{asmussen}
    S. Asmussen, P. Glynn, \emph{Stochastic Simulation: Algorithms and Analysis}, Springer, New York, 2007
 \bibitem{feller}
    W. Feller, \emph{Wstęp do rachunku prawdopodobieństwa}, PWN, Warszawa, Wydanie piąte, 1987
 \bibitem{hull}
    T. Hull, A. Dobell, \emph{Random number generators}, SIAM Rev, v.4, 1962, s. 230-254
 \bibitem{jak-szt}
    J. Jakubowski, R. Sztencel, \emph{Wstęp do teorii prawdopodobieństwa}, Script, Warszawa, Wydanie IV, 2010
 \bibitem{kim-choe}
    C. Kim, G.H. Choe, D.H. Kim, \emph{Tests of randomness by the gambler's ruin algorithm},  Applied Mathematics and Computation 199, 2008, s. 195-210
 \bibitem{knuth}
    D. Knuth, \emph{The Art of Computer Programming volume 2: Seminumerical algorithms (2nd ed.)}, Addison-Wesley, 1981
  \bibitem{lecuyer}
    P. L'Ecuyer, \emph{Efficient and Portable Combined Random Number Generator}, Communications of the ACM 31, 1988, s. 742–749, 774.
 \bibitem{wang-nic}
    Y. Wang, T. Nicol, \emph{On Statistical Distance Based Testing of Pseudo Random Sequences and Experiments with PHP and Debian OpenSSL}, Computers \& Security, 53, 44--64, 2015


 \bibitem{rorg}
    www.random.org
\end{thebibliography}



\end{document}
