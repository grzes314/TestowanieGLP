\documentclass[a4paper,11pt,twoside]{book}
\usepackage[left=2.5cm,right=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
%\usepackage[polish]{babel}
\usepackage{polski}
\usepackage{amsmath, amsthm, amssymb} %Rozne matematyczne symbole
\usepackage{graphicx} %Zalaczanie obrazkow
\usepackage{titlesec} 
\usepackage{color}
\usepackage{array}
\usepackage{wrapfig} % Opływające obrazki
\usepackage[chapter]{algorithm} % allows to keep algorithms as floats
\usepackage{algpseudocode} % allows writing pseudocodes
\usepackage{textcomp} % symbol 1/2
\usepackage{bbm} %jedineczka
\usepackage[section]{placeins} % keeps floats in their places
\usepackage{fancyhdr}


%\addto\captionsenglish{
%  \renewcommand\chaptername{}}
\renewcommand\chaptername{Część}
\titleformat{\chapter}[display]
  {\normalfont\Large\filcenter\sffamily}
  {\titlerule[1pt]%
   \vspace{1pt}%
   \titlerule
   \vspace{1pc}%
   \LARGE\MakeUppercase{\chaptertitlename} \Roman{chapter}
  }
  {1pc}
  {\titlerule
  \vspace{1pc}%
  \Huge}
%\renewcommand\thechapter{\Roman{chapter}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% Ustawienia środowika algorytmów %%%%%%%%%%%%%%%
\definecolor{comment}{RGB}{96,96,192}
\definecolor{colorForKeyWord}{RGB}{165,42,42}
\makeatletter
 \renewcommand{\ALG@name}{Algorytm} 
\makeatother  
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndFor}
\algtext*{EndFunction}
\renewcommand{\algorithmicif}{\textbf{jeżeli}}
\renewcommand{\algorithmicthen}{\textbf{to}}
\renewcommand{\algorithmicfor}{\textbf{dla}}
\renewcommand{\algorithmicwhile}{\textbf{dopóki}}
\renewcommand{\algorithmicdo}{\textbf{wykonuj}}
\renewcommand{\algorithmicreturn}{\textbf{zwróć}}
\renewcommand{\algorithmicfunction}{\textbf{procedura}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{N}{@{}m{0pt}@{}}

\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Pro}[1]{\mathbb{P}\left(#1\right)}
\newcommand{\conv}{\rightarrow}
\newcommand{\Conv}{\longrightarrow}
\newcommand{\ceil}[1]{\lceil#1\rceil}
\newcommand{\Sg}[1]{S^*_#1}
\newcommand{\Slil}[1]{S^{lil}_#1}
\newcommand{\Sasin}[1]{S^{asin}_#1}
\newcommand{\norm}[2]{\mathcal{N}\left(#1, #2\right)}
  
\newtheorem{twier}{Twierdzenie}[chapter]
\newtheorem{lemat}[twier]{Lemat}
\newtheorem{fakt}[twier]{Fakt}
\theoremstyle{definition}
\newtheorem{mydef}{Definicja}[chapter]


\newenvironment{nscenter}
 {\parskip=0pt\par\nopagebreak\centering}
 {\par\noindent\ignorespacesafterend}

 
 
\title{Zastosowanie błądzenia przypadkowego do testowania generatorów liczb pseudolosowych}
\author{Grzegorz Łoś}

\pagestyle{fancy}
\fancyhf{}
\fancyhead[RO]{\small\bfseries\thepage}
\fancyhead[LE,RO]{\small\bfseries\thepage} %do odkomentowania w wersji dwustronnej
\fancyhead[LO]{\small\bfseries\nouppercase\rightmark}
\fancyhead[RE]{\small\bfseries\nouppercase\leftmark} %do odkomentowania w wersji dwustronnej

\begin{document}

\thispagestyle{empty}
\begin{center}
\textbf{\large Uniwersytet Wrocławski\\
Wydział Matematyki i Informatyki\\
Instytut Matematyczny}\\
\vspace{4cm}
\textbf{\textit{\large Grzegorz Łoś}\\
\vspace{0.5cm}
{\Large Zastosowanie błądzenia przypadkowego do testowania generatorów liczb pseudolosowych}}\\
\end{center}
\vspace{3cm}
{\large \hspace*{6.5cm}Praca magisterska\\
\hspace*{6.5cm}napisana pod kierunkiem\\
\hspace*{6.5cm}dr. Pawła Lorka }\\
\vfill
\begin{center}
{\large Wrocław, 26 sierpnia 2015}\\
\end{center}

\newpage
\thispagestyle{empty}
\begin{minipage}{0.5\linewidth}
\end{minipage}

\newpage

% \begin{minipage}{0.8\linewidth}
% \maketitle
% \end{minipage}

\begin{minipage}{0.8\linewidth}
\tableofcontents
\end{minipage}

\chapter*{Wprowadzenie}
\addcontentsline{toc}{chapter}{\bfseries Wprowadzenie}
Wiele współczesnych technologii opiera się na randomizacji. W informatyce losowość pojawia się na każdym kroku i często nie zdajemy sprawy jak bardzo jesteśmy od niej uzależnieni. Programiści korzystają z niej na co dzień, często zupełnie nieświadomie, na przykład używając bibliotecznych implementacji algorytmu quicksort lub tablic haszujących. Randomizacja jest niezbędnym elementem w wielu innych specjalistycznych dziedzinach. Przykładowo w finansach ważną rolę odgrywają metody Monte Carlo polegające na wielokrotnej symulacji rozwoju rynku. Metody optymalizacji oparte o metaheurystyki lub algorytmy ewolucyjne nie miałyby bez losowości racji bytu.

Podane wyżej przykłady mają pewną wspólną cechę: drobne wady generatora liczb pseudolosowych (GLP), na których oparte są wspomniane metody, mogą obniżyć efektywność działania lub dokładność wyników, ale nie rujnują algorytmów całkowicie. Są jednak dziedziny, w których jakość GLP ma zasadnicze znaczenie. Dobrym przykładem jest kryptografia. Zauważalne odstępstwa od losowości mogą istotnie zwiększyć szanse złamania protokołu kryptograficznego, odkrycia klucza prywatnego, itp. Wynika stąd potrzeba zidentyfikowania tych GLP, na których można polegać.

Wyjście GLP jest po prostu ciągiem binarnych danych. Rozstrzygnięcie czy dany GLP jest wystarczająco solidny sprowadza się do odpowiedzi na pytanie czy jego wyjście jest nieodróżnialne od ciągu prawdziwie losowego. Interpretując wygenerowane bity jako +1 oraz -1, możemy łatwo zobaczyć, że wyjście GLP odpowiada realizacji błądzenia przypadkowego. Ten proces stochastyczny jest dobrze zbadany i opisany w literaturze (np. w \cite{feller}). Znamy wiele jego własności. Testowanie GLP polega na sprawdzeniu czy jego wyjście również je posiada.

W części \ref{czesc:bladzenie} opisujemy te własności błądzenia przypadkowego, które przydadzą się w dalszej części pracy. Autorzy \cite{wang-nic} zauważyli użyteczność prawa iterowanego logarytmu do testowania generatorów. W niniejszej pracy proponujemy metodę testowania generatorów opartą o prawo arcusa sinusa.

Część \ref{czesc:metoda} opisujemy dokładniej jak wykorzystać przytoczone prawa do testowania GLP. Postępujemy nieco inaczej niż w statystyce matematycznej, choć idea jest podobna. Uruchamiamy GLP $m$ razy (każdorazowo z innym ziarnem!). Na podstawie każdego ciągu zerojedynkowego otrzymanego z GLP (lub patrząc inaczej: na podstawie każdej realizacji błądzenia przypadkowego) obliczamy wartość pewnej funkcji. Znamy teoretyczny rozkład tej wartości i możemy obliczyć jego odległość od rozkładu otrzymanego empirycznie. Jeśli ta odległość jest duża, to możemy powiedzieć, że GLP jest niskiej jakości, w przeciwnym razie nie ma podstaw by go zdyskredytować.

Wyniki testów kilku znanych GLP przedstawione są w części \ref{czesc:testy}

{\bigskip \color{red} \LARGE{TODO!} Rozdmuchać wstęp}


\chapter{Błądzenie przypadkowe}
\label{czesc:bladzenie}

Jak napisaliśmy we wprowadzeniu, własności błądzenia przypadkowego (inaczej: losowego) będą kluczowe dla testowania GLP. Zebrane w tej części wiadomości opracowane są na podstawie \cite{feller}. Większość oznaczeń jest również wzorowane na tej książce.

Wyjście generowane przez GLP zawsze można traktować jako ciąg zerojedynkowy. Dlatego ważnym pojęciem będzie dla nas ciąg niezależnych prób Bernoulliego (inaczej: proces Bernoulliego) $(B_i)_{i \in \mathbb{N}}$. Dla ustalonego $p \in [0,1]$ oznaczamy w ten sposób ciąg niezależnych zmiennych losowych o jednakowym rozkładzie, taki że
\[ \Prob(B_1 = 1) = p = 1 - \Prob(B_1 = 0). \]
Możemy postrzegać $i$-ty bit wygenerowany przez GLP jako wynik $i$-tej próby Bernoulliego. Dobry generator powinien z takim samym prawdopodobieństwem losować 0 oraz 1, dlatego ograniczymy się do przypadku $p = \frac{1}{2}$.

Często będzie nam wygodniej posługiwać się ciągiem prób $(X_i)_{i \in \mathbb{N}}$, który przyjmuje wartości -1 zamiast 0, czyli
\[ X_i \stackrel{D}{=} 2 B_i -1. \]
Ponadto oznaczmy
\[ S_n = \sum_{i=1}^{n} X_i. \]
Tak zdefiniowany proces $(S_i)_{i \in \mathbb{N}}$ jest nazywany błądzeniem przypadkowym. Ciąg ten w każdym kolejnym kroku zmienia swoją wartość o 1 lub -1. Czasem wygodnie jest go postrzegać jako wynik następującej gry. Dwóch graczy rzuca idealną monetą. Jeśli wypada orzeł, to pierwszy gracz otrzymuję złotówkę od drugiego, w przeciwnym przypadku pierwszy płaci złotówkę drugiemu. Proces $S$ przedstawia zysk ustalonego gracza.

\begin{figure}[ht]
 \centering
 \includegraphics[scale=0.4]{obrazki/rw.pdf}
 \caption{Przykładowe trajektorie procesu $S$.}
 \label{fig:bladzenie}
\end{figure}

Sporo miejsca w rachunku prawdopodobieństwa poświęcono badaniu własności błądzenia przypadkowego, z których dwie omawiamy poniżej. Ideą testów, które przedstawiamy w części~\ref{czesc:metoda} jest sprawdzanie czy wyjście GLP zachowuje się tak jak to wynika z praw rachunku prawdopodobieństwa.


\section{Prawo iterowanego logarytmu}
Jest jasne, że $|S_n| \leq n$. Można się jednak domyślać, choćby na podstawie rysunku \ref{fig:bladzenie}, że duże wartości $|S_n|$ są jednak bardzo mało prawdopodobne i w praktyce z dużym prawdopodobieństwem wartości $S_n$ znajdą się w znacznie węższym przedziale niż $[-n, n]$. Słabe i mocne prawo wielkich liczb mówią nam, że
\[\frac{S_n}{n} \stackrel{\Prob}{\rightarrow} 0 \hbox{, a nawet } \frac{S_n}{n} \stackrel{p.n.}{\rightarrow} 0.\] Jest więc jasne, że odchylenia procesu $S$ od zera rosną znacznie wolniej niż liniowo. Z drugiej strony centralne twierdzenie graniczne (CTG) mówi nam, że $\frac{S_n}{\sqrt{n}} \stackrel{D}{\rightarrow} \norm{0}{1}$
co jest w pewnym sensie oszacowaniem fluktuacji $S_n$ od dołu -- będą one wychodzić poza przedział $[-\sqrt{n}, \sqrt{n}]$, mamy bowiem
\begin{fakt}
\label{fakt:bladzenie_clt}
 Błądzenie przypadkowe $S_n$  z prawdopodobieństwem 1 spełnia \[ \limsup_{n \conv \infty} \frac{S_n}{\sqrt{n}} = \infty. \]
\end{fakt}
\begin{proof}
 Z prawa 0-1 Kołmogorowa wynika, że dla dowolnego ciągu zmiennych losowych $(X_i)$ i.i.d., zdarzenia typu $\left\{ \limsup\limits_{n \conv \infty} X_n > M \right\}$ mają prawdopodobieństwo równe 0 lub 1 (patrz \cite{jak-szt}, \S7.2, zadanie 1). Weźmy dowolnie duże $M$. Mamy
 \begin{align*}
  \Prob\left(\limsup_{n \conv \infty} \frac{S_n}{\sqrt{n}} > M \right)
  &= \Prob\left(\bigcap_{n=1}^\infty \bigcup_{k \geq n} \left\{ \frac{S_k}{\sqrt{k}} > M \right\} \right)\\
  &= \lim_{n \conv \infty} \Prob\left( \bigcup_{k \geq n} \left\{ \frac{S_k}{\sqrt{k}} > M \right\} \right) \\
  &\geq \lim_{n \conv \infty} \Prob\left(\frac{S_n}{\sqrt{n}} > M \right) \\
  &= 1 - \Phi(M) > 0.
 \end{align*}
 Czyli $\Prob\left(\limsup\limits_{n \conv \infty} \frac{S_n}{\sqrt{n}} > M \right) = 1$, co wobec dowolności $M$ oznacza, że \[ \Prob\left(\limsup_{n \conv \infty} \frac{S_n}{\sqrt{n}} = \infty \right) = 1. \]
\end{proof}

Okazuje się, że fluktuacje $S$ można oszacować precyzyjniej, mówi o tym
\begin{twier}[\textbf{Prawo iterowanego logarytmu}]
\label{tw:pil}
 Błądzenie przypadkowe $S_n$  z prawdopodobieństwem 1 spełnia \[ \limsup_{n \conv \infty} \frac{S_n}{ \sqrt{2 n \log \log n} } = 1. \]
\end{twier}
\noindent Dowód można znaleźć w \cite{feller}, rozdział VIII, \S5. Oczywiście ze względu na symetrię mamy analogiczne własności do Faktu \ref{fakt:bladzenie_clt} i Twierdzenia \ref{tw:pil} dla $\liminf$.

Jak widać $n$ było zbyt dużym dzielnikiem, a $\sqrt{n}$ zbyt małym -- odchylenia $S_n$ od zera rosną proporcjonalnie do $\sqrt{n \log \log n}$. Można zatem powiedzieć, że prawo iterowanego logarytmu~(PIL) ``działa pomiędzy'' prawem wielkich liczb i centralnym twierdzeniem granicznym. Te trzy twierdzenia dają nam własności błądzenia przypadkowego, które zebrano w Tabeli \ref{tab:wlasnosci_bladzenia}.

\begin{table}[ht]
\centering
 \caption{Wnioski dotyczące błądzenia przypadkowego wynikające ze znanych twierdzeń.}
 \label{tab:wlasnosci_bladzenia}
\begin{tabular} {||c | M{2.8cm} | M{2.8cm} | M{4cm} | M{4cm} || N}  
 \hline 
   & Zbieżność według prawdop. & Zbieżność prawie na pewno & Wartość limes superior prawie na pewno & Wartość limes inferior prawie na pewno  \\ \hline 
   PWL & $ \frac{S_n}{n} \stackrel{\Prob}{\Conv} 0 $ & $ \frac{S_n}{n} \stackrel{p.n.}{\Conv} 0 $ & $\limsup\limits_{n \conv \infty} \frac{S_n}{n} = 0 $ &  $\liminf\limits_{n \conv \infty} \frac{S_n}{n} = 0 $ &\\[1cm] \hline
   PIL & $ \frac{S_n}{\sqrt{2 n \log \log n}} \stackrel{\Prob}{\Conv} 0 $ & $ \frac{S_n}{\sqrt{2 n \log \log n}} \stackrel{p.n.}{\nrightarrow} 0 $ & $\limsup\limits_{n \conv \infty} \frac{S_n}{\sqrt{2n \log \log n}} = 1 $ &  $\liminf\limits_{n \conv \infty} \frac{S_n}{\sqrt{2n \log \log n}} = -1 $ &\\[1cm] \hline
   CTG & $ \forall x\ \frac{S_n}{\sqrt{n}} \stackrel{\Prob}{\nrightarrow} x $ &  $ \forall x\ \frac{S_n}{\sqrt{n}} \stackrel{p.n.}{\nrightarrow} x $ & $\limsup\limits_{n \conv \infty} \frac{S_n}{\sqrt{n}} = \infty $ &  $\liminf\limits_{n \conv \infty} \frac{S_n}{\sqrt{n}} = -\infty $ &\\[1cm] \hline
\end{tabular}  
\end{table}

Przyjrzyjmy się Rysunkowi \ref{fig:itlog}. Widać, że funkcja $\sqrt{2 n \log \log n}$ z grubsza odpowiada fluktuacjom procesu $S_n$. Można jednak zauważyć, że kilka trajektorii po około miliardzie kroków ciągle nie mieści się w przedziale $[-\sqrt{2 n \log \log n}, \sqrt{2 n \log \log n}]$. Prawo iterowanego logarytmu mówimy nam, że dla odpowiednio dużych $n$ trajektorie nie będą wykraczać poza ten zakres z prawdopodobieństwem 1. Wniosek jaki możemy wyciągnąć z tego obrazka jest taki, że mowa tu o naprawdę olbrzymich wartościach $n$.

\begin{figure}[!ht]
 \centering
 \includegraphics[scale=0.4]{obrazki/itlog.pdf}
 \caption{Ilustracja prawa iterowanego logarytmu. Przedstawia ona 500 trajektorii błądzenia losowego, długości $2^{30}$. Im ciemniejszy jest obszar wykresu, tym większe jest w nim zagęszczenie trajektorii. Niebieska krzywa to wykresy funkcji $\sqrt{x}$ oraz $-\sqrt{x}$, zaś czerwona funkcji $\sqrt{2 x \log \log x}$ oraz $-\sqrt{2 x \log \log x}$.}
 \label{fig:itlog}
\end{figure}

Choć nie będzie przydatna w dalszej części pracy, jeszcze jedna ciekawa własność narzuca się by o niej wspomnieć. Niech $\Slil{n} = \frac{S_n}{\sqrt{2n \log \log n}}$. Z PIL wynika, że wielkość $\Slil{n}$ nie zbiega punktowo do żadnej stałej. Zachodzi natomiast zbieżność według prawdopodobieństwa. Ustalmy więc dowolnie małe $\varepsilon > 0$ i zastanówmy się jak często $\Slil{n}$ opuści epsilonowy pasek wokół zera. Możemy przyjąć $p < 1$ dowolnie bliskie jedności, a mimo to dla prawie wszystkich $n$ możemy powiedzieć, że z prawdopodobieństwem $p$ wielkość $\Slil{n}$ nie wyjdzie poza przedział $(-\varepsilon, \varepsilon)$. Tymczasem PIL równocześnie mówi nam, że ten epsilonowy pasek opuścimy nieskończenie wiele razy. Ta niesamowita, pozorna sprzeczność pokazuje jak bardzo nasza intuicja zawodzi, gdy myślimy o zjawiskach zachodzących w nieskończoności.

\section{Prawo arcusa sinusa}
\label{par:asin}

Kolejna własność błądzenia przypadkowego, którą postaramy się wykorzystać do testowania GLP jest znana jako prawo arcusa sinusa. Odpowiada ono na pytanie przez jaką frakcję czasu ustalony gracz będzie na prowadzeniu. Spodziewalibyśmy się, że w przypadku bardzo długiej gry, obaj gracze będą na prowadzeniu przez mniej więcej tyle samo czasu. Jednak pokażemy, że również w tym przypadku nasza intuicja płata nam figla.

Powiemy, że bilans gry w $k$-tym kroku ($k \geq 1$) był dodatni, jeżeli $S_k > 0$ lub $S_{k-1}~>~0$. Pomijamy tu remisy przyjmując, że w przypadku wystąpienia równej liczby reszek i orłów przewagę ma ten, kto miał ją w poprzedniej chwili. Geometrycznie oznacza to, że odcinek wykresu błądzenia losowego przebiegający pomiędzy odciętymi $k-1$ oraz $k$, musi znajdować się nad osią x-ów.

Wprowadźmy następujące oznaczenia:
\begin{itemize}
  \setlength\itemsep{1pt}
 \item $U_n$ -- zdarzenie, że w $n$-tym kroku nastąpił powrót do zera,
 \item $F_n$ -- zdarzenie, ze w $n$-tym kroku nastąpił \emph{pierwszy} powrót do zera,
 \item $u_n = \Prob(U_n)$, $f_n = \Prob(F_n)$.
 \item $p_{k,n}$ -- prawdopodobieństwo, że przez $k$ spośród pierwszych $n$ kroków gry, bilans był dodatni.
\end{itemize}
Łatwo zauważyć, że powrót do zera może nastąpić tylko w parzystym kroku, zatem
\[ \forall n \in \mathbb{N}\ \ u_{2n-1} = f_{2n-1} = 0, \]
\[ \forall k,n \in \mathbb{N}\ \ p_{2k-1, 2n} = 0, \]
Ponadto przyjmujemy, że $p_{0,0} = u_0 = 1$. Zachodzi również

\begin{lemat}
 \label{lem:uf_val}
 Dla każdego $n \in \mathbb{N}$ spełnione są poniższe tożsamości:
 \begin{align}
  u_{2n} &= \binom{2n}{n}2^{-2n}   \label{eq:u_val}\\
  u_{2n} &= \sum_{r=1}^n f_{2r} u_{2n-2r}   \label{eq:u_val_cond}\\
  f_{2n} &= \frac{1}{2n} u_{2n-2} \label{eq:f_val}\\
  f_{2n} &= u_{2n-2} - u_{2n} \label{eq:f_val2}
 \end{align}
\end{lemat}
\begin{proof}
 Wzór (\ref{eq:u_val}) wynika stąd, że wszystkich dróg długości $2n$ jest $2^{2n}$, a drogi wracające na końcu do zera odpowiadają ustawieniu $n$ orłów i $n$ reszek na $2n$ miejscach -- co robimy na $\binom{2n}{n}$ sposobów.
 
 Tożsamość (\ref{eq:u_val_cond}) wynika wprost ze wzoru na prawdopodobieństwo całkowite:
 \[ u_{2n} = \Prob(U_{2n}) = \sum_{r=1}^n \Prob(U_{2n}|F_{2r}) \Prob(F_{2r}) = \sum_{r=1}^n \Prob(U_{2n-2r}) \Prob(F_{2r}) = \sum_{r=1}^n u_{2n-2r}f_{2r}  \]
 
 Dla dowodu (\ref{eq:f_val}) wprowadźmy dodatkowe oznaczenia:
 
\begin{itemize}
  \setlength\itemsep{1pt}
  \item $N_n(a,b)$ -- liczba ścieżek od stanu $a$ do stanu $b$ w $n$ krokach,
  \item $N_n^{\neq 0}(a,b)$ -- jak $N_n(a,b)$, ale ścieżki nie mogą dotykać 0 (za wyjątkiem co najwyżej końców),
  \item $N_n^{=0}(a,b)$ -- jak $N_n(a,b)$, ale ścieżki muszą dotknąć lub przeciąć 0.
\end{itemize}
Łatwo zauważyć, że $N_n(a,b) = \binom{n}{(n+b-a)/2}$ oraz $N_n(a,b) = N_n^{\neq 0}(a,b) + N_n^{= 0}(a,b)$. Wartość $f_{2n}$ to oczywiście stosunek $N_{2n}^{\neq 0}(0,0)$ do liczby wszystkich ścieżek od stanu 0 do stanu 0 w $2n$ krokach. Dlatego liczymy
\[ N_{2n}^{\neq 0}(0,0) = N_{2n-1}^{\neq 0}(1,0) + N_{2n-1}^{\neq 0}(-1,0) = 2N_{2n-1}^{\neq 0}(1,0)= 2N_{2n-2}^{\neq 0}(1,1) \]
Patrząc na Rysunek \ref{fig:forlemma} łatwo zauważyć, że $N_{2n-2}^{=0}(1,1) = N_{2n-2}(-1,1)$. 
\begin{figure}[h]
 \centering
 \includegraphics[scale=0.5]{obrazki/forlemma.pdf}
 \caption{Ilustracja faktu $N_n^{=0}(1,1) = N_n(-1,1)$. Łatwo zobaczyć jednoznaczną odpowiedniość między oboma rodzajami ścieżek. Aż do momentu pierwszego powrotu do zera ścieżka jednego rodzaju jest odbiciem symetrycznym względem osi odciętych ścieżki drugiego rodzaju, zaś dalej ścieżki się pokrywają.}
 \label{fig:forlemma}
\end{figure}
Zatem
\begin{equation*}
 \begin{split}
   N_{2n-2}^{\neq 0}(1,1)
   &= N_{2n-2}(1,1) - N_{2n-2}^{=0}(1,1) =  N_{2n-2}(1,1) - N_{2n-2}(-1,1) \\
   &= \binom{2n-2}{n-1} - \binom{2n-2}{n} = \binom{2n-2}{n-1} - \frac{n-1}{n}\binom{2n-2}{n-1} \\
   &= \frac{1}{n} \binom{2n-2}{n-1} = \frac{2^{2n-2}}{n} u_{2n-2}
 \end{split}
\end{equation*}
Ostatecznie

\begin{equation*}
 \begin{split}
   f_{2n} &= \frac{N_{2n}^{\neq 0}(0,0)}{2^{2n}} = \frac{2N_{2n-2}^{\neq 0}(1,1)}{2^{2n}} = \frac{\frac{2^{2n-1}}{n} u_{2n-2}}{2^{2n}} = \frac{u_{2n-2}}{2n}
 \end{split}
\end{equation*}

 Formuła (\ref{eq:f_val2}) to prosta konsekwencja (\ref{eq:u_val}) i (\ref{eq:f_val}), bo
 \begin{equation*}
 \begin{split}
    u_{2n-2} - u_{2n} &= u_{2n-2} - \binom{2n}{n}2^{-2n} =  u_{2n-2} - \binom{2n-2}{n-1} \frac{(2n-1)2n}{4n^2}2^{-(2n-2)} = \\
    &= u_{2n-2}\left(1 - \frac{(2n-1)}{2n} \right) = \frac{1}{2n} u_{2n-2} =  f_{2n}.
 \end{split}
 \end{equation*}
\end{proof}

Tożsamości z Lematu \ref{lem:uf_val} intensywnie wykorzystujemy w dowodzie następującego, kluczowego faktu.
\begin{twier}
 \label{twier:disc_asine_law}
 Dla wszystkich $k, n \in \mathbb{N}$
 \begin{equation}
  p_{2k,2n} = u_{2k} u_{2n-2k} = \binom{2k}{k}\binom{2n-2k}{n-k}2^{-2n} \label{eq:disc_asine_law}
 \end{equation}
\end{twier}
\begin{proof}
Niech $q_{2n}$ oznacza prawdopodobieństwo, że w pierwszych $2n$ krokach gry ani razu nie doszło do remisu. Wzór (\ref{eq:f_val2}) daje nam
\[ q_{2n} = 1 - f_2 - f_4 - \cdots - f_{2n} = 1 - (1- u_2) - (u_2 - u_4) - \cdots - (u_{2n-2} - u_{2n}) = u_{2n}. \]
Udowodnimy teraz indukcyjnie, że
\begin{equation}
 p_{0,2n} = u_{2n}. \label{eq:disc_asine_law_k0}
\end{equation}
Łatwo sprawdzić, że $p_{0,2} = \frac{1}{2} = u_2$. Załóżmy, że $p_{0,2\tilde{n}} = u_{2\tilde{n}}$ dla $\tilde{n} < n$.  Zauważmy, że aby spędzić całą grę na minusie, musieliśmy w pierwszym kroku pójść w dół, co dzieje się z prawdopodobieństwem $\frac{1}{2}$. Dalej musiała zajść jedna z dwóch możliwości. Z prawdopodobieństwem $q_{2n}$ mogliśmy ani razu nie wrócić do zera. Mogło się też zdarzyć, że dla pewnego $r$ wróciliśmy do zera po raz pierwszy w kroku $2r$ (z prawdopodobieństwem $f_{2r}$), ale resztę czasu mimo tego spędziliśmy ``pod kreską'' (z prawdopodobieństwem $p_{0,2n-2r}$). Te rozważania, założenie indukcyjne oraz wzór (\ref{eq:u_val_cond}) dają
\begin{equation*}
 \begin{split}
  p_{0,2n} &= \frac{1}{2} \left( q_{2n} + \sum_{r=1}^n f_{2r} p_{0,2n-2r} \right) = \frac{1}{2} \left( u_{2n} + \sum_{r=1}^n f_{2r} u_{2n-2r}  \right) \\
  &= \frac{1}{2} \left( u_{2n} + u_{2n}  \right) = u_{2n},
 \end{split}
\end{equation*}
co chcieliśmy pokazać.

Teraz uogólniamy ten wynik postępując również indukcyjnie. Twierdzenie \ref{twier:disc_asine_law} jest w oczywisty sposób prawdziwe dla $n=0$. Załóżmy teraz, że dla wszystkich $\tilde{n} < n$ zachodzi $\forall 0 \leq k \leq \tilde{n}\ \ p_{2k,2\tilde{n}} = u_{2k} u_{2\tilde{n}-2k}$ i pokażemy, że $\forall 0 \leq k \leq n\ \ p_{2k,2n} = u_{2k} u_{2n-2k}$. 
Wiemy już, że teza jest prawdziwa dla $k = 0$ oraz $k = n$, gdyż
\[  p_{2n,2n} = p_{0,2n} = u_{2n} = u_{2n}u_0. \]
Dlatego weźmy dowolne $k$, takie że $0 < k < n$. Aby zaszło rozważane zdarzenie, błądzenie musi przechodzić przez 0. Załóżmy, że pierwszy raz dzieje się to w pewnym punkcie $2r$. Jeżeli w pierwszym kroku poszliśmy w górę (co dzieje się z prawdopodobieństwem $\frac{1}{2}$), to po powrocie musimy spędzić ``nad kreską'' jeszcze $2k-2r$ kroków, a szanse tego zdarzenia wynoszą $p_{2k-2r, 2n-2r}$. W przeciwnym razie po powrocie ciągle musimy być na plusie przez $2k$ kroków, co zdarzy się z prawdopodobieństwem $p_{2k,2n-2r}$. Stąd
\begin{equation*}
 \begin{split}
  p_{2k,2n} &= \frac{1}{2} \left( \sum_{r=1}^k f_{2r} p_{2k-2r,2n-2r} + \sum_{r=1}^{n-k} f_{2r} p_{2k, 2n-2r} \right) = (\bigstar)
 \end{split}
\end{equation*}
\noindent Z założenia indukcyjnego
\[ p_{2k-2r,2n-2r} = u_{2k-2r}u_{2n-2r - (2k-2r)} = u_{2k-2r}u_{2n-2k} \]
oraz
\[ p_{2k,2n-2r} = u_{2k}u_{2n-2r-2k}, \]
zatem
\begin{equation*}
 \begin{split}
  (\bigstar) &= \frac{1}{2} \left( \sum_{r=1}^k f_{2r} u_{2k-2r}u_{2n-2k} + \sum_{r=1}^{n-k} f_{2r} u_{2k}u_{2n-2r-2k} \right) \\
             &= \frac{1}{2} \left( u_{2n-2k}\sum_{r=1}^k f_{2r} u_{2k-2r} + u_{2k}\sum_{r=1}^{n-k} f_{2r} u_{2n-2r-2k} \right) \\
             &= \frac{1}{2} \left( u_{2n-2k}u_{2k} + u_{2k} u_{2n-2k} \right) = u_{2k} u_{2n-2k},
 \end{split}
\end{equation*}
co było do okazania. Korzystając z (\ref{eq:u_val}) otrzymujemy tezę.
\end{proof}

Dzięki Twierdzeniu \ref{twier:disc_asine_law} możemy obliczać dokładne prawdopodobieństwa frakcji przewagi. Na Rysunku \ref{fig:disc_asine} przedstawiony jest ich rozkład dla $n=20$. Widać wyraźnie, że równomierny podział czasu na przewagę jednego i drugiego gracza jest najmniej prawdopodobny. Najbardziej prawdopodobna jest dominacja jednego z graczy przez większość czasu. Przykładowo prawdopodobieństwo, że po 100 rzutach
\begin{itemize}
 \item jeden z graczy wygrywa przez 90-100\% czasu, wynosi 44\%.
 \item jeden z graczy ani razu nie wyjdzie na prowadzenie, wynosi 16\%.
 \item ustalony gracz będzie prowadził przez 40-60\% czasu, wynosi 14\%.
\end{itemize}

\begin{figure}[ht]
 \centering
 \includegraphics[scale=0.4]{obrazki/discasine.pdf}
 \caption{Rozkład czasu prowadzenia ustalonego gracza przy 40 rzutach monetą.}
 \label{fig:disc_asine}
\end{figure}

Wzór (\ref{eq:disc_asine_law}) jest dokładny, ale często nieporęczny. Spróbujmy znaleźć rozsądne przybliżenie. Zakładając $k \conv \infty,\ n-k \conv \infty$ i korzystając ze wzoru Stirlinga ($n! \approx \sqrt{2\pi n} \left( \frac{n}{e} \right)^n$), dostajemy
\[ \binom{2k}{k} = \frac{(2k)!}{k! k!} \approx \frac{\sqrt{4\pi k} \left( \frac{2k}{e} \right)^{2k}}{2\pi k \left( \frac{k}{e} \right)^{2k}} = \frac{2^{2k}}{\sqrt{\pi k}}, \]
i podobnie
\[ \binom{2n-2k}{n-k} \approx \frac{2^{2n-2k}}{\sqrt{\pi (n-k)}}. \]
Podstawiając to do wzoru (\ref{eq:disc_asine_law}) otrzymujemy
\begin{equation}
  \label{eq:disc_asine_approx}
   p_{2k,2n} \approx \frac{1}{\pi \sqrt{k(n-k)}}
\end{equation}

% \begin{equation*}
%  \begin{split}
%    p_{2k,2n}
%    &= \binom{2k}{k}\binom{2n-2k}{n-k} 2^{-2n} = \frac{(2k)!}{k! k!} \frac{(2n - 2k)!}{(n-k)! (n-k)!} 2^{-2n} \\
%    &\approx \frac{\sqrt{4\pi k} \left( \frac{2k}{e} \right)^{2k}}{2\pi k \left( \frac{k}{e} \right)^{2k}}
%             \frac{\sqrt{4\pi (n-k)} \left( \frac{2n-2k}{e} \right)^{2n-2k}}{2\pi (n-k) \left( \frac{n-k}{e} \right)^{2n-2k}} 2^{-2n} \\
%    &=       \frac{2^{2k}}{ \sqrt{\pi k} } \frac{ 2^{2n-2k}}{ \sqrt{\pi (n-k)} } 2^{-2n} = \frac{1}{\pi \sqrt{k(n-k)}}
%  \end{split}
% \end{equation*}

Odpowiemy teraz na następujące pytanie: \textbf{jaka jest szansa, że w bardzo długiej grze byliśmy na prowadzeniu przez co najwyżej frakcję $x$ czasu?} ($0 < x < 1$)

Niech $P_{2n}(x)$ oznacza szukane prawdopodobieństwo przy $2n$ rzutach monetą. Załóżmy na początek, że $x > \frac{1}{2}$. Wtedy
\[ P_{2n}(x) = \sum_{k:\ \frac{k}{n} < x} p_{2k,2n} = \underbrace{\sum_{k:\ \frac{k}{n} \leq \frac{1}{2}} p_{2k,2n}}_{(\spadesuit)} + \underbrace{\sum_{k:\ \frac{1}{2} < \frac{k}{n} < x} p_{2k,2n}}_{(\clubsuit)}  \]
Pamiętając o symetryczności rozkładu można zauważyć, że $(\spadesuit) \Conv \frac{1}{2}$ (można to też uzasadnić inaczej -- jeden z graczy musi być na prowadzeniu przez co najwyżej połowę czasu). Przy $n \conv \infty$ i $\frac{1}{2} < \frac{k}{n} < x < 1$ zachodzi również $k \Conv \infty$ oraz $\ n-k \Conv \infty$. Dlatego drugą sumę możemy estymować korzystając z (\ref{eq:disc_asine_approx}) oraz definicji całki Riemanna
\begin{equation*}
 \begin{split}
 (\clubsuit) &\approx \sum_{k:\ \frac{1}{2} < \frac{k}{n} < x} \frac{1}{\pi \sqrt{k(n-k)}} =  \sum_{k:\ \frac{k}{n} < x} \frac{1}{\pi n} \frac{1}{ \sqrt{\frac{k}{n} (1 - \frac{k}{n})} } \\
 &\xrightarrow[n \conv \infty]{} \frac{1}{\pi} \int_{1/2}^x \frac{dt}{\sqrt{t(1-t)}} = \frac{2}{\pi}\arcsin(\sqrt{x}) - \frac{1}{2},
 \end{split}
\end{equation*}
czyli
\[  P_{2n}(x) \xrightarrow[n \conv \infty]{} \frac{2}{\pi}\arcsin(\sqrt{x}). \]
W celu znalezienia $P_{2n}(x)$ dla $0 < x < \frac{1}{2}$ skorzystamy ze znanych własności funkcji cyklometrycznych: $\arcsin x + \arccos x = \frac{\pi}{2}$ oraz $\arccos x = \arcsin(\sqrt{1-x^2})$.
\begin{equation*}
 \begin{split}
  P_{2n}(x) &= 1 - P_{2n}(1-x) \xrightarrow[n \conv \infty]{} 1 - \frac{2}{\pi} \arcsin(\sqrt{1-x}) = 1 - \frac{2}{\pi} \arccos(\sqrt{x}) \\
  &= 1 - \frac{2}{\pi} \left( \frac{\pi}{2} - \arcsin(\sqrt{x}) \right) = \frac{2}{\pi}\arcsin(\sqrt{x}).
 \end{split}
\end{equation*}
W ten sposób udowodniliśmy
\begin{twier}[\textbf{Prawo arcusa sinusa}]
 Prawdopodobieństwo, że w $n$ krokach frakcja czasu $x$ ($0 \leq x \leq 1$), w której ustalony gracz ma przewagę (stan błądzenia przypadkowego jest dodatni), dąży przy $n \Conv \infty$ do
 \[  \frac{1}{\pi} \int_0^x \frac{dt}{\sqrt{t(1-t)}} = \frac{2}{\pi}\arcsin(\sqrt{x}) \]
\end{twier}

Innymi słowy w bardzo długiej grze frakcja czasu $x$ spędzona ``na plusie'' ma rozkład arcusa sinusa. Oto jego podstawowe własności:
\begin{itemize}
 \item gęstość $f(t) =  \frac{1}{\pi \sqrt{t(1-t)}}$,
 \item dystrybuanta $F(t) =  \frac{2}{\pi}\arcsin(\sqrt{t})$,
 \item wartość oczekiwana: $\frac{1}{2}$,
 \item wariancja: $\frac{1}{8}$.
\end{itemize}
Wykres gęstości i dystrybuanty przedstawia Rysunek \ref{fig:asine_dist}. Funkcja gęstości w kształcie litery U pokazuje, że nierówny podział czasu przewagi jest zdecydowanie bardziej prawdopodobny niż względnie równomierny.
\begin{figure}[ht]
 \centering
 \includegraphics[scale=0.3]{obrazki/asinedist.pdf}
 \caption{Rozkład arcusa sinusa.}
 \label{fig:asine_dist}
\end{figure}

Ludzka intuicja silnie podpowiada, że w grze z symetryczną monetą, każdy z graczy powinien być na plusie przez około połowę czasu. Wydaje się to logiczne -- wiadomo, że liczba powrotów błądzenia przypadkowego do zera jest nieskończona w nieskończenie długiej grze. Zatem obaj gracze mają mniej więcej tyle samo fal kiedy są na plusie. Ponadto średnia długość dodatniej fali powinna być dla obu graczy zbliżona. Co z kolei prowadzi do wniosku, że obaj powinni być na prowadzeniu przez podobną frakcję czasu. Gdzie tkwi błąd w tym rozumowaniu? Otóż MPWL dotyczy zmiennych o skończonej wartości oczekiwanej. Tymczasem oczekiwany czas powrotu do zera w błądzeniu przypadkowym okazuje się być nieskończony, co kompletnie zmyla nasze intuicje.

\chapter{Generatory liczb pseudolosowych}
\label{czesc:generatory}

Każdy intuicyjnie rozumie czym jest generator liczb pseudolosowych. Jednak dla pełności matematycznego opisu zaczniemy od przedstawienia jego ścisłej definicji. Przedstawiona tu teoria opiera się na książce \cite{asmussen}. Na jej podstawie opisujemy również kilka rodzajów generatorów liczb pseudolosowych. Sposób otrzymania z GLP ciągu bitów o dobrych własnościach również wymaga pewnego komentarza, o czym piszemy dalej. Na końcu tej części krótko omówimy znane metody testowania generatorów liczb pseudolosowych.

\section{Definicja GLP}
\label{sec:pop_gen}
Istnieją metody otrzymania liczb ,,prawdziwie losowych''. Najprostszym sposobem jest wielokrotne rzucenie kostką do gry lub monetą i stablicowanie otrzymanych wyników. Lepszym sposobem jest obserwowanie emitowanych cząsteczek przez próbkę radioaktywnego pierwiastka -- uważa się, że rozkład radioaktywny jest dobrze modelowany przez proces Poissona. Kolejnym pomysłem jest wykorzystanie szumu atmosferycznego, z tej metody korzysta strona www.random.org. Źródła takiej losowości są jednak zazwyczaj zbyt wolne, trudno dostępne lub mają pewne inherentne wady (moneta może być niesymetryczna, detektor cząstek nie rejestruje zgłoszeń o zbyt krótkim odstępie, itp.). Wynika stąd potrzeba utworzenia deterministycznych algorytmów które imitowałyby losowość. Takie algorytmy nazywamy generatorami liczb pseudolosowych. Aby były praktyczne, muszą być szybkie i obliczalne na zwykłych komputerach. Liczby przez nie generowane jedynie ,,udają'' losowe, dlatego nazywamy je pseudolosowymi.
\begin{mydef}
\label{def:glp}
 Generator liczb pseudolosowych jest to piątka\footnote{Jest to nieco inna definicja niż w \cite{asmussen}, dostosowana do naszych potrzeb. W książce Asmussena przyjmuje się $V = [0,1]$, zaś zamiast $s_0$ w definicji znajduje się $\mu$ -- rozkład prawdopodobieństwa początkowego stanu.} $\langle E, V, s_0, f, g \rangle$, gdzie $E$ jest skończoną przestrzenią stanów, $V$ jest zbiorem wartości zwracanych przez generator, $s_0$ jest to tzw. ziarno, czyli początkowy stan w ciągu stanów $(s_i)_{i=0}^\infty$, funkcja $f:E \rightarrow E$ opisuje przejścia między kolejnymi stanami: $s_n = f(s_{n-1})$, zaś $g:E\rightarrow V$, odwzorowuje stan generatora w wartość przez niego zwracaną.
\end{mydef}
Najczęściej przyjmuje się $V = (0,1)$ lub $V = \bar{M}$ dla pewnego $M$ (dla $n \in \mathbb{N}$ symbol $\bar{n}$ oznacza zbiór $\{0,1,\ldots,n-1\}$). U nas będzie zachodzić właśnie ta druga możliwość.

Zauważmy, że każdy GLP prędzej lub później ,,zapętla się'', tzn. musi istnieć takie $d$, że dla pewnego $l$ zachodzi $s_{l+d} = s_l$ (wynika to ze skończoności przestrzeni stanów). Minimalne $d$ o tej własności nazywane jest \textit{okresem} generatora. Dobre generatory powinny mieć jak najdłuższe okresy, optymalnie równe $|E|$.

Poniżej przedstawiamy popularne rodzaje GLP.

\subsection*{LCG}
Generatory LCG (od ang. \textit{linear congruential generator}) zmieniają swój stan zgodnie z rekurencją
\begin{equation}
 \label{eq:lcg}
 s_n = (a s_{n-1} + c) \mod M.
\end{equation}
Generator tej klasy jest określony przez moduł $M$, mnożnik $a$ oraz przyrost $c$, co oznaczamy $LCG(M,a,c)$. Zauważmy, że $LCG(M,a,c)$ spełnia definicję GLP z $E = \bar{M}$, $V = \bar{M}$, $f(x) = (a x + c) \mod M$ oraz $g(x) = x$.

Dobranie wartości $M, a, c$ tak by generator miał długi okres nie jest łatwym zadaniem. Z pomocą przychodzi
\begin{twier}
  Przy poniższych warunkach $LCG(M,a,c)$ ma okres równy $M$:
  \begin{itemize}
   \item $c$ oraz $M$ są względnie pierwsze,
   \item jeśli $p$ jest liczbą pierwszą i $p | M$, to $p |(a - 1)$,
   \item jeśli $4 | M$, to $4 |(a-1)$.
  \end{itemize}
\end{twier}
\noindent Powyższe twierdzenie pochodzi z pracy \cite{hull}.

Zauważmy jeszcze, że znalezienie LCG o pełnym okresie nie gwarantuje, że generator będzie dobrej jakości. Łatwo zauważyć następujący
\begin{fakt}
 \label{fakt:lcg_okres}
 Niech $M = 2^k$. Wówczas $d$ najmniej istotnych bitów $LCG(M,a,c)$ ma okres równy co najwyżej $2^d$.
\end{fakt}
\noindent W tym przypadku nie ma więc mowy o niezależności liczb generowanych przez LCG. Niektóre pakiety korzystające z LCG obchodzą ten problem zwracając tylko najbardziej znaczące bity wygenerowanych liczb.

\subsection*{MCG}
MCG (od ang. \textit{multiplicative congruential generator}) znany jest też jako GLP Lehmera lub GLP Parka-Millera. 
Jest to szczególny przypadek LCG, w którym $c = 0$, czyli kolejne stany opisuje rekurencja
\begin{equation}
 \label{eq:mcg}
 s_n = a s_{n-1} \mod M.
\end{equation}
MCG o parametrach $M$ oraz $a$ oznaczamy $MCG(M, a)$. Aby $MCG(M,a)$ mogło mieć dobre własności $M$ powinno być liczbą pierwszą lub jej potęgą, $a$ powinno być generatorem grupy $\mathbb{Z}_M^*$, a ziarno $s_0$ powinno być względnie pierwsze z $M$. 

\subsection*{GLCG}
Wyżej opisane GLP dzielą pewną wadę -- mają stosunkowo krótkie okresy. W przypadku gdy potrzebujemy dłuższych okresów przydatne mogą być uogólnione LCG (od ang. \textit{generalized linear congruential generator}). Postępują one zgodnie z rekurencją
\begin{equation}
 \label{eq:glcg}
 x_n = (a_1 x_{n-1} + a_2 x_{n-2} + \ldots + a_k x_{n-k} + c) \mod M.
\end{equation}
GLCG zmieniający stany w ten sposób oznaczamy $GLCG(M, (a_i)_{i=1}^k, c)$. Jest to ciągle GLP w myśl definicji \ref{def:glp}, gdzie $E = \bar{M}^k$, $s_n = \langle x_n, x_{n-1}, \ldots  x_{n-k+1} \rangle$, $g(s_n) = x_n$. Dobry dobór parametrów może dać okres równy $M^k-1$.

\subsection*{Generatory mieszane}
Dobrym pomysłem na ulepszenie GLP jest połączenie kilku generatorów w jeden. Załóżmy, że mamy dane $k$ GLP $\langle E_j, V_j, s_{j,0}, f_j, g_j \rangle$, $1 \leq j \leq k$, gdzie $j$-ty generator zmienia stan według zależności 
\[ s_{j,n} = f_j(s_{j,n-1}). \]
Możemy teraz zdefiniować mieszany generator w taki sposób, aby ciąg jego stanów spełniał
\[ s_n = \langle s_{1,n}, s_{1,n}, \ldots s_{k,n} \rangle\]
Niech ponadto $d_j$ oznacza okres $j$-tego ,,składowego'' generatora. Jak pokazał L'Ecuyer (\cite{lecuyer}, Lemma 2) mieszany generator ma okres $d = NWW(d_1, d_2,\ldots,d_k)$.

Szczególnym przypadkiem generatorów mieszanych są \textbf{CMCG} (od ang. \textit{combined multiplicative congruential generator}). Składa się on $k$ generatorów $MCG(M_j, a_j)$, czyli funkcją przejścia jest
\[ s_{j,n} = a_j s_{j,n-1} \mod M_j. \]
Wyjście generatora mieszanego jest
\[ g(s_n) = \left( \sum_{j=1}^{k} (-1)^{j-1} s_{j,n} \right) \mod M_1 - 1\]
Ponadto jeśli liczby $\frac{M_j-1}{2}$ są względnie pierwsze, to CMCG ma optymalny okres wynoszący $\frac{1}{2^k}(M_1-1)\cdot \ldots \cdot (M_k-1)$.

\subsection*{LFSR}
LFSR (od ang. \textit{Linear feedback shift register}) to, z grubsza rzecz ujmując, generator produkujący liczby pseudolosowe na podstawie obwód bramek logicznych. Stanem tych generatorów jest sekwencja bitów, które przekazywane są na wejście wybranych bramek. Wyjścia tych bramek stanowią wejście innych bramek, te z kolei przekazują swoje wyjścia kolejnym bramkom, itd. Wyjścia wybranych bramek mogą zmieniać stan generatora lub być zwracane jako rezultat pracy generatora. 

\subsection*{Mersenne Twister}
Mersenne Twister (MT19937) został zaproponowany przez Matsumoto i Nashimurę w \cite{twister}. Nie jest to wprawdzie klasa generatorów, ale przykład konkretnego GLP, jednak ze względu na jego ogromną popularność warto go opisać. Jest on standardowym generatorem w wielu narzędziach programistycznych, między innymi w R, Python, MATLAB, Julia. Został dołączony również do standardowej biblioteki C++11.

Stan generatora opisany jest przez 624 32-bitowe liczby
\[ x_k, x_{k+1}, \ldots x_{k+623} \]
Kolejne stany otrzymujemy ze wzoru (zapis $d[i..j]$ oznacza bity o indeksach od $i$ do $j$ w liczbie $d$, zaś symbol $\oplus$, to operacja XOR na kolejnych bitach):
\[ x_{k+624} = \begin{cases}
      x_{397+k}\oplus (0, x_k[0], x_{k+1}[1..30]) &\mbox{if } x_{k+1}[31] = 0 \\ 
      x_{397+k}\oplus (0, x_k[0], x_{k+1}[1..30]) \oplus a & \mbox{if } x_{k+1}[31] = 1.
    \end{cases} \]
gdzie $a = (9908B0D)_{16}$. Przy $k$-tym wywołaniu MT19937 zwraca jako wyjście wartość $t(x_{623+k})$, przy czym
\[ t(x) = y_3 \oplus (y_3 \gg 18), \]
gdzie
\begin{equation*} 
\begin{split}
 y_3 &= y_2 \oplus ((y_2 \ll 15)\ \&\ (EFC60000)_{16})\\
 y_2 &= y_1 \oplus ((y_1 \ll 7)\ \&\ (9D2C5680)_{16})\\
 y_1 &= x \oplus (x \gg 11)
\end{split}
\end{equation*}

Generator uzyskany w ten sposób ma okres równy $2^{19937}-1$, co wyjaśnia jego skrótową nazwę.

\section{Generowanie sekwencji bitów}
Komputery operują tylko i wyłącznie na ciągach bitów, dlatego wyjście każdego programu, w szczególności GLP, może być traktowane jako ciąg zerojedynkowy. My potrzebujemy jednak ciągów specyficznych: każdy bit musi być generowany niezależnie i z jednakowym prawdopodobieństwem przyjmować wartości zero i jeden. Dlatego dla wygody języka wprowadźmy poniższy termin.
\begin{mydef}
 \textit{Idealnym ciągiem losowych bitów} nazywamy proces Bernoulliego z prawdopodobieństwem sukcesu $p = \frac{1}{2}$
\end{mydef}

Poniżej opisujemy jak zmienić generator liczb pseudolosowych w ,,generator pseudolosowych ciągów zerojedynkowych''.

\begin{algorithm}
 \begin{algorithmic}[1]
  \Function{GenerujCiągBitów}{\texttt{glp}}
    \State $s \gets \epsilon$
      \Comment{{\color{comment} $\epsilon$ to słowo puste}}
    \While {$s$ nie jest wystarczająco długi}
      \State $a \gets$ następna liczba z \texttt{glp}
      \State $b \gets$ binarny zapis $a$ na $\ceil{\log_2 M}$ bitach
      \State $s \gets s \odot b$
      \Comment{{\color{comment} $\odot$ to operator konkatenacji}}
     \EndWhile
    \State \Return $s$.
  \EndFunction
 \end{algorithmic}
 \caption{Generowanie sekwencji bitów przy użyciu GLP.}
 \label{alg:GenerateBitSequence}
\end{algorithm}

% GLP możemy wykorzystać do wygenerowania idealnego ciągu losowych bitów w następujący sposób:
% \begin{enumerate}
%  \item Otrzymujemy z generatora liczbę $a$.
%  \item Zapisujemy $a$ binarnie na $\ceil{\log_2 M}$ bitach.
%  \item Binarny zapis $a$ doklejamy do wyjściowego ciągu.
%  \item Powtarzamy aż do otrzymynia pożądanie długiego ciągu bitów.
% \end{enumerate}

Mamy dany GLP generujący liczby całkowite ze zbioru $\bar{M} = \{0, 1, \ldots, M-1 \}$. Kolejne wywołania powinny dawać niezależne wyniki. Aby wykorzystać GLP do wygenerowania idealnego ciągu losowych bitów możemy użyć Algorytmu \ref{alg:GenerateBitSequence}. Jeśli $M$ jest potęgą dwójki, to zadziała on dobrze, mamy bowiem
\begin{lemat}
 Niech $M = 2^k$. Jeżeli GLP w każdym kroku generuje liczby niezależnie i jednostajnie w zbiorze $\bar{M}$, to Algorytm \ref{alg:GenerateBitSequence} generuje idealny ciąg losowych bitów.
\end{lemat}
\begin{proof}
 W przypadku $M = 2^k$ mamy wzajemnie jednoznaczną odpowiedniość pomiędzy zbiorem $\bar{M}$ oraz układami $k$ bitów. Oznacza to, że w jednym kroku otrzymujemy z GLP każdy możliwy układ $k$ bitów z jednakowym prawdopodobieństwem $\frac{1}{2^k}$. Łatwo zauważyć, że wtedy każdy generowany bit ma równe szanse bycia jedynką i zerem, oraz jest niezależny od pozostałych.
\end{proof}

Jednak jeśli $M$ nie jest potęgą dwójki, to procedura nie działa -- przykładowo dla $M=5$ w każdym kroku doklejamy jedną z sekwencji $\{000, 001, 010, 011, 100\}$. Wówczas w wygenerowanym ciągu spotkanie jedynki jest mniej prawdopodobne niż zera -- jedynki stanowią tylko około $\frac{1}{3}$ wszystkich wygenerowanych bitów. Ponadto nie ma niezależności -- wystąpienie jedynki na bicie o indeksie podzielnym przez 3 oznacza, że kolejne dwa bity będą zerami.

Jak widać, gdy $M$ nie jest postaci $2^k$ nie możemy w wyjściowym ciągu tak po prostu umieścić binarnego zapisu wygenerowanej liczby. Na najbardziej znaczących bitach przeważają zera. Prostym obejściem tego problemu jest ograniczenie się do mniej znaczących bitów generowanych liczb (jak się okazuje do stwierdzenia wadliwości niektórych generatorów wystarczy patrzeć na najmniej znaczący bit). One również nie mają idealnego rozkładu, co można łatwo zauważyć licząc prawdopodobieństwo wystąpienia jedynki na najmniej znaczącym bicie, jednak odstępstwo jest jednak stosunkowo niewielkie.

Inne podejście przedstawia Algorytm \ref{alg:GenerateBitSequence2}. Jego dodatkową zaletą jest możliwość modyfikacji, tak by działał dla GLP zwracających liczby z odcinka $(0,1)$.
\begin{algorithm}
 \begin{algorithmic}[1]
  \Function{GenerujCiągBitów}{\texttt{glp}, $n$}
    \State $s \gets \epsilon$
    \While {$s$ nie jest wystarczająco długi}
      \State $a \gets$ następna liczba z \texttt{glp}
      \State $b \gets$ $n$ pierwszych bitów rozwinięcia dwójkowego $\frac{a}{M}$
      \State $s \gets s \odot b$
     \EndWhile
    \State \Return $s$.
  \EndFunction
 \end{algorithmic}
 \caption{Generowanie sekwencji bitów przy użyciu GLP.}
 \label{alg:GenerateBitSequence2}
\end{algorithm}

\section{Popularne metody testowania GLP}
\label{sec:pop_testy}

Poniżej przedstawiamy kilka wybranych metod testowania GLP. Lista z pewnością jest daleka od kompletności, zwłaszcza, że zagadnienie testowania GLP cieszy się dużą popularnością.

Do testów używany jest ciąg liczb wygenerowanych przez GLP. Poniżej czasem będzie nam wygodnie przyjmować, że jest to ciąg liczb całkowitych nieujemnych
\[ X_1, X_2, X_3, \ldots \]
pretendujący do miana ciągu niezależnie i równomiernie rozłożonego w zbiorze $\bar{M}$, a czasem, że jest to ciąg
\[ U_1, U_2, U_3, \ldots \]
pretendujący do miana ciągu niezależnie i równomiernie rozłożonego na odcinku $(0,1)$.

\subsection*{Zgodność z rozkładem jednostajnym}
Pierwszym nasuwającym się sposobem sprawdzenia jakości liczb generowanych przez GLP jest zastosowanie znanego aparatu statystycznego. Możemy użyć testów zgodności z rozkładem jednostajnym, np. \textbf{testu Kołmogorowa-Smirnowa}. Niech $n$ będzie długością ciągu $(U_i)$. Dystrybuanta empiryczna jest zdefiniowana jako
\[ F_n(t) = \frac{1}{n} \sum_{i=1}^n \mathbbm{1}(U_i < t). \]
Niech
\[ D_n = \sup_{0 \leq t \leq 1} |F_n(t) - F(t)|. \]
Zauważmy, że obliczenie $D_n$ nie stanowi problemu, gdyż $F_n$ jest funkcją schodkową zmieniającą wartość w punktach $U_1, U_2,\ldots$. Twierdzenie Kołmogorowa mówi, że $\sqrt{n} D_n \stackrel{D}{\Conv} K$, gdzie $K$ jest zmienną losową o rozkładzie Kołmogorowa. Korzystając z tablic lub pakietów statystycznych możemy znaleźć $p$-wartość tego testu.

Można też użyć \textbf{testu $\chi^2$ Pearsona}. Polega on na podzieleniu odcinka na $r$ części. Niech $E_i$ będzie oczekiwaną liczbą zmiennych $U_i$, których wartość wpada to $i$-tego odcinka, zaś $O_i$ obserwowaną liczbą.

Wówczas statystyka
\begin{equation}
 \label{eq:chisq}
 T = \sum_{i=1}^r \frac{(O_i - E_i)^2}{E_i} 
\end{equation}
dąży do rozkład $\chi^2$ z $(r-1)$ stopniami swobody, oznaczany $\chi^2(r-1)$. Tak jak w przypadku testu Kołmogorowa-Smirnowa możemy poznać $p$-wartość testu korzystając z odpowiednich narzędzi.

Wiele testów zgodności opracowano dla rozkładu normalnego, do najbardziej znanych należą \textbf{test Shapiro-Wilka}, \textbf{test Jarque-Bera}, \textbf{test Andersona-Darlinga}. Aby móc z nich skorzystać, wystarczy zmapować ciąg $(U_i)$ na ciąg $(N_i)$ zmiennych losowych o rozkładzie normalnym, np. przy użyciu transformacji Boxa-Mullera.

Powyższe testy sprawdzają jedynie zgodność z rozkładem jednostajnym. Teoretycznie można ponownie wykorzystać test $\chi^2$ do sprawdzenia niezależności. Dla ustalonego $t$ dzielimy ciąg $(U_i)$ na bloki 
\begin{equation}
  \label{eq:u_blok}
 (U_1,\ldots,U_t), (U_{t+1},\ldots,U_{2t}), \ldots
\end{equation}

Otrzymujemy w ten sposób obserwacje, które powinny być jednostajnie rozłożone w hiperkostce $(0,1)^t$. Możemy ją podzielić na mniejsze kostki i skorzystać ze statystyki (\ref{eq:chisq}), aby stwierdzić czy wpada do nich odpowiednio wiele obserwacji. W praktyce jednak oczekiwana liczba obserwacji w pojedynczej kostce spada tak szybko do zera, że $T$ nie jest dobrze przybliżane przez rozkład~$\chi^2$.

\subsection*{Zgodność z twierdzeniami rachunku prawdopodobieństwa}
Wiele faktów w rachunku prawdopodobieństwa opiera się na ciągach niezależnych zmiennych losowych o jednakowym rozkładzie. Dzięki temu na podstawie ciągów $(X_i)$ lub $(U_i)$ jesteśmy w stanie otrzymać kolejne zmienne losowe, których teoretyczne rozkłady są znane. W \cite{knuth} zaproponowanych jest kilka praw, które można wykorzystać w ten sposób. Oto niektóre z nich:
\begin{itemize}
 \item \textbf{Test odstępów.} Dla ustalonego przedziału $(\alpha, \beta)$ mierzymy czasy oczekiwania na kolejne $U_i$ wpadające do tego przedziału. Otrzymane wartości powinny mieć rozkład geometryczny, co sprawdzamy testem $\chi^2$.
 \item \textbf{Test permutacyjny.} Podzielmy ciąg $(U_i)$ na bloki jak w (\ref{eq:u_blok}),  przy niezbyt dużym $t$. W każdym bloku zachodzi jedno z $t!$ możliwych uporządkowań. Rozkład na uporządkowaniach powinien być jednostajny, co ponownie weryfikujemy testem $\chi^2$.
 \item \textbf{Test kolizji.} Stanowi rozwiązanie, gdy mamy $n$ obserwacji wpadających do $m$ ,,pudełek'', przy czym $n < m$. Jak powiedzieliśmy wcześniej, w takiej sytuacji nie możemy zastosować testu $\chi^2$. Jednak da się wyliczyć teoretyczne prawdopodobieństwo otrzymania $k$ kolizji (kolizją jest trafienie obserwacji do pudełka, w którym jest już inna obserwacja). Jeśli zaobserwowana liczba kolizji nie mieści się w pewnych ramach, to możemy stwierdzić, że ciąg nie jest losowy.
\end{itemize}
Metody opisane tutaj mają pewną zaletę w stosunku do metod z poprzedniego paragrafu -- niejawnie testują również niezależność.

\subsection*{Zestawy testów}
Rozwinięciem podejścia z poprzedniego paragrafu jest tworzenie paczek testowych. Zawierają one kilkanaście lub więcej testów opartych o fakty rachunku prawdopodobieństwa, które powinien spełniać idealny ciąg losowych bitów. Znane przykłady to:
\begin{itemize}
 \item \textbf{Diehard tests.} Zestaw opracowany przez George'a Marsaglia w 1996 r. Obecnie uważany już za przestarzały.
 \item \textbf{TestU01.} Następca zestawu Diehard, który opracowali Pierre L’Ecuyer oraz Richard Simard w 2007 r.
 \item \textbf{NIST Test Suite.} Zestaw opracowany przez organizację \textit{National Institute of Standards and Technology} i ciągle rozwijany. 
\end{itemize}
 Powiemy więcej o tym ostatnim zestawie. 
 
Jakość GLP rozstrzygana jest w systematyczny sposób. Dla każdego spośród kilkunastu testów w NIST postępujemy następująco. Generujemy $m$ sekwencji bitów. Po kolei dla każdej z nich przeprowadzamy wybrany test na ustalonym poziomie istotności $\alpha = 0.01$. Test zwraca nam $p$-wartość, i jeżeli $p > \alpha$, to uznajemy, że dany ciąg bitów jest losowy. Przyjmuje się, że GLP zaliczył wykonywany test jeżeli około 97\% lub więcej sekwencji zostało uznanych za losowe (oczywiście nie można wymagać, żeby wszystkie ciągi zostały uznane za losowe, bo nawet spośród idealnych ciągów losowych bitów około $\alpha$ z nich zostanie odrzucona).

Takie podejście wydaje się rozsądne, ma jednak zasadniczą wadę. Wyobraźmy sobie, że mamy znakomity GLP $g_1$. Na jego podstawie tworzymy nowy GLP $g_2$ w taki sposób, że co setny ciąg bitów otrzymanych z $g_2$ to sekwencja samych zer, a w pozostałych przypadkach $g_2$ deleguje wygenerowanie ciągu do $g_1$. NIST Test Suite prawdopodobnie uznałby generator $g_2$ za dobry. Metoda opisana w części \ref{czesc:metoda} jest pozbawiona tej wady.

\subsection*{Test spektralny}
Testowi spektralnemu Donald Knuth poświęcił kilkanaście stron w swoim dziele \cite{knuth}, czego nie sposób tutaj streścić. Idea polega na spostrzeżeniu, że punkty w $t$-wymiarowej przestrzeni, utworzone z kolejnych wyrazów ciągu $(U_i)$ wygenerowanego przez LCG, leżą na stosunkowo niewielkiej liczbie $(t-1)$-wymiarowych hiperpłaszczyzn. Zagłębiając się w ten temat można dojść do dość skomplikowanej metody testowania LCG. Jednak w niektórych przypadkach widać gołym okiem, że generator jest zły. Takim przykładem jest niechlubny RANDU (jest to $MCG(2^{31}, 2^{16}+3)$). Na Rysunku \ref{fig:spectral} przedstawiono punkty w przestrzeni otrzymane z tego generatora. Widać wyraźnie, że układają się one na 15 płaszczyznach.
\begin{figure}[!ht]
 \centering
 \includegraphics[scale=0.4]{obrazki/spectraltest.pdf}
 \caption{Ilustracja rozmieszczenia w przestrzeni punktów generowanych przez RANDU. Każdy punkt został utworzony z trzech kolejnych liczb otrzymanych z generatora i przeskalowanych na odcinek $(0,1)$}
 \label{fig:spectral}
\end{figure}

\subsection*{Złożoność Kołmogorowa}
Zacznijmy od przykładu. Rozważmy ciągi binarne długości 32
\begin{nscenter}
\texttt{01010101010101010101010101010101} 
\end{nscenter}
oraz
\begin{nscenter}
\texttt{00110111001101001101101000110110}. 
\end{nscenter}
Choć wylosowanie obydwu z nich jest równie prawdopodobne, ten drugi uznajemy za bardziej ,,losowy''. Dzieje się tak dlatego, że pierwszy ciąg można jednoznacznie opisać w znacznie mniejszej liczbie znaków:
\begin{nscenter}
\texttt{16$\times$01},
\end{nscenter}
zaś najkrótszym opisem drugiego ciągu jest prawdopodobnie przepisanie go całego.

Tego typu intuicje próbujemy wyjaśniać za pomocą, tzw. złożoności Kołmogorowa. Służy ona do mierzenia stopnia skomplikowania ciągów znaków. Ustalmy dowolny język programowania $L$. Złożonością Kołmogorowa łańcucha znaków $s$ jest długość najkrótszego programu $P$ w języku $L$, takiego że $P$ wypisuje $s$.

Rzadko kiedy jesteśmy w stanie znaleźć dokładną wartość złożoności łańcucha $s$. Dlatego może się wydawać, że powyższe podejście jest czysto teoretyczne. Okazuje się jednak, że można złożoność $s$ oszacować. Niech $P$ będzie programem implementującym ustalony algorytm kompresji, a $\tilde{P}$ odpowiadającym mu programem dekompresji. Ponadto niech $\tilde{s}$ będzie skompresowanym łańcuchem $s$. Wówczas złożoność łańcucha $s$ jest oszacowana z góry przez sumę długości $\tilde{P}$ oraz $\tilde{s}$. Odrzucamy hipotezę o losowości $s$, gdy otrzymana wartość jest znacznie mniejsza od długości~$s$.

\subsection*{Zagadnienie ruiny gracza}
Bardzo interesująca metoda testowania GLP została przedstawiona w pracy \cite{kim-choe}. Rozważane jest błądzenie po grupie $\mathbb{Z}_n$. Autorzy badają czas dojścia do stanu $0$, co odpowiada zbiciu majątku wysokości $n$ lub bankructwu gracza ze znanego zagadnienia. W pracy opisane są trzy warianty zastosowanej metody, tutaj przedstawiamy podstawową z nich.

Ustalmy $p \in (0,1)$. Ciąg $(U_i)$ otrzymany z generatora wykorzystywany jest do poruszania się po grupie $\mathbb{Z}_n$. Jeśli w kroku $i$-tym byliśmy w stanie $s$, to przechodzimy do stanu $s+1$, gdy $U_{i+1} < p$, a do $s-1$ w przeciwnym przypadku. Dla każdego $x \in \mathbb{Z}_n$, $x \neq 0$ niech $T_x$ oznacza czas dojścia do $0$ ze stanu $x$. $N$ krotnie rozpoczynamy błądzenie po grupie z punktu $x$. W ten sposób otrzymujemy $N$ replikacji zmiennej $T_x$. Oznaczmy ich średnią przez $\overline{T}_x$. Niech $\mu$ oraz $\sigma^2$ oznaczają wartość oczekiwaną oraz wariancję $T_x$, gdy prawdopodobieństwa przejść do stanów $s+1$ oraz $s-1$ wynoszą odpowiednio $p$ oraz $1-p$. Wartości $\mu$ i $\sigma^2$ można wyznaczyć teoretycznie. Niech
\[ Z_x = \frac{\overline{T}_x - \mu}{\sigma \sqrt{N}}. \]
Przy założeniu hipotezy, o niezależności i rozkładzie jednostajnym zmiennych $(U_i)$, statystyka $Z_x$ ma w przybliżeniu rozkład normalny $\mathcal{N}(0,1)$. W związku z tym przy dużych wartościach $|Z_x|$ należy stwierdzić, że generator nie przeszedł testu w punkcie $x$. 

\subsection*{Błądzenie przypadkowe}
Testom opartym na własnościach błądzenia przypadkowego poświęcone są kolejne dwie części pracy.

\chapter{Metoda testowania}
\label{czesc:metoda}
W części \ref{czesc:bladzenie} przedstawiliśmy teorię przydatną do testowania GLP. W tej części pokazujemy jak zastosować ją w praktyce. Opisujemy w jaki sposób sprawdzać czy wygenerowane ciągi odpowiadają prawdziwie losowym realizacjom błądzenia przypadkowego. Przedstawiona tu metoda wykorzystująca prawo iterowanego logarytmu pochodzi z pracy \cite{wang-nic}. Dodatkowo proponujemy podobną metodę opartą o prawo arcusa sinusa.
%W wykonywanych obliczeniach często polegamy na aproksymacjach, dlatego w ostatnim paragrafie tej części analizujemy wielkość popełnianego błędu.



\section{Zastosowanie błądzenia przypadkowego do testowania GLP}

Ogólna idea testów GLP, którą stosujemy w tej pracy nie jest skomplikowana. Metoda polega na obliczeniu pewnych charakterystyk ciągów wygenerowanych przez GLP i porównaniu ich empirycznych rozkładów z rozkładami, które są znane dla idealnego ciągu losowych bitów.

Przykładowo, w części poświęconej prawu arcusa sinusa uzasadniliśmy, że bardziej prawdopodobna jest długa przewaga liczby jedynek nad liczbą zer niż równomierny rozkład prowadzenia. Jeśli generator sztucznie wyrównuje częstość zer i jedynek, to zauważymy odstępstwa od tej reguły. Zgodność z prawem arucsa sinusa sprawdzają testy oparte o zdefiniowaną poniżej charakterystykę $\Sasin{n}$.

Podobnie ktoś mógłby pomyśleć, że czymś pozytywnym byłyby niewielkie różnice między liczbą jedynek i zer w ciągu bitów otrzymanym z GLP. Moglibyśmy zdecydować się na jakąś ,,rozsądną'' stałą, powiedzmy 100, i uznać, że generator jest dobry jeśli różnica liczby zer i jedynek w ciągu nie przekroczy 100. Wszak duże różnice mogłyby sugerować, że mamy różne prawdopodobieństwa wystąpienia zer i jedynek. Jednak prawo iterowanego logarytmu pokazuje, że to rozumowanie jest błędne. \emph{Należy} spodziewać się fluktuacji i odstępstw od zera, a ich brak oznacza, że GLP nie generuje idealnego ciągu losowych bitów. Tę obserwację wykorzystują testy opartę o charakterystykę $\Slil{n}$.


\subsection{Test arcusa sinusa}
Niech
\begin{equation}
D_k = \mathbbm{1}\left(S_k > 0 \vee S_{k-1}>0 \right), k=1,2,\ldots,n.
\end{equation}
Zmienna $D_k$ przyjmuje wartość 1, gdy w $k$-tym kroku błądzenia przypadkowego zachodzi przewaga liczby jedynek nad zerami (traktując remisy tak jak w paragrafie \ref{par:asin}), zaś 0 w przeciwnym przypadku. Zdefiniujmy charakterystykę $\Sasin{n}$ wzorem
\begin{equation}
 \Sasin{n} = \frac{1}{n} \sum_{k=1}^n D_k.
\end{equation}
Zatem $\Sasin{n}$ jest to frakcja czasu podczas której jedynki dominowały nad zerami. Wiemy z paragrafu \ref{par:asin}, że
\[ \Pro{\Sasin{n} = \frac{k}{n}} = p_{k,n}, \]
zaś korzystając z prawa arcusa sinusa
\begin{equation}
\begin{split}
 \label{eq:prob_sasin}
  \Pro{\Sasin{n} \in (a,b)} &\approx \int_a^b \frac{dt}{\sqrt{t(1-t)}}\\
  &= \frac{2}{\pi}\arcsin(\sqrt{b}) - \frac{2}{\pi}\arcsin(\sqrt{a}).
\end{split}
\end{equation}

W celu przetestowania GLP generujemy przy jego użyciu $m$ ciągów zerojedynkowych długości~$n$. Otrzymujemy w ten sposób $m$ realizacji zmiennej $\Sasin{n}$, $j$-tą replikację oznaczamy $\Sasin{{n,j}}$. Ustalamy partycję prostej rzeczywistej i dla każdego odcinka w tej partycji zliczamy ile realizacji $\Sasin{n}$ do niego wpadło.

W testach opartych o wielkość $\Sasin{n}$ korzystamy z $(s+2)$-elementowej partycji postaci $\mathcal{P}^{asin}_s = \{ P^{asin}_0, P^{asin}_1, \ldots, P^{asin}_{s+1}\}$, gdzie
\begin{equation*}
\begin{split}
  P^{asin}_0 &= \left(-\infty, -\frac{1}{2s}\right),\\
  P^{asin}_i &= \left[\frac{2i-3}{2s}, \frac{2i-1}{2s} \right),\ \ \ 1 \leq i \leq s,\\
  P^{asin}_{s+1} &= \left[1- \frac{1}{2s}, \infty\right).
\end{split}
\end{equation*}

Możemy teraz zdefiniować dwie miary określone na partycji  $\mathcal{P}^{asin}_s$. Pierwsza z nich, $ \mu^{asin}_n$, reprezentuje teoretyczny rozkład rozważanej charakterystyki:
\begin{equation}
\label{eq:mu_asin}
 \mu^{asin}_n \left( P^{asin}_i \right) = \Pro{\Sasin{n} \in P^{asin}_i},\ \ \ 0 \leq i \leq s+1.
\end{equation}
Powyższą wartość możemy wyliczyć ze wzoru (\ref{eq:prob_sasin}).

Druga miara, $\nu^{asin}_n$,  reprezentuje rozkład empiryczny, wyznaczony w testach. Określamy
\begin{equation}
 \label{eq:emp_asin}
 \nu^{asin}_n \left( P^{asin}_i \right) = \frac{|\{ j:\ \Sasin{{n,j}} \in  P^{asin}_i, 1 \leq j \leq m\}|}{m},\ \ \ 0 \leq i \leq s+1.
\end{equation}


Jeżeli testowany GLP jest dobry, to obie miary powinny być ,,mniej więcej takie same''. Ściślej, odległość między otrzymanymi miarami powinna być mała. Korzystamy ze znanych funkcji odległości \textit{total variation distance} oraz \textit{separation distance}. Są one zdefiniowane następująco dla dowolnych miar $\mu$ oraz $\nu$:
\begin{align}
 d^{tv}(\mu, \nu) &= \sup_{A \subseteq \mathbb{R}} |\mu(A) - \nu(A)|,\\
 d^{sep}(\mu, \nu) &= \sup_{A \subseteq \mathbb{R}} \left(1 - \frac{\mu(A)}{\nu(A)}\right)
\end{align}
Ponieważ nie jesteśmy w stanie wyznaczyć supremum na całej prostej, skorzystamy z nieco uproszczonych definicji. Dla partycji $\mathcal{P}$ prostej rzeczywistej, definiujemy:
\begin{align}
 d^{tv}_\mathcal{P}(\mu, \nu) &= \frac{1}{2} \sum_{A \in \mathcal{P}} |\mu(A) - \nu(A)|
    = \sum_{\substack{A \in \mathcal{P},\\ \mu(A) >\nu(A)}} \Big(\mu(A) - \nu(A)\Big)\\
 d^{sep}_\mathcal{P}(\mu, \nu) &= \sup_{A \in \mathcal{P}}\left(1 - \frac{\mu(A)}{\nu(A)}\right)
\end{align}

Inne podejście polega na testowaniu hipotezy o zgodności rozkładów. Korzystając z terminologii statystycznej wielkości $\Sasin{{n,j}}$ będziemy nazywać obserwacjami. Hipotezą zerową jest stwierdzenie, że obserwacje mają rozkład $\mu^{asin}_n$, czyli de facto, że GLP generuje idealny ciąg losowych bitów. Niech $O_i$ oznacza liczbę obserwacji wpadających do przedziału $P^{asin}_i$, tzn.
\begin{equation}
 O_i = |\{ j:\ \Slil{{n,j}} \in  P^{asin}_i, 1 \leq j \leq m\}|,\ \ \ 0 \leq i \leq s+1,
\end{equation}
oraz $E_i$ oznacza \textit{oczekiwaną} liczbę obserwacji wpadających do tego przedziału, czyli
\begin{equation}
 E_i = m \cdot \Pro{\Sasin{n} \in P^{asin}_i},\ \ \ 0 \leq i \leq s+1.
\end{equation}
Wartość $E_i$ znamy dokładnie ze wzoru (\ref{eq:prob_sasin}). Przy hipotezie zerowej statystyka
\begin{equation}
\label{eq:chi_asin}
 T = \sum_{i=0}^{s+1} \frac{(O_i - E_i)^2}{E_i}
\end{equation}
ma w przybliżeniu rozkład chi-kwadrat z $s+1$ stopniami swobody, oznaczany $\chi^2(s+1)$. Duże wartości tej statystyki są dowodem wadliwości GLP.


\subsection{Test iterowanego logarytmu}
Przyjrzyjmy się teraz metodzie zastosowanej w \cite{wang-nic}. Jest ona podobna do metody opisanej w poprzednim paragrafie. Liczymy jedynie inną charakterystykę ciągów i dostosowujemy partycję prostej. Przypomnijmy oznaczenie
\begin{equation}
\Slil{n} = \frac{S_n}{\sqrt{2n \log \log n}}.
\end{equation}
Można łatwo znaleźć teoretyczny rozkład tej charakterystyki (tzn. rozkład dla idealnego ciągu losowych bitów). Istotnie, korzystając z centralnego twierdzenia granicznego dostajemy
\begin{equation}
\begin{split}
  \label{eq:prob_slil}
 \Pro{\Slil{n} \in (a,b)} &= \Pro{\frac{S_n}{\sqrt{n}} \in \left(a\sqrt{2 \log \log n},  b\sqrt{2 \log \log n}\right)}\\
 &\approx \Phi(b\sqrt{2 \log \log n}) - \Phi(a\sqrt{2 \log \log n})
\end{split}
\end{equation}
Jako, że $\Slil{n}$ przyjmuje swoje wartości w szerszym przedziale niż $\Sasin{n}$, dlatego korzystamy z innej partycji prostej, mianowicie $\mathcal{P}^{lil}_s = \{ P^{lil}_0, P^{lil}_1, \ldots, P^{lil}_{s+1}\}$, gdzie
\begin{equation*}
\begin{split}
  P^{lil}_0 &= (-\infty, -1),\\
  P^{lil}_i &= \left[-1 + \frac{2(i-1)}{s}, -1 + \frac{2i}{s} \right),\ \ \ 1 \leq i \leq s,\\
  P^{lil}_{s+1} &= [1, \infty).
\end{split}
\end{equation*}
Teoretyczny i empiryczny rozkład określamy w tym przypadku następująco:
\begin{equation}
 \label{eq:mu_lil}
 \mu^{lil}_n \left( P^{lil}_i \right) = \Pro{\Slil{n} \in P^{lil}_i},\ \ \ 0 \leq i \leq s+1,
\end{equation}
\begin{equation}
 \label{eq:emp_lil}
 \nu^{lil}_n \left( P^{lil}_i \right) = \frac{|\{ j:\ \Slil{{n,j}} \in  P^{lil}_i, 1 \leq j \leq m\}|}{m},\ \ \ 0 \leq i \leq s+1,
\end{equation}
przy czym wartość (\ref{eq:mu_lil}) znamy dzięki (\ref{eq:prob_slil}), zaś $\Slil{{n,j}}$ oznacza oczywiście replikacje $\Slil{n}$, obliczone dla kolejnych ciągów.

\section{Analiza błędu}
Omawiając test arcusa sinusa oraz test iterowanego logarytmu dokonywaliśmy w obliczeniach pewnych przybliżeń. Wystarcza to do przekazania idei opisanych metod testowania, jednak dla porządku należy uzasadnić, że błąd aproksymacji nie ma istotnego wpływu. W \cite{wang-nic} pokazano, że dla $n \geq 26$ błąd przybliżenia w (\ref{eq:prob_slil}) jest pomijalny. Tutaj oszacujemy błąd popełniany w (\ref{eq:prob_sasin}).

W przypadku testu arcusa sinusa korzystaliśmy z aproksymacji dwukrotnie: najpierw przybliżając $p_{2k,2n}$ używając wzoru Stirlinga, a później przybliżając sumę całką. Dla analizy pierwszego z tych przybliżeń przyda nam się następujący fakt, pochodzący z \cite{leja}.
\begin{lemat}
 Dla każdej liczby naturalnej $n$ istnieje liczba $\theta_n$, $0 < \theta_n \leq 1$, taka że
 \[ n! = \sqrt{2\pi n} \left( \frac{n}{e} \right)^n \exp\left\{\frac{\theta_n}{12n}\right\} \].
\end{lemat}
Uzupełniając obliczenia, które wykonaliśmy, aby otrzymać (\ref{eq:disc_asine_approx}) o czynnik $\exp\left\{\frac{\theta_n}{12n}\right\}$ otrzymujemy
\begin{equation}
 \label{eq:blablabla}
 p_{2k,2n} = \frac{1}{\pi \sqrt{k(n-k)}} \exp\left\{ \frac{\theta_{2k} - 4\theta_k}{24k} + \frac{\theta_{2(n-k)} - 4\theta_{n-k}}{24(n-k)} \right\}
\end{equation}
Oznaczmy
\[ d_{k,n} = \frac{1}{\pi \sqrt{k(n-k)}} \]
Naszym celem jest pokazanie, że $|p_{2k,2n} - d_{k,n}|$ jest małe. Z (\ref{eq:blablabla}) dostajemy
\[ \frac{p_{2k,2n}}{d_{k,n}} \leq \exp\left\{ \frac{1}{24k} + \frac{1}{24(n-k)} \right\} = \exp\left\{ \frac{n}{24k(n-k)} \right\} \]
oraz
\[ \frac{p_{2k,2n}}{d_{k,n}} \geq \exp\left\{ \frac{-4}{24k} + \frac{-4}{24(n-k)} \right\} = \exp\left\{ -\frac{n}{6k(n-k)} \right\}. \]
Korzystając z powyższych i z nierówności $e^x - 1 \leq 2x$ (dla $x > 0$ i dostatecznie małych) oraz $1 - e^{-x} \leq x$ uzyskujemy
\[ p_{2k,2n} - d_{k,n} \leq d_{k,n} \left(\exp\left\{ \frac{n}{24k(n-k)} \right\} - 1\right) \leq d_{k,n}\frac{n}{12k(n-k)} \]
oraz 
\[ d_{k,n} - p_{2k,2n} \leq d_{k,n} \left( 1 -\exp\left\{ -\frac{n}{6k(n-k)} \right\} \right) \leq d_{k,n}\frac{n}{6k(n-k)} \]
co razem daje 
\[ |p_{2k,2n} - d_{k,n}| \leq  d_{k,n}\frac{n}{6k(n-k)} = \frac{n}{6\pi\left( k(n-k) \right)^{\frac{3}{2}}}. \]
Ustalmy $\delta > 0$ i założymy dodatkowo, że $\delta < \frac{k}{n} < 1 - \delta$. Funkcja $k \mapsto \left( k(n-k) \right)^{3/2}$ przyjmuje minimalną wartość na brzegu przedziału, w którym się ją rozpatruje, dlatego
\[ |p_{2k,2n} - d_{k,n}| \leq \frac{n}{6\pi\left( \delta n (n- \delta n) \right)^{\frac{3}{2}}} = \frac{1}{6\pi n^2 \left( \delta(1- \delta) \right)^{\frac{3}{2}}}.  \]
Wielkość błędu aproksymacji w (\ref{eq:prob_sasin}) oszacujemy w dwóch etapach. Na początek weźmy takie liczby $a,b$, że $\delta < a < b < 1 - \delta$.
Wtedy
\begin{equation*}
 \begin{split}
  \left| \sum_{a < \frac{k}{n} < b} p_{2k,2n} -  \sum_{a < \frac{k}{n} < b}  d_{k,n} \right| &\leq \sum_{a < \frac{k}{n} < b} \left|  p_{2k,2n} -  d_{k,n} \right| = \sum_{a < \frac{k}{n} < b} \frac{1}{6\pi n^2 \left( \delta(1- \delta) \right)^{\frac{3}{2}}} \\
  &= \frac{\ceil{bn - an}}{6\pi n^2 \left( \delta(1- \delta) \right)^{\frac{3}{2}}} \leq \frac{b - a}{3\pi n \left( \delta(1- \delta) \right)^{\frac{3}{2}}} \\
  &\leq \frac{1}{3\pi n \left( \delta(1- \delta) \right)^{\frac{3}{2}}} = (\blacklozenge)
 \end{split}
\end{equation*}
Drugim źródłem niedokładności jest zastąpienie sumy przez całkę. Rozpatrzmy dowolną funkcję $f$, różniczkowalną w przedziale $(a,b)$. Podzielmy $(a,b)$ na odcinki długości $\frac{1}{n}$ i niech $x_k$ będzie dowolnym punktem w odcinku zawierającym $\frac{k}{n}$, a $M_k$ i $m_k$ odpowiednio maksymalną i minimalną wartością funkcji w tym odcinku. Korzystając z twierdzenia Lagrange'a dostajemy
% \begin{equation}
%  \begin{split}
%     \left| \int_a^b f(x) dx - \sum\limits_{i=1}^{n}\frac{b-a}{n}f(x_i) \right| &\leq \sum\limits_{i=1}^{n}\frac{b-a}{n}(M_i - m_i) = \sum\limits_{i=1}^{n}\frac{(b-a)^2}{n^2}|f'(\xi_i)| \\
%     &\leq \sum\limits_{i=1}^{n}\frac{(b-a)^2}{n^2} \sup_{a < x < b} |f'(x)| = \frac{(b-a)^2}{n} \sup_{a < x < b} |f'(x)|.
%  \end{split}
% \end{equation}
\begin{equation}
 \begin{split}
    \left| \int_a^b f(x) dx - \sum\limits_{a < \frac{k}{n} < b}\frac{1}{n}f(x_k) \right| &\leq \sum\limits_{a < \frac{k}{n} < b}\frac{1}{n}(M_i - m_i) = \sum\limits_{a < \frac{k}{n} < b}\frac{1}{n^2}|f'(\xi_i)| \\
    &\leq \sum\limits_{a < \frac{k}{n} < b}\frac{1}{n^2} \sup_{a < x < b} |f'(x)| = \frac{\ceil{bn-an}}{n^2} \sup_{a < x < b} |f'(x)| \\
    &\leq \frac{2(b-a)}{n} \sup_{a < x < b} |f'(x)|.
 \end{split}
\end{equation}
W szczególności dla
\[ f(x) = \frac{1}{\pi \sqrt{x(1-x)}} \] mamy
\[ f'(x) = \frac{2x-1}{2 \pi (x(1-x))^{\frac{3}{2}}}, \]
\[ \frac{1}{n}f\left(\frac{k}{n} \right) = \frac{1}{n\pi \sqrt{ \frac{k}{n}(1-\frac{k}{n})}} = \frac{1}{\pi \sqrt{k(n-k)}} = d_{k,n}  \]
a stąd w interesującym nas przedziale
\[  \left| \int_a^b f(x) dx - \sum\limits_{a < \frac{k}{n} < b}d_{k,n} \right| \leq \frac{2}{n} \sup_{\delta < x < 1-\delta} |f'(x)| = \frac{1 - 2\delta}{\pi n (\delta(1-\delta))^{\frac{3}{2}}} = (\bigstar) \]

W testach wykorzystamy partycję prostej $\mathcal{P}^{asin}_{40}$, dlatego u nas $\delta = \frac{1}{80} > 0.01$. Będzie ponadto $n \geq 2^{26}$. Dlatego
\begin{equation*}
 \begin{split}
  (\blacklozenge) &\leq \frac{107.72}{n} \leq 1.6 \cdot 10^{-6} \\
  (\bigstar) &\leq \frac{316.69}{n} \leq 4.7 \cdot 10^{-6}
 \end{split}
\end{equation*}
Ostatecznie
\begin{equation*}
\begin{split}
\left| \int_a^b f(x) dx - \sum\limits_{a < \frac{k}{n} < b} p_{2k,2n} \right|
  &\leq \left| \int_a^b f(x) dx - \sum\limits_{a < \frac{k}{n} < b} d_{k,n} \right| +  \left| \sum\limits_{a < \frac{k}{n} < b} d_{k,n} - \sum\limits_{a < \frac{k}{n} < b} p_{2k,2n} \right| \\
  &= (\blacklozenge) + (\bigstar) \leq 6.3 \cdot 10^{-6}
\end{split} 
\end{equation*}
co uzasadnia wzór (\ref{eq:prob_sasin}) w przypadku $\delta < a < b < 1 - \delta$. 

Musimy jeszcze zbadać niedokładność ,,na brzegu''. Mamy
\begin{equation*}
\begin{split}
  \left| \int_0^\delta f(x) dx - \sum\limits_{0 < \frac{k}{n} < \delta} p_{2k,2n} \right| &= \left| \int_0^{\frac{1}{2}} f(x) dx -  \int_\delta^{\frac{1}{2}} f(x) dx - \sum\limits_{0 < \frac{k}{n} < \frac{1}{2}} p_{2k,2n} + \sum\limits_{\delta < \frac{k}{n} < \frac{1}{2}} p_{2k,2n} \right| \\
  &= \left| \frac{1}{2} -  \int_\delta^{\frac{1}{2}} f(x) dx - \frac{1}{2} + \sum\limits_{\delta < \frac{k}{n} < \frac{1}{2}} p_{2k,2n} \right| \\
  &= \left|\int_\delta^{\frac{1}{2}} f(x) dx - \sum\limits_{\delta < \frac{k}{n} < \frac{1}{2}} p_{2k,2n} \right| \leq 6.3 \cdot 10^{-6},
\end{split}
\end{equation*}
gdzie ostatnie przejście wynika z wcześniej przeprowadzonych rachunków. Powyższe rachunki pokazują, że błąd przybliżenia jest pomijalny.


\chapter{Testy}
\label{czesc:testy}

Przebrnąwszy przez długie teorie i rozważania, przechodzimy do najciekawszej części pracy, czyli do implementacji opisanej metody testowania i sprawdzenia jej na powszechnie wykorzystywanych generatorach.

\section{Implementacja}
Testy oparte o prawa arcusa sinusa i iterowanego logarytmu przedstawione w części \ref{czesc:metoda} zaimplementowałem w języku Julia. Napisany program stanowi równie ważną część tej pracy.

Julia jest to nowoczesny (prace nad nim rozpoczęto w roku 2009) i szybki język programowania przeznaczony do obliczeń naukowych. Jest to więc narzędzie bardzo dobrze nadające się do naszych eksperymentów.

Powstały skrypt można wykorzystać do przetestowania dowolnego generatora. Trzeba zadbać jedynie o to, aby wyjście z GLP było przekazywane do programu testującego w odpowiednim formacie. Poniżej opisujemy dokładniej jak to zrobić. Jednak najpierw omówmy kilka prozaicznych kwestii, które wpłynęły na końcową architekturę programu.

\subsection{Uwagi praktyczne}
Najwygodniejszym sposobem testowania GLP byłoby prawdopodobnie podzielenia zadania na dwa etapy. W pierwszej kolejności użylibyśmy GLP do wygenerowania $m$ sekwencji bitów długości $n$, które zapisalibyśmy do pliku. Następnie program testujący wczytałby ten plik i wyliczył odpowiednie statystki, rozstrzygnął czy dane są losowe, itd. Niestety nie da się zaprojektować systemu w taki sposób, aby opisany tryb pracy był możliwy.

Powodem jest olbrzymia ilość używanych danych. W celu przetestowania większości GLP wykorzystujemy je do wygenerowania $m = 10000$ ciągów bitów długości $n = 2^{34}$. Rodzi to niebagatelne problemy implementacyjne. Pojedynczy ciąg ma rozmiar $2GB$, więc w pamięci przeciętnego komputera nie zmieści się ich nawet kilka. Wymusza to przetwarzanie ciągów jednego po drugim -- po obliczeniu statystyk dla jednego ciągu należy natychmiast zwolnić pamięć dla kolejnego. Większym problemem jest jednak fakt, że dane zajmują łącznie $20TB$, więc zapisanie ich do pliku jest możliwe na mało której maszynie -- co uzasadnia dlaczego pomysł przedstawiony w poprzednim akapicie jest niewykonalny.

Jedyną możliwością jest postępowanie w taki sposób, by GLP i skrypt funkcjonowały naprzemiennie. GLP generuje sekwencję bitów, program testujący ją analizuje, po czym pamięć zostaje zwolniona i możemy powtórzyć procedurę. Na szczęście istnieje prosty środek pozwalający zorganizować obliczenia w ten sposób. Mowa tu o unixowym mechanizmie potoku. Wystarczy wyniki z GLP przekazywać na standardowe wyjście, które będzie połączone ze standardowym wejściem programu testującego. System operacyjny sam zadba o to, żeby oba procesy działały na zmianę.

Kolejną istotną kwestią jest tryb zapisu danych do strumienia wejścia-wyjścia. Zwróćmy uwagę, że gdybyśmy przekazywali dane w trybie tekstowym, to przekazywany łańcuch znaków zajmowałby 8 razy więcej miejsca niż jest to potrzebne -- każdy bit byłby reprezentowany jako jednobajtowy znak \texttt{'0'} lub \texttt{'1'}. Byłoby to fatalne podejście, gdyż, po pierwsze, prawdopodobnie nie starczyłoby pamięci do zapisania całego ciągu. Po drugie, nawet gdyby pamięci było wystarczająco, to zapisywanie i wczytywanie tych danych do i ze strumienia kilkukrotnie wydłużyłoby (i tak bardzo długi) czas pracy programu. Dlatego dane przekazujemy w trybie binarnym.

Ostatnią rzeczą, na którą warto zwrócić uwagę, jest opłacalność oddzielenia obliczania charakterystyk ciągów ($\Sasin{n}$ lub $\Slil{n}$) od ich porównywania z teoretycznym rozkładem. Lepiej jest zapisywać wartości charakterystyk do pliku, a następnie oddzielnym skryptem badać zgodność z oczekiwanym rozkładem. Dzięki takiemu podejściu można uruchomić instancje generatora na wielu maszynach (zadbawszy o to by korzystały one z innych ziaren). Po zakończeniu obliczeń łatwo jest scalić wyniki i wyznaczyć sumaryczne statystyki.

\subsection{Użycie programu}
\paragraph{Uruchomienie.}
Załóżmy, że dysponujemy programem \texttt{gen.bin} generującym sekwencje pseudolosowych bitów. Powiedzmy, że przyjmuje on z linii poleceń dwa argumenty oznaczające liczbę i logarytm długości generowanych ciągów. Opiszmy jak przetestować ten generator.

Z perspektywy użytkownika najważniejsze jest, że punkt startowy programu testującego jest w pliku \texttt{Tester.jl}. Program wczytuje ze standardowego wejścia strumień bitów. Do pliku podanego w linii poleceń zapisuje wyniki swoich obliczeń, tj. wartości $\Sasin{n}$ lub $\Slil{n}$. Po zakończeniu działania używamy skryptu \texttt{ResultReader.jl} do wyznaczenia rozkładów empirycznych ze wzorów (\ref{eq:emp_asin}) i (\ref{eq:emp_lil}). Skrypt następnie wyliczy odległości $d^{tv}$ i $d^{sep}$ oraz $p$-wartości testu zgodności~$\chi^2$.

Skrypt \texttt{Tester.jl} przyjmuje z linii poleceń następujące argumenty:
\begin{itemize}
 \item \texttt{testType} -- słowo \texttt{asin} lub \texttt{lil} oznaczające którą z charakterystyk $\Sasin{n}$ i $\Slil{n}$ obliczamy,
 \item \texttt{nrOfCheckPoints} -- Dla ciągów długości $n$ do pliku wynikowego zapisujemy nie tylko $S^{\bullet}_{n}$, ale również $S^{\bullet}_{n/2}$, $S^{\bullet}_{n/4}$, itd.  \texttt{nrOfCheckPoints} to liczba tych wartości.
 \item \texttt{pathToFile} -- nazwa pliku do którego zapisujemy wyniki.
\end{itemize}
Podobne argumenty ma skrypt \texttt{ResultReader.jl}:
\begin{itemize}
 \item \texttt{testType} -- słowo \texttt{asin} lub \texttt{lil} mówiące, z którą teoretyczną miarą należy porównywać wyniki,
 \item \texttt{logLength} -- liczba naturalna oznaczająca logarytm długości ciągów wykorzystanych do otrzymania podanych wyników,
 \item \texttt{pathToFile} -- nazwa pliku z którego odczytujemy wyniki.
\end{itemize}
Wróćmy do generatora \texttt{gen.bin}. Możemy go przetestować wywołując z konsoli przykładowo \newline
\texttt{\hspace*{20pt}\$ ./gen.bin 1000 25 | julia Main.jl asin 5 wyniki.csv}\newline
\noindent W ten sposób testujemy generator \texttt{gen.bin} na podstawie 1000 ciągów zawierających $2^{25}$ bitów przy użyciu charakterystyki $\Sasin{n}$. Wyniki znajdą się w pliku \texttt{wyniki.csv}. Po zakończeniu obliczeń możemy je zinterpretować poleceniem \newline
  \texttt{\hspace*{20pt}\$ julia ResultReader.jl asin 25 wyniki.csv} \newline
Podobnie można przetestować dowolny inny generator, pamiętając, że jego wyjście musi być zapisane zgodnie z formatem opisanym poniżej.

\paragraph{Testowanie własnych GLP.}
Z punktu widzenia osoby, która chce wykorzystać program do sprawdzenia swojego GLP ważne jest co dokładnie ma się znaleźć w strumieniu wejściowym programu testującego. Format jest prosty:
\begin{itemize}
 \item Pierwsze 8 bajtów strumienia zawiera 64-bitową wartość typu \texttt{integer} oznaczającą liczbę ciągów bitów.
 \item Kolejne 8 bajtów zawiera 64-bitową wartość typu \texttt{integer} oznaczającą długość pojedynczego ciągu.
 \item Dalej następuje $m \cdot n$ bitów danych, przy czym każde kolejne $n$ bitów traktowane jest jako jeden ciąg używany w testach.
\end{itemize}
\emph{Uwaga.} Nie ma żadnych ,,specjalnych'' bitów oznaczających przerwy między ciągami, ani niczego podobnego. Po wczytaniu $n$ bitów jednego ciągu, kolejny bit jest uważany za pierwszy bit następnego ciągu.


\section{Wyniki}
Przyjrzyjmy się wynikom testowania kilku znanych GLP. Każdy z nich testowany był przy użyciu $m = 10000$ ciągów. Długością używanej sekwencji było w większości przypadków $n=2^{34}$.  

Wartości $\Sasin{\bullet}$ lub $\Slil{\bullet}$ obliczone zostały nie tylko dla całych ciągów, ale także dla podciągów długości $n/2$, $n/4$, itd. Pozwala to obserwować zgodność z pożądaną miarą na różnych etapach. Do badania zgodności użyto partycji $\mathcal{P}^{lil}_{40}$ dla testów korzystających z charakterystyki $\Slil{n}$ i partycji $\mathcal{P}^{asin}_{40}$ dla testów korzystających z charakterystyki $\Sasin{n}$. Dla kolejnych podciągów zostały wyznaczone miary empiryczne 

Do testów wykorzystano własne implementacje rozpatrywanych GLP (za wyjątkiem MT19937). Program wywołujący GLP korzystał losowych ziaren pobranych ze strony www.random.org. Przed wygenerowaniem każdej kolejnej sekwencji bitów ustawiano nowe ziarno generatora. 

Podkreślmy, że otrzymanie poniższych wyników nie było błahostką. Przetestowanie jednego GLP na 10 komputerach w pracowni Instytutu Informatyki zajmowało około półtorej doby.

% \begin{table}[ht!]
% \centering
%  \caption{Wyniki dla $\Sasin{n}$.}
%  \label{tab:xxx_asin}
% \begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
%  \hline
%      n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
% 
%  
% \end{tabular}  
% \end{table}
% \begin{table}[ht!]
% \centering
%  \caption{Wyniki dla $\Slil{n}$.}
%  \label{tab:xxx_lil}
% \begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
%  \hline 
%      n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
%  
% \end{tabular}  
% \end{table}

\subsection{RANDU}
Na rozgrzewkę rozpoczynamy od generatora, który od dawna nie jest w użyciu. RANDU powstał na początku lat 60. Wspominaliśmy już o nim w paragrafie \ref{sec:pop_testy}, dając go jako przykład generatora fatalnie oblewającego test spektralny.

RANDU jest to po prostu $MCG(2^{31}, 65539)$. Wybór liczby $65539$ wydawał się dobry, gdyż $65539 = 2^{16} + 3$, co umożliwiało szybkie, sprzętowe wykonanie mnożenia.

Rezultaty testowania RANDU przedstawione są w Tabelach \ref{tab:randu_asin} i \ref{tab:randu_lil}. Pierwszy wiersz w tabelach oznacza długości podciągów. Dla podciągu długości $n_k$ została obliczona teoretyczna miara $\mu_{n_k}$ według wzoru (\ref{eq:mu_asin}) lub (\ref{eq:mu_lil}) oraz empiryczna miara $\nu_k$ według wzoru (\ref{eq:emp_asin}) lub (\ref{eq:emp_lil}). Każda kolumna zawiera wartości $d^{tv}(\mu_{n_k}, \nu_{n_k})$, $d^{sep}(\mu_{n_k}, \nu_{n_k})$, $d^{sep}(\nu_{n_k}, \mu_{n_k})$ oraz $p$-wartość statystyki (\ref{eq:chi_asin}). Dla wszystkich GLP tabele z wynikami zrobione są w ten sam sposób.

\begin{table}[ht!]
\centering
 \caption{Wyniki testowania generatora RANDU charakterystyką $\Sasin{n}$.}
 \label{tab:randu_asin}
\begin{tabular} {||c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{21}$ &  $2^{22}$ &  $2^{23}$ &  $2^{24}$ &  $2^{25}$ &  $2^{26}$\\ \hline
     tv &  0.4604 &  0.4616 &  0.4637 &   0.467 &  0.4695 &  0.4697\\ \hline
   sep1 &  0.6017 &  0.6646 &  0.6229 &  0.6138 &  0.5934 &  0.6453\\ \hline
   sep2 &  0.8659 &  0.8662 &  0.8667 &  0.8675 &  0.8681 &  0.8682\\ \hline
  p-val &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0\\ \hline
\end{tabular}  
\end{table}

\begin{table}[ht!]
\centering
 \caption{Wyniki testowania generatora RANDU charakterystyką $\Slil{n}$.}
 \label{tab:randu_lil}
\begin{tabular} {||c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{21}$ &  $2^{22}$ &  $2^{23}$ &  $2^{24}$ &  $2^{25}$ &  $2^{26}$\\ \hline
     tv &  0.4955 &   0.496 &  0.4965 &  0.4969 &  0.4973 &  0.4977\\ \hline
   sep1 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
   sep2 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
  p-val &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0\\ \hline
\end{tabular}  
\end{table}

Jak widać nie trzeba bardzo długich ciągów, aby przekonać się, że wyjścia RANDU nie można uznać za losowe. Nie bez przyczyny został on szybko wyparty przez lepsze generatory.

\FloatBarrier
\subsection{Biblioteczny rand w Microsoft Visual C++}
Funkcja \texttt{rand} w Microsoft Visual C++ opiera się o $LCG(2^{32}, 214013, 2531011)$. Od zwykłego LCG różni się jednak tym, że zwracane są jedynie bity na pozycjach 30..16. 

Jest to jeden z generatorów testowanych w \cite{wang-nic}. Podobnie jak autorzy tej pracy odrzucamy najmniej istotne 7 bitów liczb zwracanych przez funkcję \texttt{rand} (czyli używamy tylko bitów 30..23 liczby otrzymanej z LCG). Wyniki testów zestawiono w Tabelach \ref{tab:svis_asin} i \ref{tab:svis_lil}.

\begin{table}[ht!]
\centering
 \caption{Wyniki testowania standardowego generatora w Visual C++ charakterystyką $\Sasin{n}$.}
 \label{tab:svis_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0289 &  0.0387 &  0.0615 &  0.0768 &  0.0848 &  0.0928 &  0.0965 &   0.187 &  0.2093\\ \hline
   sep1 &   0.203 &  0.1548 &  0.1952 &  0.2605 &  0.3664 &   0.421 &  0.5528 &  0.6533 &  0.8163\\ \hline
   sep2 &   0.178 &   0.226 &  0.2524 &  0.2364 &  0.2841 &   0.316 &  0.4336 &  0.6112 &  0.4328\\ \hline
  p-val &  0.0128 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0\\ \hline 
\end{tabular}  
\end{table}

\begin{table}[ht!]
\centering
 \caption{Wyniki testowania standardowego generatora w Visual C++ charakterystyką $\Slil{n}$.}
 \label{tab:svis_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &   0.035 &  0.0512 &  0.0938 &  0.1234 &  0.1672 &  0.2423 &    0.31 &  0.4991 &    0.95\\ \hline
   sep1 &  0.2831 &  0.7419 &  0.9731 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
   sep2 &   0.267 &  0.1416 &  0.2635 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
  p-val &  0.0001 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0\\ \hline
\end{tabular}  
\end{table}

Pamiętajmy, że okresem tego LCG jest $2^{31}$, a z jednego wywołania otrzymujemy $8 = 2^3$ bitów. Oznacza to, że aby dostać ciąg bitów długości $2^{34}$ przechodzimy przez pełen cykl generatora. Musieliśmy więc każdy układ 8 bitów wygenerować tyle samo razy, skąd wniosek, że po $2^{34}$ krokach błądzenie losowe zawsze wracało do zera. Widać to w wynikach testowania charakterystyką $\Slil{n}$. Wszystkie obserwacje wpadają do przedziału $[0, 0.05)$. Jego teoretyczna miara wynosi $\sim 0.05$, a stąd $d^{tv}(\mu^{lil}_n, \nu^{lil}_n) \approx 0.95$.

Ktoś złośliwy mógłby powiedzieć, że w bardzo złożony sposób udowodniliśmy oczywisty fakt, że generatory o krótkich okresach nie nadają się do generowania dużych ilości liczb pseudolosowych. Zauważmy jednak, że analizowany generator ma problemy już przy $n=2^{26}$, a do wygenerowania błądzenia tej długości potrzeba tylko $2^{23} / 2^{31} = 1/2^8 \approx 0.4\%$ całego okresu. Trudno jest więc ocenić ten generator dobrze, nawet w kategorii generatorów o krótkich okresach.

\textbf{Co zaskakujące, NIST Test Suite uznaje ten generator za poprawny}, jak zauważyli autorzy \cite{wang-nic}.

\FloatBarrier
\subsection{Biblioteczny rand w Borland C/C++}
Funkcja \texttt{rand} w środowisku Borland jest implementacją $LCG(2^{32}, 22695477, 1)$, która, podobnie jak \texttt{rand} w Visual C++, zwraca jedynie bity 30..16.

Postępując tak jak w poprzednim przykładzie bierzemy do testów tylko 8 najistotniejszych bitów zwróconej liczby.
\begin{table}[ht!]
\centering
 \caption{Wyniki testowania standardowego generatora w Borland C/C++ charakterystyką $\Sasin{n}$.}
 \label{tab:borland_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0384 &  0.0502 &  0.0697 &  0.0861 &  0.1394 &  0.1562 &  0.1231 &  0.1466 &  0.2148\\ \hline
   sep1 &  0.1293 &   0.184 &  0.2513 &  0.3733 &  0.5485 &  0.5149 &  0.5696 &  0.6873 &  0.8219\\ \hline
   sep2 &  0.2024 &   0.212 &  0.2096 &  0.3137 &  0.3702 &   0.373 &  0.3799 &  0.5167 &  0.4009\\ \hline
  p-val &  0.0001 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0\\ \hline
\end{tabular}  
\end{table}

\begin{table}[ht!]
\centering
 \caption{Wyniki testowania standardowego generatora w Borland C/C++ charakterystyką $\Slil{n}$.}
 \label{tab:borland_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0684 &  0.1002 &  0.1395 &   0.194 &  0.3187 &  0.4136 &  0.4685 &  0.5911 &    0.95\\ \hline
   sep1 &  0.6752 &  0.7357 &  0.9193 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
   sep2 &   0.161 &  0.2565 &  0.2978 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
  p-val &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0\\ \hline
\end{tabular}  
\end{table}
Wyniki przedstawione są w Tabelach \ref{tab:borland_asin} i \ref{tab:borland_lil}. Są one bardzo podobne do wyników generatora z Visual C++ i tyczą się ich te same uwagi.

\FloatBarrier
\subsection{Biblioteczny rand w BSD libc}
Funkcja \texttt{rand} z biblioteki systemu BSD używała kiedyś implementacji $LCG(2^{31}, 1103515245, 12345)$ i w przeciwieństwie do dwóch poprzednich generatorów zwraca wszystkie bity generowanych liczb -- i do tego testu użyliśmy ich wszystkich. Tabele \ref{tab:bsd_asin} i \ref{tab:bsd_lil} obrazują dlaczego generator zmieniono.

\begin{table}[ht!]
\centering
 \caption{Wyniki testowania starego generatora z BSD libc charakterystyką $\Sasin{n}$.}
 \label{tab:bsd_asin}
\begin{tabular} {||c|c|c|c|c|c|c||}  
 \hline 
      n &  $2^{21}$ &  $2^{22}$ &  $2^{23}$ &  $2^{24}$ &  $2^{25}$ &  $2^{26}$\\ \hline
     tv &  0.0883 &   0.098 &  0.0862 &  0.0951 &  0.0936 &  0.1081\\ \hline
   sep1 &  0.3402 &  0.3979 &  0.4111 &   0.498 &  0.4346 &  0.4751\\ \hline
   sep2 &  0.3634 &  0.4198 &  0.3053 &  0.2805 &  0.3424 &   0.407\\ \hline
  p-val &  0.1829 &  0.0326 &  0.1874 &  0.1767 &  0.1181 &  0.0051\\ \hline

\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki testowania starego generatora z BSD libc charakterystyką $\Slil{n}$.}
 \label{tab:bsd_lil}
\begin{tabular} {||c|c|c|c|c|c|c||}  
 \hline 
      n &  $2^{21}$ &  $2^{22}$ &  $2^{23}$ &  $2^{24}$ &  $2^{25}$ &  $2^{26}$\\ \hline
     tv &  0.1956 &  0.2191 &  0.2472 &  0.2539 &  0.2464 &  0.2707\\ \hline
   sep1 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
   sep2 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
  p-val &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0\\ \hline
\end{tabular}  
\end{table}

Nie potrzeba bardzo długich ciągów, by zobaczyć, że wyjście funkcji \texttt{rand} z BSD nie jest losowe. Przyczyną tak złych wyników jest fakt, że to LCG wykonuje obliczenia modulo $2^{31}$. Jak zauważyliśmy przytaczając Fakt \ref{fakt:lcg_okres}, w takiej sytuacji okres $d$ najmniej znaczących bitów wynosi $2^d$. W efekcie liczba zer i jedynek jest zbyt wyrównana co łatwo wyłapują testy oparte o własności błądzenia przypadkowego. Trzeba jednak przyznać, że charakterystyka $\Slil{n}$ pokazuje do zdecydowanie wyraźniej.

\FloatBarrier
\subsection{Biblioteczny rand w GLIBC}
Funkcja \texttt{rand} z GNU C Library korzysta z bardziej skomplikowanego generatora od testowanych do tej pory. Jego stan opisany jest przez 34 liczby  $x_i, x_{i+1},\ldots, x_{i+33}$. Generator inicjowany jest ziarnem $s$, $0 \leq s \leq 2^{31}$, zaś początkowym stanem jest
\begin{align*}
  x_0 &= s & \\
  x_i &= 16807 x_{i-1} \mod (2^{31}-1),\ \ &\mbox{gdy } 0 < i < 31\\
  x_i &= x_{31-i}, &\mbox{gdy } i \in \{31,32,33\}.
\end{align*}
Kolejne wartości $x_i$ wyznaczane są ze wzoru
\[ x_i = (x_{i-3} + x_{i-31}) \mod2^{32}. \]
Przy $k$-tym wywołaniu generator zwraca $x_{k+343} \gg 1$.
\begin{table}[ht!]
\centering
 \caption{Wyniki testowania standardowego generatora w GCC charakterystyką $\Sasin{n}$.}
 \label{tab:mojkomp_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0287 &  0.0217 &  0.0264 &  0.0228 &  0.0202 &  0.0289 &  0.0237 &  0.0254 &   0.023\\ \hline
   sep1 &  0.1157 &  0.1249 &  0.1629 &  0.1186 &  0.1661 &  0.1717 &  0.1306 &  0.1316 &  0.1405\\ \hline
   sep2 &  0.1754 &  0.1909 &  0.1436 &  0.1869 &   0.111 &  0.1344 &  0.1786 &  0.1262 &  0.1879\\ \hline
  p-val &  0.0649 &  0.5511 &  0.2565 &  0.4887 &  0.7967 &  0.1115 &   0.588 &  0.2599 &   0.393\\ \hline 
\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki testowania standardowego generatora w GCC charakterystyką $\Slil{n}$.}
 \label{tab:mojkomp_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0185 &  0.0243 &  0.0242 &  0.0244 &   0.021 &   0.022 &   0.033 &  0.0237 &  0.0221\\ \hline
   sep1 &  0.2593 &   0.164 &  0.1844 &  0.2573 &   0.201 &  0.1853 &  0.2882 &  0.4129 &  0.2563\\ \hline
   sep2 &  0.0807 &  0.2133 &  0.2299 &   0.206 &  0.1839 &  0.2361 &  0.3456 &  0.2078 &  0.1405\\ \hline
  p-val &  0.9627 &  0.4878 &  0.5225 &  0.3022 &  0.6978 &  0.5382 &  0.0009 &  0.4903 &  0.7901\\ \hline
\end{tabular}  
\end{table}

Do testów brane były wszystkie 31 bitów zwracanych przez GLP. Na podstawie wyników zebranych w Tabelach \ref{tab:mojkomp_asin} i \ref{tab:mojkomp_lil} można stwierdzić, że test arcusa sinusa nie daje podstaw do odrzucania hipotezy o losowości sekwencji generowanych przez analizowany GLP. W przypadku testu iterowanego logarytmu, rezultaty dla $n=2^{34}$ i $n=2^{33}$ również sugerowałyby, że GLP jest dobry. Jednakże dla $n=2^{32}$ obserwujemy coś dziwnego, mamy bardzo niską $p$-wartość wynoszącą około $1/1000$. Może to być kwestia przypadku -- z prawdopodobieństwem $1/1000$ zdarzyłoby się to nawet generatorowi liczb prawdziwie losowych. Popatrzmy jeszcze na $p$-wartości dla dziesięciu 1000-elementowych podzbiorów danych. Wynoszą one: $0.9313$, $0.0949$, $0.8859$, $0.1739$, $0.3675$, $0.0334$, $0.0321$, $0.1824$, $0.0017$, $0.6205$. Zauważmy, że aż 4 $p$-wartości są mniejsze niż $0.1$. W przypadku ciągów prawdziwie losowych zdarzenie, że 4 lub więcej otrzymanych $p$-wartości znajdzie się w tym przedziale wynosi
\[ 1 - \sum_{i=0}^3 \binom{10}{i} \left(\frac{1}{10}\right)^{i} \left(1 - \frac{1}{10}\right)^{10-i} \approx 0.0016 \]
To wszystko wskazuje na to, że generator z GLIBC również należy uznać za podejrzany.

\FloatBarrier
\subsection{Minstd}
Minstd jest to $MCG(2^{31}-1, 16807)$. Do generowania ciągów zerojedynkowych użyto bitów 15..8 zwracanych przez GLP. Tabele \ref{tab:minstd_asin} i \ref{tab:minstd_lil}, pokazują że nie jest to dobry GLP. 

\begin{table}[ht!]
\centering
 \caption{Wyniki testowania generatora Minstd charakterystyką $\Sasin{n}$.}
 \label{tab:minstd_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0327 &  0.0335 &  0.0528 &  0.0719 &  0.0865 &  0.1044 &  0.1597 &  0.2021 &  0.2083\\ \hline
   sep1 &  0.1514 &  0.1324 &   0.184 &  0.2413 &  0.2228 &   0.667 &  0.4911 &   0.708 &  0.8247\\ \hline
   sep2 &  0.1753 &  0.1494 &  0.2649 &  0.3751 &  0.3998 &  0.3787 &  0.3673 &  0.5063 &  0.4246\\ \hline
  p-val &  0.0056 &  0.0112 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0\\ \hline
\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki testowania generatora Minstd charakterystyką $\Slil{n}$.}
 \label{tab:minstd_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0429 &  0.0623 &   0.086 &  0.1055 &  0.1856 &  0.2046 &  0.3543 &  0.3182 &    0.95\\ \hline
   sep1 &  0.6128 &  0.5142 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
   sep2 &   0.147 &  0.2076 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0 &     1.0\\ \hline
  p-val &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0 &     0.0\\ \hline
\end{tabular}  
\end{table}



\FloatBarrier
\subsection{CMRG}
CMRG (od ang. \textit{combined multiple recursive generator}) jest jednym z generatorów mieszanych, omawianych w paragrafie \ref{sec:pop_gen}. Działa on 
\begin{equation}
 \begin{split}
    Z_n &= X_n - Y_n \mod 2^{31}-1 \\
    X_n &= 63308 X_{n-2} - 183326 X_{n-3} \mod 2^{31}-1 \\
    Y_n &= 86098 Y_{n-1} - 539608 Y_{n-3} \mod 2^{31} - 2000169
 \end{split}
\end{equation}
Do testów braliśmy bity 15..7 zmiennej $Z_n$. Tabele \ref{tab:cmrg_asin} i \ref{tab:cmrg_lil} przedstawiają ich wyniki.

\begin{table}[ht!]
\centering
 \caption{Wyniki testowania generatora CMRG charakterystyką $\Sasin{n}$.}
 \label{tab:cmrg_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0191 &  0.0234 &  0.0235 &  0.0239 &  0.0232 &  0.0228 &  0.0218 &  0.0268 &  0.0272\\ \hline
   sep1 &  0.1135 &   0.209 &  0.1187 &   0.145 &  0.1607 &  0.1191 &  0.1232 &  0.1592 &  0.2084\\ \hline
   sep2 &  0.1402 &    0.09 &  0.1084 &  0.1477 &  0.1934 &  0.1666 &  0.0931 &  0.1353 &  0.1158\\ \hline
  p-val &  0.9752 &  0.4101 &  0.8062 &  0.7402 &  0.4043 &  0.6841 &  0.9821 &  0.2394 &  0.1343\\ \hline
\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki testowania generatora CMRG charakterystyką $\Slil{n}$.}
 \label{tab:cmrg_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0218 &  0.0241 &  0.0272 &  0.0203 &  0.0231 &  0.0251 &   0.033 &  0.0233 &  0.0234\\ \hline
   sep1 &  0.2336 &  0.3273 &  0.1803 &  0.2551 &  0.3085 &   0.169 &  0.3146 &   0.331 &  0.2829\\ \hline
   sep2 &   0.345 &  0.2105 &  0.1234 &  0.1032 &  0.1901 &  0.1658 &  0.2302 &  0.2486 &  0.1761\\ \hline
  p-val &  0.6046 &  0.2689 &   0.203 &  0.8653 &  0.4581 &  0.3966 &  0.0128 &  0.4106 &  0.6803\\ \hline
\end{tabular}  
\end{table}
\FloatBarrier

  Na pierwszy rzut oka widać, że mamy tu do czynienia z generatorem o dłuższym okresie. Wyniki nie dają żadnych podstaw do odrzucenia hipotezy o losowości ciągów bitów. Trzeba tu jednak odnotować, że w \cite{kim-choe} przedstawiono silne dowody, że wyjście tego generatora nie jest dobre.

\FloatBarrier
\subsection{Mersenne Twister}
Generator Mersenne Twister został przetestowany przy użyciu implementacji dostępnej w języku C++11.

\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Sasin{n}$.}
 \label{tab:twister_asin}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0271 &  0.0245 &  0.0226 &  0.0281 &  0.0276 &  0.0228 &  0.0271 &   0.023 &  0.0255\\ \hline
   sep1 &  0.1421 &  0.1606 &  0.1167 &  0.1824 &  0.1938 &  0.1571 &   0.176 &  0.1397 &  0.1573\\ \hline
   sep2 &  0.1133 &  0.1195 &  0.1056 &  0.1399 &  0.1256 &  0.1287 &  0.1391 &  0.0994 &  0.1444\\ \hline
  p-val &  0.2755 &  0.6823 &  0.8804 &  0.0801 &  0.1267 &  0.7343 &  0.1094 &  0.7596 &  0.2029\\ \hline
\end{tabular}  
\end{table}
\begin{table}[ht!]
\centering
 \caption{Wyniki dla $\Slil{n}$.}
 \label{tab:twister_lil}
\begin{tabular} {||c|c|c|c|c|c|c|c|c|c|c|c||}  
 \hline 
     n &  $2^{26}$ &  $2^{27}$ &  $2^{28}$ &  $2^{29}$ &  $2^{30}$ &  $2^{31}$ &  $2^{32}$ &  $2^{33}$ &  $2^{34}$\\ \hline
     tv &  0.0254 &  0.0264 &  0.0238 &  0.0234 &  0.0266 &  0.0265 &   0.029 &  0.0299 &  0.0215\\ \hline
   sep1 &  0.1853 &  0.2147 &  0.1689 &  0.3093 &  0.1887 &  0.2015 &  0.2877 &  0.2318 &  0.1953\\ \hline
   sep2 &  0.2841 &  0.1696 &  0.1917 &  0.1282 &  0.2017 &  0.2492 &  0.1842 &  0.1541 &  0.1716\\ \hline
  p-val &  0.2483 &   0.369 &  0.5965 &  0.5066 &  0.2196 &  0.1495 &  0.0499 &  0.0189 &   0.817\\ \hline
\end{tabular}  
\end{table}


\FloatBarrier
\section{Podsumowanie}
Testy oparte o własności błądzenia przypadkowego wydają się być dobrym uzupełnieniem standardowych pakietów testowych takich jak np. NIST Test Suite, zwłaszcza, że mają potencjał do wyłapywania problemów nieco innej natury.

\begin{thebibliography}{99}
 \bibitem{asmussen}
    S. Asmussen, P. Glynn, \emph{Stochastic Simulation: Algorithms and Analysis}, Springer, New York, 2007
 \bibitem{feller}
    W. Feller, \emph{Wstęp do rachunku prawdopodobieństwa}, PWN, Warszawa, Wydanie piąte, 1987
 \bibitem{hull}
    T. Hull, A. Dobell, \emph{Random number generators}, SIAM Rev, v.4, 1962, s. 230-254
 \bibitem{jak-szt}
    J. Jakubowski, R. Sztencel, \emph{Wstęp do teorii prawdopodobieństwa}, Script, Warszawa, Wydanie IV, 2010
 \bibitem{kim-choe}
    C. Kim, G.H. Choe, D.H. Kim, \emph{Tests of randomness by the gambler's ruin algorithm},  Applied Mathematics and Computation 199, 2008, s. 195-210
 \bibitem{knuth}
    D. Knuth, \emph{The Art of Computer Programming volume 2: Seminumerical algorithms (2nd ed.)}, Addison-Wesley, 1981
  \bibitem{lecuyer}
    P. L'Ecuyer, \emph{Efficient and Portable Combined Random Number Generator}, Communications of the ACM 31, 1988, s. 742–749, 774.
  \bibitem{twister}
    M. Matsumoto, T. Nishimura, \emph{Mersenne twister: a 623-dimensionally equidistributed uniform pseudo-random number generator}, ACM Transactions on Modeling and Computer Simulation 8 (1), 1998, s. 3–30.
 \bibitem{wang-nic}
    Y. Wang, T. Nicol, \emph{On Statistical Distance Based Testing of Pseudo Random Sequences and Experiments with PHP and Debian OpenSSL}, Computers \& Security, 53, 44--64, 2015

\end{thebibliography}



\end{document}
